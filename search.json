[
  {
    "objectID": "notebooks/Gradient Descent-2d.html",
    "href": "notebooks/Gradient Descent-2d.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib inline\n\n\nimport numpy as np\n\n\n4.1*4.1\n\n16.81\n\n\n\n4.1-0.2*2*4.1\n\n2.46\n\n\n\nx = 4.1\nalpha = 0.2\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    print(\"(\",round(x, 2),\",\" ,round(x*x, 2),\")\")\n\n( 2.46 , 6.05 )\n( 1.48 , 2.18 )\n( 0.89 , 0.78 )\n( 0.53 , 0.28 )\n( 0.32 , 0.1 )\n( 0.19 , 0.04 )\n( 0.11 , 0.01 )\n( 0.07 , 0.0 )\n( 0.04 , 0.0 )\n( 0.02 , 0.0 )\n\n\n\nx = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    st = \"\"\"\n    \n    \\begin{frame}{Iteration %d}\n    \\begin{columns}\n\n\n        \\begin{column}{0.6\\textwidth}\n            \\begin{adjustbox}{max totalsize={\\textwidth},center}\n                \\begin{tikzpicture}\n\n                    \\begin{axis}[\n                        xlabel=$x$,\n                        ylabel=$y$,\n                        xmin=-4.2,\n                        xmax=4.2,\n                        axis x line*=bottom,\n                        axis y line*=left,\n                        xtick align=outside,\n                        ytick align=outside,\n                        legend pos=outer north east\n                        ]\n                        \\addplot[mark=none, gray] {x^2};\\addlegendentry{$y=x^2$}\n                        \\addplot[only marks, mark=*]\n                        coordinates{ % plot 1 data set\n                            (%s,%s)\n                            }; \n\n\n\n                        \\end{axis}\n\n                \\end{tikzpicture}\n            \\end{adjustbox}\n        \\end{column}\n    \\begin{column}{0.5\\textwidth}\n    \\begin{adjustbox}{max totalsize={\\textwidth},center}\n        \\begin{tikzpicture}\n        \\begin{axis}\n        [\n        title={Contour plot, view from top},\n        view={0}{90},\n        xlabel=$x$,\n        ylabel=$y$,\n        axis x line*=bottom,\n        axis y line*=left,\n        xtick align=outside,\n        ytick align=outside,\n        unit vector ratio*=1 1 1,\n        ]\n        \\addplot3[\n        contour gnuplot={number=14,}\n        ]\n        {x^2};\n        \\addplot[only marks, mark=*]\n        coordinates{ % plot 1 data set\n            (%f,%f)\n        }; \n        \\end{axis}\n        \\end{tikzpicture}\n        \\end{adjustbox}\n    \\end{column}\n    \\end{columns}\n\n\n    \\end{frame}\n    \"\"\" %(i, i, i, i, i)\n\nValueError: unsupported format character 'p' (0x70) at index 793\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\n8.2*4.1\n\n33.62\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\nlatexify()\nval = -7.2\n\nplt.scatter([val],func(np.array([val])), color='k')\nax.annotate('Local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='grey', shrink=0.0001))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y, color='grey')\nformat_axes(plt.gca())\nplt.xlabel(\"x\")\nplt.ylabel(\"y=f(x)\")\nplt.savefig(\"../gradient-descent/local-minima.eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nimport sys\nsys.path.append(\"../\")\n\n\nfrom latexify import format_axes, latexify\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = 0.95\niterations = 10\nlatexify()\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5, color='grey')\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-real-output.html",
    "href": "notebooks/decision-tree-real-input-real-output.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\nfrom latexify import latexify, format_axes\n\n\n# Create dataset\nx = np.array([1, 2, 3, 4, 5, 6])\ny = np.array([0, 0, 1, 1, 2, 2])\n\n# plot data\nlatexify()\nplt.scatter(x, y, color='k')\nformat_axes(plt.gca()) \nplt.savefig(\"../figures/decision-trees/ri-ro-dataset.pdf\")\n\n\n\n\n\n# Depth 0 tree\n# Average of all y values\ny_pred = np.mean(y)\n# Plot data\nlatexify()\nplt.scatter(x, y, color='C1', label='data')\n# Plot prediction\nplt.plot([0, 7], [y_pred, y_pred], color='k', linestyle='-', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/ri-ro-depth-0.pdf\")\n\n\n\n\n\n# Depth 1 tree\n\n\n# getting the same via sklearn\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\ndt = DecisionTreeRegressor(max_depth=1)\ndt.fit(x.reshape(-1, 1), y)\ny_pred = dt.predict(x.reshape(-1, 1))\n\n# Plot data\nlatexify()\nplt.scatter(x, y, color='C1', label='Data')\n\n\nx_test = np.linspace(0, 7, 500)\ny_test = dt.predict(x_test.reshape(-1, 1))\nplt.plot(x_test, y_test, color='k', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/ri-ro-depth-1.pdf\")\n\n\n\n\n\n# Plot tree using sklearn export\nfrom sklearn.tree import export_graphviz\nimport graphviz\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/ri-ro-depth-1-sklearn\")\n\n'../figures/decision-trees/ri-ro-depth-1-sklearn.pdf'\n\n\n\n# Depth 2 tree\ndt = DecisionTreeRegressor(max_depth=2)\ndt.fit(x.reshape(-1, 1), y)\n\n# Plot data\nlatexify()\nplt.scatter(x, y, color='C1', label='Data')\n# Plot decision boundary\n\nx_test = np.linspace(0, 7, 500)\ny_test = dt.predict(x_test.reshape(-1, 1))\nplt.plot(x_test, y_test, color='k', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/ri-ro-depth-2.pdf\")\n\n\n\n\n\n# Plot dt\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/ri-ro-depth-2-sklearn\")\n\n'../figures/decision-trees/ri-ro-depth-2-sklearn.pdf'\n\n\n\n### Sine daatset\nx = np.linspace(0, 2*np.pi, 200)\ny = np.sin(x)\n\n# Plot data\nlatexify()\nplt.scatter(x, y, color='k', s=1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/sine-dataset.pdf\")\n\n\n\n\n\ndt = DecisionTreeRegressor(max_depth=1)\ndt.fit(x.reshape(-1, 1), y)\n\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/sine-depth-1-sklearn\")\n\n'../figures/decision-trees/sine-depth-1-sklearn.pdf'\n\n\n\nlatexify()\nplt.scatter(x, y, color='C1', label='Data')\n# Plot decision boundary\n\nx_test = np.linspace(0, 7, 500)\ny_test = dt.predict(x_test.reshape(-1, 1))\nplt.plot(x_test, y_test, color='k', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/sine-depth-1.pdf\")\n\n\n\n\n\ndt = DecisionTreeRegressor(max_depth=4)\n\nplt.scatter(x, y, color='C1', label='Data',s=1)\n\ndt.fit(x.reshape(-1, 1), y)\ny_test = dt.predict(x_test.reshape(-1, 1))\nplt.plot(x_test, y_test, color='k', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/sine-depth-4.pdf\")\n\n\n\n\n\n### Dataset for showing bias-variance tradeoff\n\n# Create dataset\nX = np.array([\n    [1, 1],\n    [2, 1],\n    [3, 1],\n    [5, 1],\n    [6, 1],\n    [7, 1],\n    [1, 2],\n    [2, 2],\n    [6, 2],\n    [7, 2],\n    [1, 4],\n    [7, 4] \n])\n\ny = np.array([0, 0, 0, 1, 1, 1, 0, 1, 0, 1 ,0, 1])\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset.pdf\")\n\n\n\n\n\ndt = DecisionTreeClassifier(max_depth=1)\ndt.fit(X, y)\n\n# Predict in entire 2d space and contour plot\nx1 = np.linspace(0, 8, 100)\nx2 = np.linspace(0, 5, 100)\n\nX1, X2 = np.meshgrid(x1, x2)\nX_test = np.stack([X1.flatten(), X2.flatten()], axis=1)\ny_test = dt.predict(X_test)\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.contourf(X1, X2, y_test.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-depth-1.pdf\")\n\n\n\n\n\n# Now, plot the dt via sklearn export\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/bias-variance-depth-1-sklearn\")\n\n'../figures/decision-trees/bias-variance-depth-1-sklearn.pdf'\n\n\n\n# No depth limit\ndt = DecisionTreeClassifier()\ndt.fit(X, y)\n\n# Predict in entire 2d space and contour plot\ny_test = dt.predict(X_test)\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.contourf(X1, X2, y_test.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-full-depth.pdf\")\n\n\n\n\n\n# export tree \ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/bias-variance-full-depth-sklearn\")\n\n'../figures/decision-trees/bias-variance-full-depth-sklearn.pdf'\n\n\n\n# Bias variance dataset 2\n# X is all integers from (1, 1) to (6, 6)\n\nX = np.array([[i, j] for i in range(1, 7) for j in range(1, 7)])\n\n\ny = np.zeros(len(X), dtype=int)\n\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\n&lt;matplotlib.collections.PathCollection at 0x150a41df0&gt;\n\n\n\n\n\n\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 4)\ny[special_condition] = 0\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2.pdf\")\n\n\n\n\n\n# X_test random uniform frmo (1, 1) to (6, 6) of size 1000\nX_test = np.random.uniform(1, 6, size=(1000, 2))\ny_test = np.zeros(len(X_test), dtype=int)\n\ny_test[(2 &lt;= X_test[:, 0]) & (X_test[:, 0] &lt;= 5) & (2 &lt;= X_test[:, 1]) & (X_test[:, 1] &lt;= 5)] = 1\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-test.pdf\")\n\n\n\n\n\n# Underfitting\n# Depth-1 tree\ndt = DecisionTreeClassifier(max_depth=2)\ndt.fit(X, y)\n\n\n# Predict in entire 2d space and contour plot\nx1 = np.linspace(0.5, 6.5, 100)\nx2 = np.linspace(0.5, 6.5, 100)\n# Contour plot\nX1, X2 = np.meshgrid(x1, x2)\nX_contour = np.stack([X1.flatten(), X2.flatten()], axis=1)\n\ny_contour = dt.predict(X_contour)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-depth-2.pdf\")\n\n# Export tree\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/bias-variance-depth-2-sklearn\")\n\n'../figures/decision-trees/bias-variance-depth-2-sklearn.pdf'\n\n\n\n\n\n\n# Overfitting\ndt = DecisionTreeClassifier()\ndt.fit(X, y)\n\n# Predict in entire 2d space and contour plot\ny_contour = dt.predict(X_contour)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-full-depth.pdf\")\n\n# Export tree\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/bias-variance-full-depth-sklearn\")\n\n'../figures/decision-trees/bias-variance-full-depth-sklearn.pdf'\n\n\n\n\n\n\n### Good fit\ndt = DecisionTreeClassifier(max_depth=4)\n\ndt.fit(X, y)\n\n# Predict in entire 2d space and contour plot\ny_contour = dt.predict(X_contour)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-good-fit.pdf\")\n\n# Export tree\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/bias-variance-good-fit-sklearn\")\n\n'../figures/decision-trees/bias-variance-good-fit-sklearn.pdf'\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n### Train and test accuracy vs depth\n\ndepths = np.arange(2, 10)\ntrain_accs = {}\ntest_accs = {}\nfor depth in depths:\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n    train_accs[depth] = accuracy_score(y, dt.predict(X))\n    test_accs[depth] = accuracy_score(y_test, dt.predict(X_test))\n\n\ntrain_accs = pd.Series(train_accs)\ntest_accs = pd.Series(test_accs)\n\nplt.plot(depths, train_accs, label='Train')\nplt.plot(depths, test_accs, label='Test')\nplt.xlabel(\"Depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.ylim(0, 1.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth.pdf\")\n\n# Highlight area of underfitting (depth &lt; 4) fill with green \nplt.fill_between(depths, 0, 1, where=depths &lt;= 4, color='g', alpha=0.1, label='Underfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-underfitting.pdf\")\n\n\n# Highlight area of overfitting (depth &gt;7 4) fill with red\nplt.fill_between(depths, 0, 1, where=depths &gt;= 7, color='r', alpha=0.1, label='Overfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-overfitting.pdf\")\n\n\n# Highlight good fit area (4 &lt; depth &lt; 7) fill with blue\nplt.fill_between(depths, 0, 1, where=(depths &gt;= 4) & (depths &lt;= 7), color='b', alpha=0.1, label='Good fit')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-good-fit.pdf\")\n\n\n\n\n\n\n# Slight variation of the dataset leads to a completely different tree\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\n\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 3)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-2.pdf\")\n\n\n\n\n\ndt = DecisionTreeRegressor()\ndt.fit(X, y)\n\n# Export\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\ngraph = graphviz.Source(dot_data)\ngraph.format = 'pdf'\ngraph.render(\"../figures/decision-trees/bias-variance-full-depth-sklearn-2\")\n\n'../figures/decision-trees/bias-variance-full-depth-sklearn-2.pdf'"
  },
  {
    "objectID": "notebooks/logistic-apple-oranges.html",
    "href": "notebooks/logistic-apple-oranges.html",
    "title": "Fitting linear model",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\nimport matplotlib.patches as mpatches\n\n\nx = np.array([0, 0.1, 0.2, 0.3, 0.6, 0.7, 0.9])\n\n\ny = (x&gt;0.4).astype('int')\n\n\ny\n\narray([0, 0, 0, 0, 1, 1, 1])\n\n\n\n\nlatexify()\nplt.scatter(x, np.zeros_like(x), c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.xlabel('Radius')\nplt.gca().yaxis.set_visible(False) \nformat_axes(plt.gca())\nplt.savefig(\"logistic-orange-tomatoes-original.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.savefig(\"logistic-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nlinr_reg = LinearRegression()\n\n\nlinr_reg.fit(x.reshape(-1, 1), y)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.axhline(y=1, color='grey', label='P(y=1)')\nplt.axhline(y=0, color='grey')\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.savefig(\"linear-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n (0.5-linr_reg.intercept_)/linr_reg.coef_\n\narray([0.44857143])\n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.savefig(\"linear-orange-tomatoes-decision.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nx_dash = np.append(x, 2.5)\ny_dash = np.append(y, 1)\nlinr_reg.fit(x_dash.reshape(-1, 1), y_dash)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\n\nplt.plot(np.linspace(0, 2.5, 50), linr_reg.predict(np.linspace(0, 2.5, 50).reshape(-1, 1)))\nplt.scatter(x_dash, y_dash, c=y_dash)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.savefig(\"linear-orange-tomatoes-decision-modified.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(penalty='none', solver='lbfgs')\n\n\nclf.fit(x.reshape(-1,1), y)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\nclf.coef_\n\narray([[55.99493009]])\n\n\n\n-clf.intercept_[0]/clf.coef_[0]\n\narray([0.4484548])\n\n\n\nclf.intercept_\n\narray([-25.11119514])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\nlatexify()\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nplt.axvline(x = -clf.intercept_[0]/clf.coef_[0],label='Decision Boundary',linestyle='--',color='k',lw=1)\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.legend(handles=[black_patch, yellow_patch, blue_patch])\nplt.axhspan(0,1, xmin=0, xmax=0.49, linestyle='--',color='darkblue',lw=1, alpha=0.2)\nplt.axhspan(0,0.001, xmin=0, xmax=0.49, linestyle='--',color='k',lw=1, )\n\nplt.axhspan(0,1, xmax=1, xmin=0.49, linestyle='--',color='yellow',lw=1, alpha=0.2)\nplt.axhspan(1,1.001,  xmax=1, xmin=0.49, linestyle='--',color='k',lw=1, )\nplt.savefig(\"linear-orange-tomatoes-decision-ideal.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\nplt.scatter(x, y, c=y)\nlatexify()\nplt.axvline(-clf.intercept_[0]/clf.coef_[0], lw=2, color='black')\nplt.axhline(0.5, linestyle='--',color='k',lw=3, label='P(y=1) = P(y=0)')\nplt.ylabel(\"P(y=1)\")\nplt.xlabel('Radius')\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nsigmoid_patch = mpatches.Patch(color='steelblue', label='Sigmoid')\nplt.legend(handles=[black_patch, yellow_patch, blue_patch, sigmoid_patch])\nformat_axes(plt.gca())\nplt.title(\"Logistic Regression\")\nplt.savefig(\"logistic.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\n\n\nformat_axes(plt.gca())\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\sigma(z)$\")\nplt.savefig(\"logistic-function.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/cost-iteration-notebook.html",
    "href": "notebooks/cost-iteration-notebook.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n\n\n\nX = np.array([1, 2, 3])\ny = np.array([1, 2, 3])\n\n\ndef y_hat(X, theta_0, theta_1):\n    return theta_0 + theta_1*X\n\n\ndef cost(X, y, theta_0, theta_1):\n    yh = y_hat(X, theta_0, theta_1)\n    return (y-yh).T@(y-yh)\n\n\ntheta_0 = 4\ntheta_1 = 0\nalpha = 0.1\ncosts = np.zeros(1000)\ntheta_0_list = np.zeros(1000)\ntheta_1_list = np.zeros(1000)\n\nfor i in range(1000):\n    costs[i] = cost(X, y, theta_0, theta_1)\n    theta_0 = theta_0 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).mean())\n    theta_1 = theta_1 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).T@X)/len(X)\n    theta_0_list[i] = theta_0\n    theta_1_list[i] = theta_1\n\n\nimport sys\nsys.path.append(\"../\")\nfrom latexify import *\n\n\nlatexify()\nplt.plot(costs[:200], 'k')\nformat_axes(plt.gca())\nplt.ylabel(\"Cost\")\nplt.xlabel(\"Iteration\")\nplt.savefig(\"../gradient-descent/gd-iter-cost.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\nfor i in range(0, 200, 20):\n    plt.title(label=\"Fit at iteration {}\".format(i))\n    plt.plot(X, theta_0_list[i]+theta_1_list[i]*X, color='k')\n    plt.scatter(X, y, color='k')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/fit-iteration-{}.pdf\".format(i), bbox_inches=\"tight\", transparent=True)\n    plt.cla()\n\n\n\n\n\ntheta_0 = 4\ntheta_1 = 0\n(y_hat(X, theta_0, theta_1)-y).mean()\n\n2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1)).mean()\n\n-2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1))@X\n\n-10"
  },
  {
    "objectID": "notebooks/logistic-circular.html",
    "href": "notebooks/logistic-circular.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n\n\n# Choose some points between\n\n\nnp.random.seed(0)\nx1 = np.random.randn(1, 100)\nx2 = np.random.randn(1, 100)\n\n\ny = x1**2 + x2**2\n\n\nx1\n\narray([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n        -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n         0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n         0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n        -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n        -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n         0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n         0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n        -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n        -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n        -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n         0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n        -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n        -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n         0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n        -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n        -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n         1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n        -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n         0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936]])\n\n\n\ny[y&gt;1] = 1\ny[y&lt;1] = 0\n\nc = 0\nfor i in range(100):\n    if y[0, i] == 1:\n        y[0, i] = 0\n        c += 1\n    if c == 10:\n        break\n\n\nlatexify()\nplt.scatter(x1, x2, c=y,s=5)\n\n\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig(\"logisitic-circular-data.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nnp.vstack((x1, x2)).shape\n\n(2, 100)\n\n\n\nclf_1 = LogisticRegression(penalty='none',solver='newton-cg')\nclf_1.fit(np.vstack((x1, x2)).T, y.T)\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf_1.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"logisitic-linear-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nnew_x = np.zeros((4, 100))\n\n\nnew_x[0] = x1\nnew_x[1] = x2\nnew_x[2] = x1**2\nnew_x[3] = x2**2\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(new_x.T, y.T)\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\nclf.coef_\n\narray([[-0.50464855, -0.30337009,  1.08937351,  0.73697949]])\n\n\n\nnew_x.T[:, 0]\n\narray([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n        0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n        0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n       -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n       -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n        0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n        0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n       -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n       -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n       -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n        0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n       -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n       -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n        0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n       -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n       -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n        1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n       -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n        0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936])\n\n\n\nX = np.vstack((x1, x2)).T\nX.shape\n\n(100, 2)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"logisitic-circular-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nZ.shape\n\n(261, 272)\n\n\n\nnp.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())]\n\narray([[-2.85298982, -2.52340315,  8.13955089,  6.36756347],\n       [-2.83298982, -2.52340315,  8.0258313 ,  6.36756347],\n       [-2.81298982, -2.52340315,  7.9129117 ,  6.36756347],\n       ...,\n       [ 2.52701018,  2.67659685,  6.38578047,  7.16417069],\n       [ 2.54701018,  2.67659685,  6.48726088,  7.16417069],\n       [ 2.56701018,  2.67659685,  6.58954129,  7.16417069]])\n\n\n\nxx.ravel()\n\narray([-2.85298982, -2.83298982, -2.81298982, ...,  2.52701018,\n        2.54701018,  2.56701018])\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - h, X[:, 0].max() + h\ny_min, y_max = X[:, 1].min() - h, X[:, 1].max() + h\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n# Put the result into a color plot\nZ = Z[:, 0].reshape(xx.shape)\nlatexify()\nplt.contourf(xx, yy, Z,levels=np.linspace(0, 1.1, num=10),cmap='Blues')\nplt.gca().set_aspect('equal')\n#plt.scatter(x1, x2, c=y)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.colorbar(label='P(Tomatoes)')\nplt.savefig(\"logisitic-circular-probability.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nxx.shape\n\n(233, 244)\n\n\n\nZ.size\n\n56852\n\n\n\nnp.linspace(0, 1.1, num=50)\n\narray([0.        , 0.02244898, 0.04489796, 0.06734694, 0.08979592,\n       0.1122449 , 0.13469388, 0.15714286, 0.17959184, 0.20204082,\n       0.2244898 , 0.24693878, 0.26938776, 0.29183673, 0.31428571,\n       0.33673469, 0.35918367, 0.38163265, 0.40408163, 0.42653061,\n       0.44897959, 0.47142857, 0.49387755, 0.51632653, 0.53877551,\n       0.56122449, 0.58367347, 0.60612245, 0.62857143, 0.65102041,\n       0.67346939, 0.69591837, 0.71836735, 0.74081633, 0.76326531,\n       0.78571429, 0.80816327, 0.83061224, 0.85306122, 0.8755102 ,\n       0.89795918, 0.92040816, 0.94285714, 0.96530612, 0.9877551 ,\n       1.01020408, 1.03265306, 1.05510204, 1.07755102, 1.1       ])"
  },
  {
    "objectID": "notebooks/xor.html",
    "href": "notebooks/xor.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nimport numpy as np \n\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\ny = np.array([[0],[1],[1],[0]])\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=2))\nmodel.add(Activation('sigmoid'))\n#model.add(Dense(1))\n#model.add(Activation('sigmoid'))\n\nsgd = SGD(lr=0.1)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\nmodel.fit(X, y,  batch_size=1, nb_epoch=1000)\n\nEpoch 1/1000\n4/4 [==============================] - 0s 12ms/step - loss: 0.8211\nEpoch 2/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.8000\nEpoch 3/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7830\nEpoch 4/1000\n4/4 [==============================] - 0s 988us/step - loss: 0.7705\nEpoch 5/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.7608\nEpoch 6/1000\n4/4 [==============================] - 0s 949us/step - loss: 0.7534\nEpoch 7/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7478\nEpoch 8/1000\n1/4 [======&gt;.......................] - ETA: 0s - loss: 0.85904/4 [==============================] - 0s 1ms/step - loss: 0.7435\nEpoch 9/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7402\nEpoch 10/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7379\nEpoch 11/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7358\nEpoch 12/1000\n4/4 [==============================] - 0s 950us/step - loss: 0.7339\nEpoch 13/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7332\nEpoch 14/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7321\nEpoch 15/1000\n4/4 [==============================] - 0s 923us/step - loss: 0.7307\nEpoch 16/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7306\nEpoch 17/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7299\nEpoch 18/1000\n4/4 [==============================] - 0s 883us/step - loss: 0.7291\nEpoch 19/1000\n4/4 [==============================] - 0s 980us/step - loss: 0.7287\nEpoch 20/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7282\nEpoch 21/1000\n4/4 [==============================] - 0s 822us/step - loss: 0.7277\nEpoch 22/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7268\nEpoch 23/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7271\nEpoch 24/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7268\nEpoch 25/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7265\nEpoch 26/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7259\nEpoch 27/1000\n4/4 [==============================] - 0s 926us/step - loss: 0.7256\nEpoch 28/1000\n4/4 [==============================] - 0s 888us/step - loss: 0.7254\nEpoch 29/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.7247\nEpoch 30/1000\n4/4 [==============================] - 0s 920us/step - loss: 0.7243\nEpoch 31/1000\n4/4 [==============================] - 0s 974us/step - loss: 0.7246\nEpoch 32/1000\n4/4 [==============================] - 0s 983us/step - loss: 0.7244\nEpoch 33/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7240\nEpoch 34/1000\n4/4 [==============================] - 0s 977us/step - loss: 0.7234\nEpoch 35/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.7233\nEpoch 36/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7237\nEpoch 37/1000\n4/4 [==============================] - 0s 970us/step - loss: 0.7234\nEpoch 38/1000\n4/4 [==============================] - 0s 904us/step - loss: 0.7232\nEpoch 39/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7232\nEpoch 40/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7224\nEpoch 41/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7229\nEpoch 42/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7228\nEpoch 43/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7225\nEpoch 44/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7222\nEpoch 45/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7223\nEpoch 46/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7219\nEpoch 47/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7219\nEpoch 48/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7219\nEpoch 49/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7208\nEpoch 50/1000\n4/4 [==============================] - 0s 986us/step - loss: 0.7215\nEpoch 51/1000\n4/4 [==============================] - 0s 923us/step - loss: 0.7213\nEpoch 52/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7215\nEpoch 53/1000\n4/4 [==============================] - 0s 948us/step - loss: 0.7211\nEpoch 54/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7213\nEpoch 55/1000\n4/4 [==============================] - 0s 818us/step - loss: 0.7202\nEpoch 56/1000\n4/4 [==============================] - 0s 910us/step - loss: 0.7209\nEpoch 57/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7209\nEpoch 58/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7202\nEpoch 59/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7207\nEpoch 60/1000\n4/4 [==============================] - 0s 935us/step - loss: 0.7201\nEpoch 61/1000\n4/4 [==============================] - 0s 897us/step - loss: 0.7207\nEpoch 62/1000\n4/4 [==============================] - 0s 982us/step - loss: 0.7200\nEpoch 63/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7204\nEpoch 64/1000\n4/4 [==============================] - 0s 986us/step - loss: 0.7199\nEpoch 65/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7198\nEpoch 66/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7203\nEpoch 67/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.7197\nEpoch 68/1000\n4/4 [==============================] - 0s 821us/step - loss: 0.7193\nEpoch 69/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 70/1000\n4/4 [==============================] - 0s 857us/step - loss: 0.7196\nEpoch 71/1000\n4/4 [==============================] - 0s 858us/step - loss: 0.7196\nEpoch 72/1000\n4/4 [==============================] - 0s 1000us/step - loss: 0.7195\nEpoch 73/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7199\nEpoch 74/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7197\nEpoch 75/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7197\nEpoch 76/1000\n4/4 [==============================] - 0s 819us/step - loss: 0.7189\nEpoch 77/1000\n4/4 [==============================] - 0s 834us/step - loss: 0.7199\nEpoch 78/1000\n4/4 [==============================] - 0s 830us/step - loss: 0.7198\nEpoch 79/1000\n4/4 [==============================] - 0s 800us/step - loss: 0.7196\nEpoch 80/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7194\nEpoch 81/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7195\nEpoch 82/1000\n4/4 [==============================] - 0s 897us/step - loss: 0.7190\nEpoch 83/1000\n4/4 [==============================] - 0s 803us/step - loss: 0.7196\nEpoch 84/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.7189\nEpoch 85/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 86/1000\n4/4 [==============================] - 0s 795us/step - loss: 0.7195\nEpoch 87/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.7195\nEpoch 88/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7193\nEpoch 89/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 90/1000\n4/4 [==============================] - 0s 807us/step - loss: 0.7188\nEpoch 91/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7195\nEpoch 92/1000\n4/4 [==============================] - 0s 980us/step - loss: 0.7195\nEpoch 93/1000\n4/4 [==============================] - 0s 824us/step - loss: 0.7191\nEpoch 94/1000\n4/4 [==============================] - 0s 958us/step - loss: 0.7188\nEpoch 95/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7193\nEpoch 96/1000\n4/4 [==============================] - 0s 950us/step - loss: 0.7188\nEpoch 97/1000\n4/4 [==============================] - 0s 773us/step - loss: 0.7188\nEpoch 98/1000\n4/4 [==============================] - 0s 939us/step - loss: 0.7191\nEpoch 99/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 100/1000\n4/4 [==============================] - 0s 885us/step - loss: 0.7190\nEpoch 101/1000\n4/4 [==============================] - 0s 973us/step - loss: 0.7193\nEpoch 102/1000\n4/4 [==============================] - 0s 987us/step - loss: 0.7193\nEpoch 103/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7193\nEpoch 104/1000\n4/4 [==============================] - 0s 940us/step - loss: 0.7192\nEpoch 105/1000\n4/4 [==============================] - 0s 813us/step - loss: 0.7191\nEpoch 106/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 107/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 108/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 109/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 110/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.7188\nEpoch 111/1000\n4/4 [==============================] - 0s 862us/step - loss: 0.7188\nEpoch 112/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 113/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 114/1000\n4/4 [==============================] - 0s 862us/step - loss: 0.7181\nEpoch 115/1000\n4/4 [==============================] - 0s 782us/step - loss: 0.7188\nEpoch 116/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 117/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 118/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7182\nEpoch 119/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.7186\nEpoch 120/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7193\nEpoch 121/1000\n4/4 [==============================] - 0s 851us/step - loss: 0.7192\nEpoch 122/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 123/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 124/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 125/1000\n4/4 [==============================] - 0s 839us/step - loss: 0.7191\nEpoch 126/1000\n4/4 [==============================] - 0s 990us/step - loss: 0.7180\nEpoch 127/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 128/1000\n4/4 [==============================] - 0s 809us/step - loss: 0.7191\nEpoch 129/1000\n4/4 [==============================] - 0s 868us/step - loss: 0.7183\nEpoch 130/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 131/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 132/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 133/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 134/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 135/1000\n4/4 [==============================] - 0s 935us/step - loss: 0.7183\nEpoch 136/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 137/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 138/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 139/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 140/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 141/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 142/1000\n4/4 [==============================] - 0s 966us/step - loss: 0.7190\nEpoch 143/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 144/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 145/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 146/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 147/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 148/1000\n4/4 [==============================] - 0s 978us/step - loss: 0.7183\nEpoch 149/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 150/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 151/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 152/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 153/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 154/1000\n4/4 [==============================] - 0s 996us/step - loss: 0.7179\nEpoch 155/1000\n4/4 [==============================] - 0s 996us/step - loss: 0.7188\nEpoch 156/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 157/1000\n4/4 [==============================] - 0s 969us/step - loss: 0.7186\nEpoch 158/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 159/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 160/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 161/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 162/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 163/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 164/1000\n4/4 [==============================] - 0s 886us/step - loss: 0.7186\nEpoch 165/1000\n4/4 [==============================] - 0s 798us/step - loss: 0.7188\nEpoch 166/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 167/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 168/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 169/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 170/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 171/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 172/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 173/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7189\nEpoch 174/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 175/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 176/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 177/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.7180\nEpoch 178/1000\n4/4 [==============================] - 0s 851us/step - loss: 0.7188\nEpoch 179/1000\n4/4 [==============================] - 0s 875us/step - loss: 0.7187\nEpoch 180/1000\n4/4 [==============================] - 0s 969us/step - loss: 0.7181\nEpoch 181/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7185\nEpoch 182/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 183/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7182\nEpoch 184/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7185\nEpoch 185/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 186/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 187/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 188/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 189/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 190/1000\n4/4 [==============================] - 0s 914us/step - loss: 0.7183\nEpoch 191/1000\n4/4 [==============================] - 0s 922us/step - loss: 0.7180\nEpoch 192/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 193/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 194/1000\n4/4 [==============================] - 0s 974us/step - loss: 0.7185\nEpoch 195/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 196/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 197/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 198/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 199/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 200/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 201/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 202/1000\n4/4 [==============================] - 0s 891us/step - loss: 0.7191\nEpoch 203/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 204/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 205/1000\n4/4 [==============================] - 0s 935us/step - loss: 0.7184\nEpoch 206/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 207/1000\n4/4 [==============================] - 0s 961us/step - loss: 0.7183\nEpoch 208/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.7183\nEpoch 209/1000\n4/4 [==============================] - 0s 826us/step - loss: 0.7190\nEpoch 210/1000\n4/4 [==============================] - 0s 992us/step - loss: 0.7190\nEpoch 211/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.7183\nEpoch 212/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.7190\nEpoch 213/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 214/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 215/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 216/1000\n4/4 [==============================] - 0s 992us/step - loss: 0.7183\nEpoch 217/1000\n4/4 [==============================] - 0s 807us/step - loss: 0.7183\nEpoch 218/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.7190\nEpoch 219/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 220/1000\n4/4 [==============================] - 0s 916us/step - loss: 0.7187\nEpoch 221/1000\n4/4 [==============================] - 0s 938us/step - loss: 0.7188\nEpoch 222/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 223/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 224/1000\n4/4 [==============================] - 0s 847us/step - loss: 0.7183\nEpoch 225/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 226/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 227/1000\n4/4 [==============================] - 0s 927us/step - loss: 0.7188\nEpoch 228/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.7190\nEpoch 229/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 230/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 231/1000\n4/4 [==============================] - 0s 799us/step - loss: 0.7183\nEpoch 232/1000\n4/4 [==============================] - 0s 897us/step - loss: 0.7183\nEpoch 233/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 234/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7179\nEpoch 235/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 236/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 237/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7190\nEpoch 238/1000\n4/4 [==============================] - 0s 891us/step - loss: 0.7190\nEpoch 239/1000\n4/4 [==============================] - 0s 950us/step - loss: 0.7189\nEpoch 240/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 241/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 242/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 243/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 244/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 245/1000\n4/4 [==============================] - 0s 883us/step - loss: 0.7190\nEpoch 246/1000\n4/4 [==============================] - 0s 951us/step - loss: 0.7190\nEpoch 247/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 248/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 249/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 250/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 251/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7188\nEpoch 252/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 253/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 254/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 255/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 256/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 257/1000\n4/4 [==============================] - 0s 986us/step - loss: 0.7189\nEpoch 258/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 259/1000\n4/4 [==============================] - 0s 933us/step - loss: 0.7179\nEpoch 260/1000\n4/4 [==============================] - 0s 958us/step - loss: 0.7183\nEpoch 261/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 262/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 263/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 264/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 265/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.7187\nEpoch 266/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 267/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 268/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 269/1000\n4/4 [==============================] - 0s 998us/step - loss: 0.7190\nEpoch 270/1000\n4/4 [==============================] - 0s 942us/step - loss: 0.7190\nEpoch 271/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 272/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 273/1000\n4/4 [==============================] - 0s 811us/step - loss: 0.7188\nEpoch 274/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 275/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 276/1000\n4/4 [==============================] - 0s 774us/step - loss: 0.7186\nEpoch 277/1000\n4/4 [==============================] - 0s 975us/step - loss: 0.7190\nEpoch 278/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.7190\nEpoch 279/1000\n4/4 [==============================] - 0s 856us/step - loss: 0.7186\nEpoch 280/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 281/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 282/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 283/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 284/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 285/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 286/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 287/1000\n4/4 [==============================] - 0s 937us/step - loss: 0.7187\nEpoch 288/1000\n4/4 [==============================] - 0s 841us/step - loss: 0.7189\nEpoch 289/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 290/1000\n4/4 [==============================] - 0s 814us/step - loss: 0.7190\nEpoch 291/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 292/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 293/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 294/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.7186\nEpoch 295/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 296/1000\n4/4 [==============================] - 0s 853us/step - loss: 0.7179\nEpoch 297/1000\n4/4 [==============================] - 0s 921us/step - loss: 0.7188\nEpoch 298/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.7188\nEpoch 299/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 300/1000\n4/4 [==============================] - 0s 968us/step - loss: 0.7183\nEpoch 301/1000\n4/4 [==============================] - 0s 919us/step - loss: 0.7188\nEpoch 302/1000\n4/4 [==============================] - 0s 938us/step - loss: 0.7188\nEpoch 303/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 304/1000\n4/4 [==============================] - 0s 991us/step - loss: 0.7183\nEpoch 305/1000\n4/4 [==============================] - 0s 953us/step - loss: 0.7183\nEpoch 306/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 307/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 308/1000\n4/4 [==============================] - 0s 903us/step - loss: 0.7190\nEpoch 309/1000\n4/4 [==============================] - 0s 870us/step - loss: 0.7190\nEpoch 310/1000\n4/4 [==============================] - 0s 784us/step - loss: 0.7186\nEpoch 311/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 312/1000\n4/4 [==============================] - 0s 858us/step - loss: 0.7183\nEpoch 313/1000\n4/4 [==============================] - 0s 817us/step - loss: 0.7190\nEpoch 314/1000\n4/4 [==============================] - 0s 995us/step - loss: 0.7179\nEpoch 315/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 316/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 317/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.7188\nEpoch 318/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 319/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 320/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 321/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 322/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 323/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 324/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 325/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 326/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 327/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 328/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 329/1000\n4/4 [==============================] - 0s 933us/step - loss: 0.7190\nEpoch 330/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 331/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 332/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 333/1000\n4/4 [==============================] - 0s 979us/step - loss: 0.7186\nEpoch 334/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 335/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 336/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 337/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7179\nEpoch 338/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 339/1000\n4/4 [==============================] - 0s 961us/step - loss: 0.7188\nEpoch 340/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 341/1000\n4/4 [==============================] - 0s 946us/step - loss: 0.7186\nEpoch 342/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 343/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 344/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 345/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 346/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 347/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 348/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 349/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 350/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 351/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7179\nEpoch 352/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7186\nEpoch 353/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 354/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 355/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 356/1000\n4/4 [==============================] - 0s 959us/step - loss: 0.7183\nEpoch 357/1000\n4/4 [==============================] - 0s 944us/step - loss: 0.7183\nEpoch 358/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7179\nEpoch 359/1000\n4/4 [==============================] - 0s 920us/step - loss: 0.7187\nEpoch 360/1000\n4/4 [==============================] - 0s 970us/step - loss: 0.7190\nEpoch 361/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 362/1000\n4/4 [==============================] - 0s 921us/step - loss: 0.7190\nEpoch 363/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 364/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 365/1000\n4/4 [==============================] - 0s 897us/step - loss: 0.7183\nEpoch 366/1000\n4/4 [==============================] - 0s 890us/step - loss: 0.7190\nEpoch 367/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 368/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 369/1000\n4/4 [==============================] - 0s 904us/step - loss: 0.7183\nEpoch 370/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 371/1000\n4/4 [==============================] - 0s 940us/step - loss: 0.7184\nEpoch 372/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 373/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 374/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7186\nEpoch 375/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 376/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 377/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 378/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 379/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.7188\nEpoch 380/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 381/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 382/1000\n4/4 [==============================] - 0s 993us/step - loss: 0.7188\nEpoch 383/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 384/1000\n4/4 [==============================] - 0s 822us/step - loss: 0.7188\nEpoch 385/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.7183\nEpoch 386/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 387/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.7183\nEpoch 388/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 389/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 390/1000\n4/4 [==============================] - 0s 975us/step - loss: 0.7188\nEpoch 391/1000\n4/4 [==============================] - 0s 888us/step - loss: 0.7183\nEpoch 392/1000\n4/4 [==============================] - 0s 983us/step - loss: 0.7187\nEpoch 393/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 394/1000\n4/4 [==============================] - 0s 964us/step - loss: 0.7180\nEpoch 395/1000\n4/4 [==============================] - 0s 791us/step - loss: 0.7183\nEpoch 396/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 397/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 398/1000\n4/4 [==============================] - 0s 946us/step - loss: 0.7187\nEpoch 399/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 400/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7191\nEpoch 401/1000\n4/4 [==============================] - 0s 930us/step - loss: 0.7183\nEpoch 402/1000\n4/4 [==============================] - 0s 960us/step - loss: 0.7187\nEpoch 403/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 404/1000\n4/4 [==============================] - 0s 901us/step - loss: 0.7184\nEpoch 405/1000\n4/4 [==============================] - 0s 823us/step - loss: 0.7188\nEpoch 406/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 407/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 408/1000\n4/4 [==============================] - 0s 795us/step - loss: 0.7184\nEpoch 409/1000\n4/4 [==============================] - 0s 971us/step - loss: 0.7180\nEpoch 410/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 411/1000\n4/4 [==============================] - 0s 779us/step - loss: 0.7180\nEpoch 412/1000\n4/4 [==============================] - 0s 841us/step - loss: 0.7190\nEpoch 413/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 414/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 415/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 416/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 417/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 418/1000\n4/4 [==============================] - 0s 980us/step - loss: 0.7183\nEpoch 419/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 420/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 421/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 422/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 423/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 424/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 425/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7187\nEpoch 426/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 427/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.7180\nEpoch 428/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 429/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 430/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 431/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 432/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 433/1000\n4/4 [==============================] - 0s 975us/step - loss: 0.7186\nEpoch 434/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 435/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 436/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 437/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 438/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 439/1000\n4/4 [==============================] - 0s 984us/step - loss: 0.7186\nEpoch 440/1000\n4/4 [==============================] - 0s 979us/step - loss: 0.7186\nEpoch 441/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 442/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 443/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 444/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 445/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 446/1000\n4/4 [==============================] - 0s 894us/step - loss: 0.7190\nEpoch 447/1000\n4/4 [==============================] - 0s 910us/step - loss: 0.7183\nEpoch 448/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 449/1000\n4/4 [==============================] - 0s 864us/step - loss: 0.7190\nEpoch 450/1000\n4/4 [==============================] - 0s 969us/step - loss: 0.7190\nEpoch 451/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 452/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 453/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 454/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 455/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 456/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 457/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 458/1000\n4/4 [==============================] - 0s 824us/step - loss: 0.7183\nEpoch 459/1000\n4/4 [==============================] - 0s 974us/step - loss: 0.7188\nEpoch 460/1000\n4/4 [==============================] - 0s 853us/step - loss: 0.7183\nEpoch 461/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 462/1000\n4/4 [==============================] - 0s 910us/step - loss: 0.7183\nEpoch 463/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 464/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 465/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 466/1000\n4/4 [==============================] - 0s 975us/step - loss: 0.7190\nEpoch 467/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 468/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 469/1000\n4/4 [==============================] - 0s 954us/step - loss: 0.7190\nEpoch 470/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7187\nEpoch 471/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 472/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.7187\nEpoch 473/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.7191\nEpoch 474/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 475/1000\n4/4 [==============================] - 0s 986us/step - loss: 0.7188\nEpoch 476/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 477/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 478/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 479/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 480/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 481/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.7190\nEpoch 482/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 483/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 484/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 485/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 486/1000\n4/4 [==============================] - 0s 908us/step - loss: 0.7183\nEpoch 487/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 488/1000\n4/4 [==============================] - 0s 933us/step - loss: 0.7186\nEpoch 489/1000\n4/4 [==============================] - 0s 966us/step - loss: 0.7190\nEpoch 490/1000\n4/4 [==============================] - 0s 849us/step - loss: 0.7190\nEpoch 491/1000\n4/4 [==============================] - 0s 771us/step - loss: 0.7190\nEpoch 492/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 493/1000\n4/4 [==============================] - 0s 843us/step - loss: 0.7188\nEpoch 494/1000\n4/4 [==============================] - 0s 829us/step - loss: 0.7183\nEpoch 495/1000\n4/4 [==============================] - 0s 791us/step - loss: 0.7186\nEpoch 496/1000\n4/4 [==============================] - 0s 886us/step - loss: 0.7186\nEpoch 497/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 498/1000\n4/4 [==============================] - 0s 910us/step - loss: 0.7183\nEpoch 499/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 500/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 501/1000\n4/4 [==============================] - 0s 931us/step - loss: 0.7190\nEpoch 502/1000\n4/4 [==============================] - 0s 942us/step - loss: 0.7188\nEpoch 503/1000\n4/4 [==============================] - 0s 997us/step - loss: 0.7190\nEpoch 504/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 505/1000\n4/4 [==============================] - 0s 922us/step - loss: 0.7190\nEpoch 506/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 507/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 508/1000\n4/4 [==============================] - 0s 913us/step - loss: 0.7189\nEpoch 509/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 510/1000\n4/4 [==============================] - 0s 898us/step - loss: 0.7186\nEpoch 511/1000\n4/4 [==============================] - 0s 883us/step - loss: 0.7190\nEpoch 512/1000\n4/4 [==============================] - 0s 791us/step - loss: 0.7190\nEpoch 513/1000\n4/4 [==============================] - 0s 873us/step - loss: 0.7183\nEpoch 514/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 515/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 516/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 517/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 518/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.7188\nEpoch 519/1000\n4/4 [==============================] - 0s 929us/step - loss: 0.7190\nEpoch 520/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 521/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7186\nEpoch 522/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.7180\nEpoch 523/1000\n4/4 [==============================] - 0s 945us/step - loss: 0.7183\nEpoch 524/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 525/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 526/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 527/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 528/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 529/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 530/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 531/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 532/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.7183\nEpoch 533/1000\n4/4 [==============================] - 0s 931us/step - loss: 0.7188\nEpoch 534/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 535/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 536/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.7190\nEpoch 537/1000\n4/4 [==============================] - 0s 994us/step - loss: 0.7183\nEpoch 538/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 539/1000\n4/4 [==============================] - 0s 891us/step - loss: 0.7190\nEpoch 540/1000\n4/4 [==============================] - 0s 968us/step - loss: 0.7190\nEpoch 541/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 542/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 543/1000\n4/4 [==============================] - 0s 900us/step - loss: 0.7183\nEpoch 544/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 545/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 546/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 547/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 548/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 549/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.7183\nEpoch 550/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 551/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 552/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7183\nEpoch 553/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 554/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 555/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 556/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 557/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 558/1000\n4/4 [==============================] - 0s 958us/step - loss: 0.7187\nEpoch 559/1000\n4/4 [==============================] - 0s 968us/step - loss: 0.7190\nEpoch 560/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 561/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7189\nEpoch 562/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 563/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 564/1000\n4/4 [==============================] - 0s 912us/step - loss: 0.7191\nEpoch 565/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 566/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 567/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.7186\nEpoch 568/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7180\nEpoch 569/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 570/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 571/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 572/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 573/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 574/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 575/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 576/1000\n4/4 [==============================] - 0s 837us/step - loss: 0.7190\nEpoch 577/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.7188\nEpoch 578/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 579/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 580/1000\n4/4 [==============================] - 0s 821us/step - loss: 0.7183\nEpoch 581/1000\n4/4 [==============================] - 0s 958us/step - loss: 0.7189\nEpoch 582/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 583/1000\n4/4 [==============================] - 0s 931us/step - loss: 0.7190\nEpoch 584/1000\n4/4 [==============================] - 0s 936us/step - loss: 0.7190\nEpoch 585/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 586/1000\n4/4 [==============================] - 0s 977us/step - loss: 0.7188\nEpoch 587/1000\n4/4 [==============================] - 0s 921us/step - loss: 0.7190\nEpoch 588/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 589/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 590/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 591/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 592/1000\n4/4 [==============================] - 0s 970us/step - loss: 0.7188\nEpoch 593/1000\n4/4 [==============================] - 0s 927us/step - loss: 0.7183\nEpoch 594/1000\n4/4 [==============================] - 0s 844us/step - loss: 0.7189\nEpoch 595/1000\n4/4 [==============================] - 0s 806us/step - loss: 0.7190\nEpoch 596/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 597/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 598/1000\n4/4 [==============================] - 0s 888us/step - loss: 0.7180\nEpoch 599/1000\n4/4 [==============================] - 0s 895us/step - loss: 0.7188\nEpoch 600/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 601/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 602/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 603/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 604/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.7183\nEpoch 605/1000\n4/4 [==============================] - 0s 941us/step - loss: 0.7180\nEpoch 606/1000\n4/4 [==============================] - 0s 974us/step - loss: 0.7190\nEpoch 607/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.7183\nEpoch 608/1000\n4/4 [==============================] - 0s 845us/step - loss: 0.7189\nEpoch 609/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 610/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 611/1000\n4/4 [==============================] - 0s 826us/step - loss: 0.7184\nEpoch 612/1000\n4/4 [==============================] - 0s 813us/step - loss: 0.7191\nEpoch 613/1000\n4/4 [==============================] - 0s 858us/step - loss: 0.7191\nEpoch 614/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 615/1000\n4/4 [==============================] - 0s 906us/step - loss: 0.7183\nEpoch 616/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 617/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 618/1000\n4/4 [==============================] - 0s 930us/step - loss: 0.7186\nEpoch 619/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 620/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 621/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 622/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 623/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 624/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 625/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 626/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 627/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7184\nEpoch 628/1000\n4/4 [==============================] - 0s 988us/step - loss: 0.7188\nEpoch 629/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 630/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 631/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 632/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 633/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7184\nEpoch 634/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 635/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7191\nEpoch 636/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7185\nEpoch 637/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 638/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 639/1000\n4/4 [==============================] - 0s 886us/step - loss: 0.7191\nEpoch 640/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 641/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 642/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 643/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 644/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 645/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 646/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 647/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 648/1000\n4/4 [==============================] - 0s 904us/step - loss: 0.7189\nEpoch 649/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 650/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 651/1000\n4/4 [==============================] - 0s 958us/step - loss: 0.7190\nEpoch 652/1000\n4/4 [==============================] - 0s 990us/step - loss: 0.7191\nEpoch 653/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 654/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 655/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 656/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 657/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 658/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 659/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 660/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 661/1000\n4/4 [==============================] - 0s 995us/step - loss: 0.7186\nEpoch 662/1000\n4/4 [==============================] - 0s 911us/step - loss: 0.7180\nEpoch 663/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 664/1000\n4/4 [==============================] - 0s 950us/step - loss: 0.7188\nEpoch 665/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 666/1000\n4/4 [==============================] - 0s 952us/step - loss: 0.7183\nEpoch 667/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 668/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 669/1000\n4/4 [==============================] - 0s 964us/step - loss: 0.7190\nEpoch 670/1000\n4/4 [==============================] - 0s 877us/step - loss: 0.7190\nEpoch 671/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.7190\nEpoch 672/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 673/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 674/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.7184\nEpoch 675/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 676/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 677/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 678/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 679/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 680/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 681/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 682/1000\n4/4 [==============================] - 0s 929us/step - loss: 0.7187\nEpoch 683/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 684/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 685/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 686/1000\n4/4 [==============================] - 0s 914us/step - loss: 0.7179\nEpoch 687/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 688/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 689/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 690/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 691/1000\n4/4 [==============================] - 0s 976us/step - loss: 0.7190\nEpoch 692/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 693/1000\n4/4 [==============================] - 0s 941us/step - loss: 0.7190\nEpoch 694/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 695/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 696/1000\n4/4 [==============================] - 0s 835us/step - loss: 0.7190\nEpoch 697/1000\n4/4 [==============================] - 0s 984us/step - loss: 0.7188\nEpoch 698/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 699/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 700/1000\n4/4 [==============================] - 0s 813us/step - loss: 0.7190\nEpoch 701/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.7190\nEpoch 702/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 703/1000\n4/4 [==============================] - 0s 911us/step - loss: 0.7190\nEpoch 704/1000\n4/4 [==============================] - 0s 802us/step - loss: 0.7190\nEpoch 705/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 706/1000\n4/4 [==============================] - 0s 975us/step - loss: 0.7183\nEpoch 707/1000\n4/4 [==============================] - 0s 866us/step - loss: 0.7183\nEpoch 708/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.7186\nEpoch 709/1000\n4/4 [==============================] - 0s 796us/step - loss: 0.7186\nEpoch 710/1000\n4/4 [==============================] - 0s 922us/step - loss: 0.7186\nEpoch 711/1000\n4/4 [==============================] - 0s 946us/step - loss: 0.7183\nEpoch 712/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.7188\nEpoch 713/1000\n4/4 [==============================] - 0s 869us/step - loss: 0.7190\nEpoch 714/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 715/1000\n4/4 [==============================] - 0s 988us/step - loss: 0.7180\nEpoch 716/1000\n4/4 [==============================] - 0s 911us/step - loss: 0.7186\nEpoch 717/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.7183\nEpoch 718/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 719/1000\n4/4 [==============================] - 0s 970us/step - loss: 0.7190\nEpoch 720/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 721/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 722/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.7183\nEpoch 723/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 724/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 725/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7179\nEpoch 726/1000\n4/4 [==============================] - 0s 959us/step - loss: 0.7190\nEpoch 727/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 728/1000\n4/4 [==============================] - 0s 936us/step - loss: 0.7180\nEpoch 729/1000\n4/4 [==============================] - 0s 962us/step - loss: 0.7183\nEpoch 730/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 731/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 732/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 733/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 734/1000\n4/4 [==============================] - 0s 926us/step - loss: 0.7188\nEpoch 735/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 736/1000\n4/4 [==============================] - 0s 979us/step - loss: 0.7187\nEpoch 737/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 738/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 739/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 740/1000\n4/4 [==============================] - 0s 869us/step - loss: 0.7185\nEpoch 741/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.7182\nEpoch 742/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 743/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 744/1000\n4/4 [==============================] - 0s 952us/step - loss: 0.7187\nEpoch 745/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7187\nEpoch 746/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 747/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 748/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 749/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 750/1000\n4/4 [==============================] - 0s 924us/step - loss: 0.7191\nEpoch 751/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 752/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 753/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 754/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 755/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7181\nEpoch 756/1000\n4/4 [==============================] - 0s 905us/step - loss: 0.7182\nEpoch 757/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7192\nEpoch 758/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 759/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 760/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 761/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 762/1000\n4/4 [==============================] - 0s 964us/step - loss: 0.7188\nEpoch 763/1000\n4/4 [==============================] - 0s 986us/step - loss: 0.7188\nEpoch 764/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 765/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 766/1000\n4/4 [==============================] - 0s 792us/step - loss: 0.7190\nEpoch 767/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 768/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 769/1000\n4/4 [==============================] - 0s 990us/step - loss: 0.7190\nEpoch 770/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 771/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 772/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 773/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 774/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 775/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.7190\nEpoch 776/1000\n4/4 [==============================] - 0s 820us/step - loss: 0.7186\nEpoch 777/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.7186\nEpoch 778/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 779/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 780/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.7187\nEpoch 781/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 782/1000\n4/4 [==============================] - 0s 936us/step - loss: 0.7189\nEpoch 783/1000\n4/4 [==============================] - 0s 957us/step - loss: 0.7190\nEpoch 784/1000\n4/4 [==============================] - 0s 839us/step - loss: 0.7190\nEpoch 785/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 786/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 787/1000\n4/4 [==============================] - 0s 960us/step - loss: 0.7183\nEpoch 788/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 789/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 790/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 791/1000\n4/4 [==============================] - 0s 919us/step - loss: 0.7183\nEpoch 792/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 793/1000\n4/4 [==============================] - 0s 901us/step - loss: 0.7179\nEpoch 794/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7190\nEpoch 795/1000\n4/4 [==============================] - 0s 996us/step - loss: 0.7190\nEpoch 796/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 797/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 798/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 799/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 800/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 801/1000\n4/4 [==============================] - 0s 876us/step - loss: 0.7190\nEpoch 802/1000\n4/4 [==============================] - 0s 830us/step - loss: 0.7190\nEpoch 803/1000\n4/4 [==============================] - 0s 893us/step - loss: 0.7190\nEpoch 804/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.7186\nEpoch 805/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.7187\nEpoch 806/1000\n4/4 [==============================] - 0s 903us/step - loss: 0.7180\nEpoch 807/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.7191\nEpoch 808/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 809/1000\n4/4 [==============================] - 0s 978us/step - loss: 0.7187\nEpoch 810/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 811/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 812/1000\n4/4 [==============================] - 0s 966us/step - loss: 0.7187\nEpoch 813/1000\n4/4 [==============================] - 0s 870us/step - loss: 0.7187\nEpoch 814/1000\n4/4 [==============================] - 0s 982us/step - loss: 0.7187\nEpoch 815/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 816/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 817/1000\n4/4 [==============================] - 0s 882us/step - loss: 0.7190\nEpoch 818/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 819/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 820/1000\n4/4 [==============================] - 0s 891us/step - loss: 0.7187\nEpoch 821/1000\n4/4 [==============================] - 0s 861us/step - loss: 0.7189\nEpoch 822/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 823/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 824/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 825/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 826/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 827/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 828/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 829/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 830/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 831/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 832/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 833/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 834/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 835/1000\n4/4 [==============================] - 0s 885us/step - loss: 0.7183\nEpoch 836/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 837/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 838/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 839/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7179\nEpoch 840/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 841/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 842/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 843/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 844/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 845/1000\n4/4 [==============================] - 0s 933us/step - loss: 0.7183\nEpoch 846/1000\n4/4 [==============================] - 0s 977us/step - loss: 0.7188\nEpoch 847/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 848/1000\n4/4 [==============================] - 0s 923us/step - loss: 0.7186\nEpoch 849/1000\n4/4 [==============================] - 0s 952us/step - loss: 0.7188\nEpoch 850/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 851/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 852/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 853/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 854/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 855/1000\n4/4 [==============================] - 0s 908us/step - loss: 0.7183\nEpoch 856/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 857/1000\n4/4 [==============================] - 0s 942us/step - loss: 0.7190\nEpoch 858/1000\n4/4 [==============================] - 0s 815us/step - loss: 0.7189\nEpoch 859/1000\n4/4 [==============================] - 0s 3ms/step - loss: 0.7188\nEpoch 860/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 861/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 862/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 863/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 864/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 865/1000\n4/4 [==============================] - 0s 964us/step - loss: 0.7188\nEpoch 866/1000\n4/4 [==============================] - 0s 881us/step - loss: 0.7190\nEpoch 867/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 868/1000\n4/4 [==============================] - 0s 961us/step - loss: 0.7188\nEpoch 869/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 870/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 871/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 872/1000\n4/4 [==============================] - 0s 841us/step - loss: 0.7186\nEpoch 873/1000\n4/4 [==============================] - 0s 902us/step - loss: 0.7186\nEpoch 874/1000\n4/4 [==============================] - 0s 960us/step - loss: 0.7187\nEpoch 875/1000\n4/4 [==============================] - 0s 789us/step - loss: 0.7188\nEpoch 876/1000\n4/4 [==============================] - 0s 810us/step - loss: 0.7190\nEpoch 877/1000\n4/4 [==============================] - 0s 812us/step - loss: 0.7190\nEpoch 878/1000\n4/4 [==============================] - 0s 985us/step - loss: 0.7186\nEpoch 879/1000\n4/4 [==============================] - 0s 832us/step - loss: 0.7183\nEpoch 880/1000\n4/4 [==============================] - 0s 931us/step - loss: 0.7183\nEpoch 881/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 882/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 883/1000\n4/4 [==============================] - 0s 979us/step - loss: 0.7190\nEpoch 884/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.7190\nEpoch 885/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 886/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 887/1000\n4/4 [==============================] - 0s 827us/step - loss: 0.7189\nEpoch 888/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 889/1000\n4/4 [==============================] - 0s 880us/step - loss: 0.7189\nEpoch 890/1000\n4/4 [==============================] - 0s 977us/step - loss: 0.7190\nEpoch 891/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 892/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7184\nEpoch 893/1000\n4/4 [==============================] - 0s 920us/step - loss: 0.7187\nEpoch 894/1000\n4/4 [==============================] - 0s 823us/step - loss: 0.7190\nEpoch 895/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 896/1000\n4/4 [==============================] - 0s 869us/step - loss: 0.7190\nEpoch 897/1000\n4/4 [==============================] - 0s 794us/step - loss: 0.7184\nEpoch 898/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.7189\nEpoch 899/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 900/1000\n4/4 [==============================] - 0s 866us/step - loss: 0.7187\nEpoch 901/1000\n4/4 [==============================] - 0s 777us/step - loss: 0.7183\nEpoch 902/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7189\nEpoch 903/1000\n4/4 [==============================] - 0s 812us/step - loss: 0.7183\nEpoch 904/1000\n4/4 [==============================] - 0s 965us/step - loss: 0.7183\nEpoch 905/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 906/1000\n4/4 [==============================] - 0s 881us/step - loss: 0.7184\nEpoch 907/1000\n4/4 [==============================] - 0s 953us/step - loss: 0.7191\nEpoch 908/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 909/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 910/1000\n4/4 [==============================] - 0s 819us/step - loss: 0.7190\nEpoch 911/1000\n4/4 [==============================] - 0s 798us/step - loss: 0.7188\nEpoch 912/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 913/1000\n4/4 [==============================] - 0s 992us/step - loss: 0.7190\nEpoch 914/1000\n4/4 [==============================] - 0s 860us/step - loss: 0.7190\nEpoch 915/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 916/1000\n4/4 [==============================] - 0s 982us/step - loss: 0.7180\nEpoch 917/1000\n4/4 [==============================] - 0s 865us/step - loss: 0.7190\nEpoch 918/1000\n4/4 [==============================] - 0s 857us/step - loss: 0.7186\nEpoch 919/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 920/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 921/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.7183\nEpoch 922/1000\n4/4 [==============================] - 0s 811us/step - loss: 0.7188\nEpoch 923/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 924/1000\n4/4 [==============================] - 0s 873us/step - loss: 0.7188\nEpoch 925/1000\n4/4 [==============================] - 0s 754us/step - loss: 0.7188\nEpoch 926/1000\n4/4 [==============================] - 0s 933us/step - loss: 0.7186\nEpoch 927/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7180\nEpoch 928/1000\n4/4 [==============================] - 0s 926us/step - loss: 0.7189\nEpoch 929/1000\n4/4 [==============================] - 0s 817us/step - loss: 0.7190\nEpoch 930/1000\n4/4 [==============================] - 0s 929us/step - loss: 0.7187\nEpoch 931/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7189\nEpoch 932/1000\n4/4 [==============================] - 0s 861us/step - loss: 0.7180\nEpoch 933/1000\n4/4 [==============================] - 0s 839us/step - loss: 0.7189\nEpoch 934/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.7184\nEpoch 935/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 936/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 937/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 938/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 939/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 940/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 941/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7190\nEpoch 942/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 943/1000\n4/4 [==============================] - 0s 988us/step - loss: 0.7190\nEpoch 944/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 945/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 946/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7180\nEpoch 947/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 948/1000\n4/4 [==============================] - 0s 978us/step - loss: 0.7190\nEpoch 949/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 950/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7188\nEpoch 951/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 952/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 953/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 954/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 955/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 956/1000\n4/4 [==============================] - 0s 926us/step - loss: 0.7188\nEpoch 957/1000\n4/4 [==============================] - 0s 919us/step - loss: 0.7188\nEpoch 958/1000\n4/4 [==============================] - 0s 876us/step - loss: 0.7189\nEpoch 959/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 960/1000\n4/4 [==============================] - 0s 983us/step - loss: 0.7183\nEpoch 961/1000\n4/4 [==============================] - 0s 916us/step - loss: 0.7190\nEpoch 962/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7186\nEpoch 963/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.7180\nEpoch 964/1000\n4/4 [==============================] - 0s 931us/step - loss: 0.7191\nEpoch 965/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.7184\nEpoch 966/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 967/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 968/1000\n4/4 [==============================] - 0s 945us/step - loss: 0.7180\nEpoch 969/1000\n4/4 [==============================] - 0s 840us/step - loss: 0.7180\nEpoch 970/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7187\nEpoch 971/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7191\nEpoch 972/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 973/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7184\nEpoch 974/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 975/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7187\nEpoch 976/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 977/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 978/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 979/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 980/1000\n4/4 [==============================] - 0s 911us/step - loss: 0.7190\nEpoch 981/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.7190\nEpoch 982/1000\n4/4 [==============================] - 0s 838us/step - loss: 0.7188\nEpoch 983/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 984/1000\n4/4 [==============================] - 0s 900us/step - loss: 0.7190\nEpoch 985/1000\n4/4 [==============================] - 0s 843us/step - loss: 0.7183\nEpoch 986/1000\n4/4 [==============================] - 0s 872us/step - loss: 0.7190\nEpoch 987/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 988/1000\n4/4 [==============================] - 0s 833us/step - loss: 0.7183\nEpoch 989/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7183\nEpoch 990/1000\n4/4 [==============================] - 0s 937us/step - loss: 0.7186\nEpoch 991/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.7190\nEpoch 992/1000\n4/4 [==============================] - 0s 816us/step - loss: 0.7190\nEpoch 993/1000\n4/4 [==============================] - 0s 945us/step - loss: 0.7188\nEpoch 994/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7183\nEpoch 995/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.7190\nEpoch 996/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\nEpoch 997/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7188\nEpoch 998/1000\n4/4 [==============================] - 0s 983us/step - loss: 0.7186\nEpoch 999/1000\n4/4 [==============================] - 0s 895us/step - loss: 0.7180\nEpoch 1000/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7190\n\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n\n\n&lt;keras.callbacks.callbacks.History at 0x7f1732d03310&gt;\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nimport numpy as np \n\nX = np.array([[0,0, 0,0],[0,1, 0,0],[1,0, 0,1],[1,1, 1,1]])\ny = np.array([[0],[1],[1],[0]])\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=4))\nmodel.add(Activation('sigmoid'))\n#model.add(Dense(1))\n#model.add(Activation('sigmoid'))\n\nsgd = SGD(lr=0.1)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\nmodel.fit(X, y,  batch_size=1, nb_epoch=1000)\n\nEpoch 1/1000\n4/4 [==============================] - 0s 11ms/step - loss: 0.8330\nEpoch 2/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.8056\nEpoch 3/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.7829\nEpoch 4/1000\n4/4 [==============================] - 0s 971us/step - loss: 0.7635\nEpoch 5/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7500\nEpoch 6/1000\n4/4 [==============================] - 0s 993us/step - loss: 0.7378\nEpoch 7/1000\n4/4 [==============================] - 0s 969us/step - loss: 0.7275\nEpoch 8/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7169\nEpoch 9/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7081\nEpoch 10/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.7003\nEpoch 11/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.6926\nEpoch 12/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.6860\nEpoch 13/1000\n4/4 [==============================] - 0s 969us/step - loss: 0.6793\nEpoch 14/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.6725\nEpoch 15/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6664\nEpoch 16/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6607\nEpoch 17/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6559\nEpoch 18/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6503\nEpoch 19/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.6435\nEpoch 20/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6396\nEpoch 21/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.6338\nEpoch 22/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6289\nEpoch 23/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.6246\nEpoch 24/1000\n4/4 [==============================] - 0s 826us/step - loss: 0.6192\nEpoch 25/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.6151\nEpoch 26/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6105\nEpoch 27/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6059\nEpoch 28/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.6003\nEpoch 29/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.5972\nEpoch 30/1000\n4/4 [==============================] - 0s 994us/step - loss: 0.5930\nEpoch 31/1000\n4/4 [==============================] - 0s 905us/step - loss: 0.5886\nEpoch 32/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5843\nEpoch 33/1000\n4/4 [==============================] - 0s 995us/step - loss: 0.5798\nEpoch 34/1000\n4/4 [==============================] - 0s 957us/step - loss: 0.5766\nEpoch 35/1000\n4/4 [==============================] - 0s 890us/step - loss: 0.5727\nEpoch 36/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.5688\nEpoch 37/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.5657\nEpoch 38/1000\n4/4 [==============================] - 0s 895us/step - loss: 0.5617\nEpoch 39/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.5581\nEpoch 40/1000\n4/4 [==============================] - 0s 862us/step - loss: 0.5547\nEpoch 41/1000\n4/4 [==============================] - 0s 964us/step - loss: 0.5510\nEpoch 42/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.5472\nEpoch 43/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5440\nEpoch 44/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5410\nEpoch 45/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5373\nEpoch 46/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.5346\nEpoch 47/1000\n4/4 [==============================] - 0s 960us/step - loss: 0.5311\nEpoch 48/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5272\nEpoch 49/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5245\nEpoch 50/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.5215\nEpoch 51/1000\n4/4 [==============================] - 0s 937us/step - loss: 0.5188\nEpoch 52/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.5153\nEpoch 53/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.5125\nEpoch 54/1000\n4/4 [==============================] - 0s 833us/step - loss: 0.5091\nEpoch 55/1000\n4/4 [==============================] - 0s 868us/step - loss: 0.5065\nEpoch 56/1000\n4/4 [==============================] - 0s 825us/step - loss: 0.5034\nEpoch 57/1000\n4/4 [==============================] - 0s 892us/step - loss: 0.5012\nEpoch 58/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4985\nEpoch 59/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.4956\nEpoch 60/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4928\nEpoch 61/1000\n4/4 [==============================] - 0s 877us/step - loss: 0.4897\nEpoch 62/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4872\nEpoch 63/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.4847\nEpoch 64/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4822\nEpoch 65/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4796\nEpoch 66/1000\n4/4 [==============================] - ETA: 0s - loss: 0.481 - 0s 2ms/step - loss: 0.4768\nEpoch 67/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4744\nEpoch 68/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.4715\nEpoch 69/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4694\nEpoch 70/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4663\nEpoch 71/1000\n4/4 [==============================] - 0s 886us/step - loss: 0.4644\nEpoch 72/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4616\nEpoch 73/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4596\nEpoch 74/1000\n4/4 [==============================] - 0s 884us/step - loss: 0.4572\nEpoch 75/1000\n4/4 [==============================] - 0s 976us/step - loss: 0.4549\nEpoch 76/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4524\nEpoch 77/1000\n4/4 [==============================] - 0s 892us/step - loss: 0.4501\nEpoch 78/1000\n4/4 [==============================] - 0s 788us/step - loss: 0.4477\nEpoch 79/1000\n4/4 [==============================] - 0s 966us/step - loss: 0.4453\nEpoch 80/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.4432\nEpoch 81/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4409\nEpoch 82/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.4386\nEpoch 83/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4366\nEpoch 84/1000\n4/4 [==============================] - ETA: 0s - loss: 0.438 - 0s 915us/step - loss: 0.4345\nEpoch 85/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4323\nEpoch 86/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4299\nEpoch 87/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4280\nEpoch 88/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4258\nEpoch 89/1000\n4/4 [==============================] - 0s 956us/step - loss: 0.4238\nEpoch 90/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4218\nEpoch 91/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4196\nEpoch 92/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4175\nEpoch 93/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4157\nEpoch 94/1000\n4/4 [==============================] - 0s 993us/step - loss: 0.4137\nEpoch 95/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4116\nEpoch 96/1000\n4/4 [==============================] - 0s 987us/step - loss: 0.4097\nEpoch 97/1000\n4/4 [==============================] - 0s 862us/step - loss: 0.4075\nEpoch 98/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4054\nEpoch 99/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.4039\nEpoch 100/1000\n4/4 [==============================] - 0s 865us/step - loss: 0.4020\nEpoch 101/1000\n4/4 [==============================] - 0s 859us/step - loss: 0.4000\nEpoch 102/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3982\nEpoch 103/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3961\nEpoch 104/1000\n4/4 [==============================] - 0s 853us/step - loss: 0.3945\nEpoch 105/1000\n4/4 [==============================] - 0s 922us/step - loss: 0.3925\nEpoch 106/1000\n4/4 [==============================] - 0s 885us/step - loss: 0.3906\nEpoch 107/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3890\nEpoch 108/1000\n4/4 [==============================] - 0s 822us/step - loss: 0.3872\nEpoch 109/1000\n4/4 [==============================] - 0s 861us/step - loss: 0.3853\nEpoch 110/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3836\nEpoch 111/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3819\nEpoch 112/1000\n4/4 [==============================] - 0s 902us/step - loss: 0.3802\nEpoch 113/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3785\nEpoch 114/1000\n4/4 [==============================] - 0s 813us/step - loss: 0.3767\nEpoch 115/1000\n4/4 [==============================] - 0s 802us/step - loss: 0.3747\nEpoch 116/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3732\nEpoch 117/1000\n4/4 [==============================] - 0s 853us/step - loss: 0.3716\nEpoch 118/1000\n4/4 [==============================] - 0s 951us/step - loss: 0.3697\nEpoch 119/1000\n4/4 [==============================] - 0s 806us/step - loss: 0.3682\nEpoch 120/1000\n4/4 [==============================] - 0s 995us/step - loss: 0.3667\nEpoch 121/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3648\nEpoch 122/1000\n4/4 [==============================] - 0s 943us/step - loss: 0.3635\nEpoch 123/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3619\nEpoch 124/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3603\nEpoch 125/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3586\nEpoch 126/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3572\nEpoch 127/1000\n4/4 [==============================] - 0s 953us/step - loss: 0.3555\nEpoch 128/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3541\nEpoch 129/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3526\nEpoch 130/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3509\nEpoch 131/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3493\nEpoch 132/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3481\nEpoch 133/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3466\nEpoch 134/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3449\nEpoch 135/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3436\nEpoch 136/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3422\nEpoch 137/1000\n4/4 [==============================] - ETA: 0s - loss: 0.346 - 0s 1ms/step - loss: 0.3407\nEpoch 138/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3393\nEpoch 139/1000\n4/4 [==============================] - 0s 965us/step - loss: 0.3377\nEpoch 140/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3364\nEpoch 141/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3349\nEpoch 142/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3335\nEpoch 143/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3320\nEpoch 144/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.3306\nEpoch 145/1000\n4/4 [==============================] - 0s 980us/step - loss: 0.3294\nEpoch 146/1000\n4/4 [==============================] - 0s 998us/step - loss: 0.3279\nEpoch 147/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.3267\nEpoch 148/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.3255\nEpoch 149/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3239\nEpoch 150/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3228\nEpoch 151/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.3215\nEpoch 152/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3200\nEpoch 153/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3189\nEpoch 154/1000\n4/4 [==============================] - 0s 986us/step - loss: 0.3176\nEpoch 155/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3164\nEpoch 156/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3150\nEpoch 157/1000\n4/4 [==============================] - 0s 868us/step - loss: 0.3136\nEpoch 158/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3125\nEpoch 159/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3111\nEpoch 160/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3101\nEpoch 161/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3088\nEpoch 162/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3075\nEpoch 163/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3063\nEpoch 164/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3052\nEpoch 165/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3040\nEpoch 166/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3028\nEpoch 167/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3017\nEpoch 168/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.3004\nEpoch 169/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2993\nEpoch 170/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2982\nEpoch 171/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2969\nEpoch 172/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2957\nEpoch 173/1000\n4/4 [==============================] - 0s 945us/step - loss: 0.2948\nEpoch 174/1000\n4/4 [==============================] - 0s 968us/step - loss: 0.2935\nEpoch 175/1000\n4/4 [==============================] - 0s 966us/step - loss: 0.2925\nEpoch 176/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2914\nEpoch 177/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2902\nEpoch 178/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2891\nEpoch 179/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2880\nEpoch 180/1000\n4/4 [==============================] - 0s 953us/step - loss: 0.2871\nEpoch 181/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.2858\nEpoch 182/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2849\nEpoch 183/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2837\nEpoch 184/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2826\nEpoch 185/1000\n4/4 [==============================] - 0s 907us/step - loss: 0.2817\nEpoch 186/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2806\nEpoch 187/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.2796\nEpoch 188/1000\n4/4 [==============================] - 0s 956us/step - loss: 0.2786\nEpoch 189/1000\n4/4 [==============================] - 0s 946us/step - loss: 0.2775\nEpoch 190/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2764\nEpoch 191/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.2755\nEpoch 192/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2745\nEpoch 193/1000\n4/4 [==============================] - 0s 940us/step - loss: 0.2736\nEpoch 194/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2726\nEpoch 195/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2716\nEpoch 196/1000\n4/4 [==============================] - 0s 943us/step - loss: 0.2706\nEpoch 197/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2696\nEpoch 198/1000\n4/4 [==============================] - 0s 971us/step - loss: 0.2687\nEpoch 199/1000\n4/4 [==============================] - 0s 979us/step - loss: 0.2677\nEpoch 200/1000\n4/4 [==============================] - 0s 884us/step - loss: 0.2666\nEpoch 201/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2658\nEpoch 202/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2649\nEpoch 203/1000\n4/4 [==============================] - 0s 988us/step - loss: 0.2639\nEpoch 204/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2630\nEpoch 205/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.2621\nEpoch 206/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2612\nEpoch 207/1000\n4/4 [==============================] - 0s 894us/step - loss: 0.2602\nEpoch 208/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2593\nEpoch 209/1000\n4/4 [==============================] - 0s 978us/step - loss: 0.2584\nEpoch 210/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2576\nEpoch 211/1000\n4/4 [==============================] - 0s 816us/step - loss: 0.2566\nEpoch 212/1000\n4/4 [==============================] - 0s 752us/step - loss: 0.2558\nEpoch 213/1000\n4/4 [==============================] - 0s 905us/step - loss: 0.2549\nEpoch 214/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2540\nEpoch 215/1000\n4/4 [==============================] - 0s 855us/step - loss: 0.2531\nEpoch 216/1000\n4/4 [==============================] - 0s 828us/step - loss: 0.2522\nEpoch 217/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2514\nEpoch 218/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2505\nEpoch 219/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2497\nEpoch 220/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2489\nEpoch 221/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2480\nEpoch 222/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2471\nEpoch 223/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2464\nEpoch 224/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2455\nEpoch 225/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2446\nEpoch 226/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2438\nEpoch 227/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2431\nEpoch 228/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2422\nEpoch 229/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2415\nEpoch 230/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2406\nEpoch 231/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2399\nEpoch 232/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2390\nEpoch 233/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2383\nEpoch 234/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2375\nEpoch 235/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2367\nEpoch 236/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2359\nEpoch 237/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2352\nEpoch 238/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2345\nEpoch 239/1000\n4/4 [==============================] - 0s 827us/step - loss: 0.2337\nEpoch 240/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2329\nEpoch 241/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2322\nEpoch 242/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2315\nEpoch 243/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2307\nEpoch 244/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2299\nEpoch 245/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2292\nEpoch 246/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2285\nEpoch 247/1000\n4/4 [==============================] - 0s 967us/step - loss: 0.2278\nEpoch 248/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.2271\nEpoch 249/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2264\nEpoch 250/1000\n4/4 [==============================] - 0s 976us/step - loss: 0.2256\nEpoch 251/1000\n4/4 [==============================] - 0s 995us/step - loss: 0.2249\nEpoch 252/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2242\nEpoch 253/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2235\nEpoch 254/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2228\nEpoch 255/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2221\nEpoch 256/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2215\nEpoch 257/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2208\nEpoch 258/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2200\nEpoch 259/1000\n4/4 [==============================] - 0s 952us/step - loss: 0.2194\nEpoch 260/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.2187\nEpoch 261/1000\n4/4 [==============================] - 0s 843us/step - loss: 0.2181\nEpoch 262/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2174\nEpoch 263/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2167\nEpoch 264/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2161\nEpoch 265/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2154\nEpoch 266/1000\n4/4 [==============================] - 0s 987us/step - loss: 0.2148\nEpoch 267/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2141\nEpoch 268/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2135\nEpoch 269/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2129\nEpoch 270/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2122\nEpoch 271/1000\n4/4 [==============================] - 0s 834us/step - loss: 0.2116\nEpoch 272/1000\n4/4 [==============================] - 0s 814us/step - loss: 0.2110\nEpoch 273/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2103\nEpoch 274/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.2097\nEpoch 275/1000\n4/4 [==============================] - 0s 846us/step - loss: 0.2091\nEpoch 276/1000\n4/4 [==============================] - 0s 804us/step - loss: 0.2084\nEpoch 277/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2079\nEpoch 278/1000\n4/4 [==============================] - 0s 784us/step - loss: 0.2072\nEpoch 279/1000\n4/4 [==============================] - 0s 879us/step - loss: 0.2066\nEpoch 280/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2060\nEpoch 281/1000\n4/4 [==============================] - 0s 914us/step - loss: 0.2054\nEpoch 282/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2048\nEpoch 283/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.2042\nEpoch 284/1000\n4/4 [==============================] - 0s 881us/step - loss: 0.2037\nEpoch 285/1000\n4/4 [==============================] - 0s 872us/step - loss: 0.2030\nEpoch 286/1000\n4/4 [==============================] - ETA: 0s - loss: 0.207 - 0s 1ms/step - loss: 0.2025\nEpoch 287/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2019\nEpoch 288/1000\n4/4 [==============================] - 0s 827us/step - loss: 0.2013\nEpoch 289/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2007\nEpoch 290/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.2002\nEpoch 291/1000\n4/4 [==============================] - 0s 996us/step - loss: 0.1996\nEpoch 292/1000\n4/4 [==============================] - 0s 826us/step - loss: 0.1990\nEpoch 293/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1984\nEpoch 294/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1979\nEpoch 295/1000\n4/4 [==============================] - 0s 821us/step - loss: 0.1973\nEpoch 296/1000\n4/4 [==============================] - 0s 795us/step - loss: 0.1968\nEpoch 297/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1962\nEpoch 298/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1957\nEpoch 299/1000\n4/4 [==============================] - 0s 839us/step - loss: 0.1951\nEpoch 300/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1945\nEpoch 301/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1940\nEpoch 302/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1935\nEpoch 303/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1930\nEpoch 304/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1924\nEpoch 305/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1919\nEpoch 306/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1913\nEpoch 307/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1909\nEpoch 308/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1903\nEpoch 309/1000\n4/4 [==============================] - 0s 970us/step - loss: 0.1898\nEpoch 310/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1893\nEpoch 311/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1888\nEpoch 312/1000\n4/4 [==============================] - 0s 908us/step - loss: 0.1882\nEpoch 313/1000\n4/4 [==============================] - 0s 947us/step - loss: 0.1877\nEpoch 314/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1872\nEpoch 315/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1867\nEpoch 316/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1862\nEpoch 317/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1857\nEpoch 318/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1852\nEpoch 319/1000\n4/4 [==============================] - 0s 931us/step - loss: 0.1847\nEpoch 320/1000\n4/4 [==============================] - 0s 881us/step - loss: 0.1842\nEpoch 321/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1837\nEpoch 322/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1832\nEpoch 323/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.1828\nEpoch 324/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1822\nEpoch 325/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1818\nEpoch 326/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1813\nEpoch 327/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1808\nEpoch 328/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1803\nEpoch 329/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1799\nEpoch 330/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1794\nEpoch 331/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1789\nEpoch 332/1000\n4/4 [==============================] - 0s 976us/step - loss: 0.1785\nEpoch 333/1000\n4/4 [==============================] - 0s 962us/step - loss: 0.1780\nEpoch 334/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1775\nEpoch 335/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.1771\nEpoch 336/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1766\nEpoch 337/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1762\nEpoch 338/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1757\nEpoch 339/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1753\nEpoch 340/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1748\nEpoch 341/1000\n4/4 [==============================] - 0s 857us/step - loss: 0.1744\nEpoch 342/1000\n4/4 [==============================] - 0s 974us/step - loss: 0.1739\nEpoch 343/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1735\nEpoch 344/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1731\nEpoch 345/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1726\nEpoch 346/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1722\nEpoch 347/1000\n4/4 [==============================] - 0s 833us/step - loss: 0.1717\nEpoch 348/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1713\nEpoch 349/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1709\nEpoch 350/1000\n4/4 [==============================] - 0s 943us/step - loss: 0.1705\nEpoch 351/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1700\nEpoch 352/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1696\nEpoch 353/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1692\nEpoch 354/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1687\nEpoch 355/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1683\nEpoch 356/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1679\nEpoch 357/1000\n4/4 [==============================] - 0s 976us/step - loss: 0.1675\nEpoch 358/1000\n4/4 [==============================] - 0s 840us/step - loss: 0.1671\nEpoch 359/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1667\nEpoch 360/1000\n4/4 [==============================] - 0s 895us/step - loss: 0.1663\nEpoch 361/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1659\nEpoch 362/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1655\nEpoch 363/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1651\nEpoch 364/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1647\nEpoch 365/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1642\nEpoch 366/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1638\nEpoch 367/1000\n4/4 [==============================] - 0s 941us/step - loss: 0.1634\nEpoch 368/1000\n4/4 [==============================] - 0s 948us/step - loss: 0.1631\nEpoch 369/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1627\nEpoch 370/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1623\nEpoch 371/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1619\nEpoch 372/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1615\nEpoch 373/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1611\nEpoch 374/1000\n4/4 [==============================] - 0s 943us/step - loss: 0.1607\nEpoch 375/1000\n4/4 [==============================] - 0s 912us/step - loss: 0.1603\nEpoch 376/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1600\nEpoch 377/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1596\nEpoch 378/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1592\nEpoch 379/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1588\nEpoch 380/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1585\nEpoch 381/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1581\nEpoch 382/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1577\nEpoch 383/1000\n4/4 [==============================] - 0s 921us/step - loss: 0.1573\nEpoch 384/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.1570\nEpoch 385/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1566\nEpoch 386/1000\n4/4 [==============================] - 0s 998us/step - loss: 0.1562\nEpoch 387/1000\n4/4 [==============================] - 0s 833us/step - loss: 0.1559\nEpoch 388/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1555\nEpoch 389/1000\n4/4 [==============================] - 0s 967us/step - loss: 0.1551\nEpoch 390/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1548\nEpoch 391/1000\n4/4 [==============================] - 0s 991us/step - loss: 0.1544\nEpoch 392/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1541\nEpoch 393/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1537\nEpoch 394/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1534\nEpoch 395/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1530\nEpoch 396/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1527\nEpoch 397/1000\n4/4 [==============================] - 0s 966us/step - loss: 0.1523\nEpoch 398/1000\n4/4 [==============================] - 0s 847us/step - loss: 0.1520\nEpoch 399/1000\n4/4 [==============================] - 0s 889us/step - loss: 0.1516\nEpoch 400/1000\n4/4 [==============================] - 0s 997us/step - loss: 0.1513\nEpoch 401/1000\n4/4 [==============================] - 0s 869us/step - loss: 0.1509\nEpoch 402/1000\n4/4 [==============================] - 0s 987us/step - loss: 0.1506\nEpoch 403/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1503\nEpoch 404/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1499\nEpoch 405/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1496\nEpoch 406/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1492\nEpoch 407/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1489\nEpoch 408/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1486\nEpoch 409/1000\n4/4 [==============================] - 0s 842us/step - loss: 0.1483\nEpoch 410/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1479\nEpoch 411/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1476\nEpoch 412/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1472\nEpoch 413/1000\n4/4 [==============================] - 0s 923us/step - loss: 0.1469\nEpoch 414/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1466\nEpoch 415/1000\n4/4 [==============================] - 0s 965us/step - loss: 0.1463\nEpoch 416/1000\n4/4 [==============================] - 0s 848us/step - loss: 0.1460\nEpoch 417/1000\n4/4 [==============================] - 0s 1000us/step - loss: 0.1457\nEpoch 418/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1453\nEpoch 419/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1450\nEpoch 420/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1447\nEpoch 421/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1444\nEpoch 422/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1441\nEpoch 423/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1438\nEpoch 424/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1434\nEpoch 425/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1431\nEpoch 426/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1428\nEpoch 427/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1425\nEpoch 428/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1422\nEpoch 429/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1419\nEpoch 430/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1416\nEpoch 431/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1413\nEpoch 432/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1410\nEpoch 433/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1407\nEpoch 434/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1404\nEpoch 435/1000\n4/4 [==============================] - 0s 991us/step - loss: 0.1401\nEpoch 436/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1398\nEpoch 437/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1395\nEpoch 438/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1392\nEpoch 439/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1389\nEpoch 440/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1386\nEpoch 441/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1383\nEpoch 442/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1380\nEpoch 443/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1377\nEpoch 444/1000\n4/4 [==============================] - 0s 972us/step - loss: 0.1374\nEpoch 445/1000\n4/4 [==============================] - 0s 939us/step - loss: 0.1372\nEpoch 446/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1369\nEpoch 447/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1366\nEpoch 448/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1363\nEpoch 449/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1360\nEpoch 450/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1358\nEpoch 451/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1355\nEpoch 452/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1352\nEpoch 453/1000\n4/4 [==============================] - 0s 945us/step - loss: 0.1349\nEpoch 454/1000\n4/4 [==============================] - 0s 965us/step - loss: 0.1346\nEpoch 455/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1344\nEpoch 456/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1341\nEpoch 457/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1338\nEpoch 458/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1335\nEpoch 459/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1332\nEpoch 460/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1330\nEpoch 461/1000\n4/4 [==============================] - 0s 992us/step - loss: 0.1327\nEpoch 462/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1324\nEpoch 463/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1322\nEpoch 464/1000\n4/4 [==============================] - 0s 967us/step - loss: 0.1319\nEpoch 465/1000\n4/4 [==============================] - 0s 949us/step - loss: 0.1316\nEpoch 466/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1314\nEpoch 467/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.1311\nEpoch 468/1000\n4/4 [==============================] - 0s 937us/step - loss: 0.1309\nEpoch 469/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1306\nEpoch 470/1000\n4/4 [==============================] - ETA: 0s - loss: 0.186 - 0s 1ms/step - loss: 0.1304\nEpoch 471/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1301\nEpoch 472/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1298\nEpoch 473/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.1295\nEpoch 474/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.1293\nEpoch 475/1000\n4/4 [==============================] - 0s 926us/step - loss: 0.1291\nEpoch 476/1000\n4/4 [==============================] - 0s 954us/step - loss: 0.1288\nEpoch 477/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1285\nEpoch 478/1000\n4/4 [==============================] - 0s 990us/step - loss: 0.1283\nEpoch 479/1000\n4/4 [==============================] - 0s 807us/step - loss: 0.1280\nEpoch 480/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.1278\nEpoch 481/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1275\nEpoch 482/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1273\nEpoch 483/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1270\nEpoch 484/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1268\nEpoch 485/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1266\nEpoch 486/1000\n4/4 [==============================] - 0s 800us/step - loss: 0.1263\nEpoch 487/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1261\nEpoch 488/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1258\nEpoch 489/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.1256\nEpoch 490/1000\n4/4 [==============================] - 0s 942us/step - loss: 0.1253\nEpoch 491/1000\n4/4 [==============================] - 0s 944us/step - loss: 0.1251\nEpoch 492/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1248\nEpoch 493/1000\n4/4 [==============================] - 0s 883us/step - loss: 0.1246\nEpoch 494/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1244\nEpoch 495/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1241\nEpoch 496/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1239\nEpoch 497/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1237\nEpoch 498/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1234\nEpoch 499/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.1232\nEpoch 500/1000\n4/4 [==============================] - 0s 795us/step - loss: 0.1229\nEpoch 501/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1227\nEpoch 502/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1225\nEpoch 503/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1223\nEpoch 504/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1220\nEpoch 505/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1218\nEpoch 506/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1216\nEpoch 507/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1213\nEpoch 508/1000\n4/4 [==============================] - 0s 940us/step - loss: 0.1211\nEpoch 509/1000\n4/4 [==============================] - 0s 943us/step - loss: 0.1209\nEpoch 510/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1207\nEpoch 511/1000\n4/4 [==============================] - 0s 912us/step - loss: 0.1204\nEpoch 512/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1202\nEpoch 513/1000\n4/4 [==============================] - 0s 866us/step - loss: 0.1200\nEpoch 514/1000\n4/4 [==============================] - 0s 876us/step - loss: 0.1198\nEpoch 515/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1195\nEpoch 516/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1193\nEpoch 517/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1191\nEpoch 518/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1189\nEpoch 519/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1187\nEpoch 520/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1184\nEpoch 521/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.1182\nEpoch 522/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1180\nEpoch 523/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1178\nEpoch 524/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1176\nEpoch 525/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1174\nEpoch 526/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1171\nEpoch 527/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1169\nEpoch 528/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1167\nEpoch 529/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1165\nEpoch 530/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1163\nEpoch 531/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1161\nEpoch 532/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1159\nEpoch 533/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1157\nEpoch 534/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.1155\nEpoch 535/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1153\nEpoch 536/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1151\nEpoch 537/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1149\nEpoch 538/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1147\nEpoch 539/1000\n4/4 [==============================] - ETA: 0s - loss: 0.075 - 0s 988us/step - loss: 0.1144\nEpoch 540/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1142\nEpoch 541/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1140\nEpoch 542/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1138\nEpoch 543/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1136\nEpoch 544/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1134\nEpoch 545/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1132\nEpoch 546/1000\n4/4 [==============================] - 0s 983us/step - loss: 0.1130\nEpoch 547/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1128\nEpoch 548/1000\n4/4 [==============================] - 0s 929us/step - loss: 0.1126\nEpoch 549/1000\n4/4 [==============================] - 0s 933us/step - loss: 0.1124\nEpoch 550/1000\n4/4 [==============================] - 0s 942us/step - loss: 0.1122\nEpoch 551/1000\n4/4 [==============================] - 0s 969us/step - loss: 0.1120\nEpoch 552/1000\n4/4 [==============================] - 0s 871us/step - loss: 0.1118\nEpoch 553/1000\n4/4 [==============================] - 0s 809us/step - loss: 0.1117\nEpoch 554/1000\n4/4 [==============================] - 0s 997us/step - loss: 0.1115\nEpoch 555/1000\n4/4 [==============================] - 0s 949us/step - loss: 0.1113\nEpoch 556/1000\n4/4 [==============================] - 0s 871us/step - loss: 0.1111\nEpoch 557/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.1109\nEpoch 558/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1107\nEpoch 559/1000\n4/4 [==============================] - 0s 965us/step - loss: 0.1105\nEpoch 560/1000\n4/4 [==============================] - 0s 895us/step - loss: 0.1103\nEpoch 561/1000\n4/4 [==============================] - 0s 880us/step - loss: 0.1101\nEpoch 562/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1099\nEpoch 563/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1097\nEpoch 564/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1096\nEpoch 565/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1094\nEpoch 566/1000\n4/4 [==============================] - 0s 816us/step - loss: 0.1092\nEpoch 567/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1090\nEpoch 568/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1088\nEpoch 569/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1086\nEpoch 570/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1084\nEpoch 571/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1083\nEpoch 572/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1081\nEpoch 573/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1079\nEpoch 574/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1077\nEpoch 575/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1075\nEpoch 576/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1073\nEpoch 577/1000\n4/4 [==============================] - 0s 962us/step - loss: 0.1072\nEpoch 578/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1070\nEpoch 579/1000\n4/4 [==============================] - 0s 984us/step - loss: 0.1068\nEpoch 580/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1066\nEpoch 581/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1065\nEpoch 582/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1063\nEpoch 583/1000\n4/4 [==============================] - 0s 951us/step - loss: 0.1061\nEpoch 584/1000\n4/4 [==============================] - 0s 954us/step - loss: 0.1059\nEpoch 585/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1057\nEpoch 586/1000\n4/4 [==============================] - 0s 899us/step - loss: 0.1056\nEpoch 587/1000\n4/4 [==============================] - 0s 951us/step - loss: 0.1054\nEpoch 588/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1052\nEpoch 589/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1051\nEpoch 590/1000\n4/4 [==============================] - 0s 920us/step - loss: 0.1049\nEpoch 591/1000\n4/4 [==============================] - 0s 924us/step - loss: 0.1047\nEpoch 592/1000\n4/4 [==============================] - 0s 850us/step - loss: 0.1045\nEpoch 593/1000\n4/4 [==============================] - 0s 873us/step - loss: 0.1044\nEpoch 594/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1042\nEpoch 595/1000\n4/4 [==============================] - 0s 866us/step - loss: 0.1040\nEpoch 596/1000\n4/4 [==============================] - 0s 854us/step - loss: 0.1039\nEpoch 597/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.1037\nEpoch 598/1000\n4/4 [==============================] - 0s 804us/step - loss: 0.1035\nEpoch 599/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.1033\nEpoch 600/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1032\nEpoch 601/1000\n4/4 [==============================] - 0s 821us/step - loss: 0.1030\nEpoch 602/1000\n4/4 [==============================] - 0s 831us/step - loss: 0.1028\nEpoch 603/1000\n4/4 [==============================] - 0s 903us/step - loss: 0.1027\nEpoch 604/1000\n4/4 [==============================] - 0s 780us/step - loss: 0.1025\nEpoch 605/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.1023\nEpoch 606/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1022\nEpoch 607/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1020\nEpoch 608/1000\n4/4 [==============================] - 0s 864us/step - loss: 0.1018\nEpoch 609/1000\n4/4 [==============================] - 0s 799us/step - loss: 0.1017\nEpoch 610/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1015\nEpoch 611/1000\n4/4 [==============================] - 0s 972us/step - loss: 0.1014\nEpoch 612/1000\n4/4 [==============================] - 0s 816us/step - loss: 0.1012\nEpoch 613/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1010\nEpoch 614/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1009\nEpoch 615/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1007\nEpoch 616/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1006\nEpoch 617/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1004\nEpoch 618/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1002\nEpoch 619/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.1001\nEpoch 620/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0999\nEpoch 621/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0998\nEpoch 622/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0996\nEpoch 623/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0995\nEpoch 624/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0993\nEpoch 625/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0991\nEpoch 626/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0990\nEpoch 627/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0988\nEpoch 628/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0987\nEpoch 629/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.0985\nEpoch 630/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0984\nEpoch 631/1000\n4/4 [==============================] - 0s 968us/step - loss: 0.0982\nEpoch 632/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0981\nEpoch 633/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0979\nEpoch 634/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0978\nEpoch 635/1000\n4/4 [==============================] - 0s 891us/step - loss: 0.0976\nEpoch 636/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0975\nEpoch 637/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0973\nEpoch 638/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0972\nEpoch 639/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0970\nEpoch 640/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0969\nEpoch 641/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0967\nEpoch 642/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0966\nEpoch 643/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0964\nEpoch 644/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0963\nEpoch 645/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0961\nEpoch 646/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0960\nEpoch 647/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0958\nEpoch 648/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0957\nEpoch 649/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0955\nEpoch 650/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0954\nEpoch 651/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0953\nEpoch 652/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0951\nEpoch 653/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0950\nEpoch 654/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0948\nEpoch 655/1000\n4/4 [==============================] - 0s 922us/step - loss: 0.0947\nEpoch 656/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0945\nEpoch 657/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0944\nEpoch 658/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0943\nEpoch 659/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0941\nEpoch 660/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0940\nEpoch 661/1000\n4/4 [==============================] - 0s 942us/step - loss: 0.0938\nEpoch 662/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0937\nEpoch 663/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0936\nEpoch 664/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0934\nEpoch 665/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0933\nEpoch 666/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0931\nEpoch 667/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.0930\nEpoch 668/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0929\nEpoch 669/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0927\nEpoch 670/1000\n4/4 [==============================] - 0s 928us/step - loss: 0.0926\nEpoch 671/1000\n4/4 [==============================] - 0s 836us/step - loss: 0.0925\nEpoch 672/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0923\nEpoch 673/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.0922\nEpoch 674/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.0921\nEpoch 675/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0919\nEpoch 676/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0918\nEpoch 677/1000\n4/4 [==============================] - 0s 964us/step - loss: 0.0916\nEpoch 678/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0915\nEpoch 679/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0914\nEpoch 680/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0912\nEpoch 681/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0911\nEpoch 682/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0910\nEpoch 683/1000\n4/4 [==============================] - 0s 952us/step - loss: 0.0909\nEpoch 684/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0907\nEpoch 685/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0906\nEpoch 686/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0905\nEpoch 687/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0903\nEpoch 688/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0902\nEpoch 689/1000\n4/4 [==============================] - 0s 869us/step - loss: 0.0901\nEpoch 690/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0899\nEpoch 691/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0898\nEpoch 692/1000\n4/4 [==============================] - 0s 997us/step - loss: 0.0897\nEpoch 693/1000\n4/4 [==============================] - 0s 909us/step - loss: 0.0896\nEpoch 694/1000\n4/4 [==============================] - 0s 830us/step - loss: 0.0894\nEpoch 695/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0893\nEpoch 696/1000\n4/4 [==============================] - 0s 894us/step - loss: 0.0892\nEpoch 697/1000\n4/4 [==============================] - 0s 830us/step - loss: 0.0890\nEpoch 698/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0889\nEpoch 699/1000\n4/4 [==============================] - 0s 936us/step - loss: 0.0888\nEpoch 700/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0887\nEpoch 701/1000\n4/4 [==============================] - 0s 860us/step - loss: 0.0885\nEpoch 702/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0884\nEpoch 703/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0883\nEpoch 704/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0882\nEpoch 705/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0880\nEpoch 706/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0879\nEpoch 707/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0878\nEpoch 708/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0877\nEpoch 709/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.0876\nEpoch 710/1000\n4/4 [==============================] - 0s 825us/step - loss: 0.0874\nEpoch 711/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0873\nEpoch 712/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0872\nEpoch 713/1000\n4/4 [==============================] - 0s 914us/step - loss: 0.0871\nEpoch 714/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0869\nEpoch 715/1000\n4/4 [==============================] - 0s 949us/step - loss: 0.0868\nEpoch 716/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0867\nEpoch 717/1000\n4/4 [==============================] - 0s 892us/step - loss: 0.0866\nEpoch 718/1000\n4/4 [==============================] - 0s 886us/step - loss: 0.0865\nEpoch 719/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0863\nEpoch 720/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0862\nEpoch 721/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0861\nEpoch 722/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0860\nEpoch 723/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0859\nEpoch 724/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0858\nEpoch 725/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0856\nEpoch 726/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0855\nEpoch 727/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0854\nEpoch 728/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0853\nEpoch 729/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0852\nEpoch 730/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0851\nEpoch 731/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0849\nEpoch 732/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0848\nEpoch 733/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0847\nEpoch 734/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0846\nEpoch 735/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0845\nEpoch 736/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0844\nEpoch 737/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0843\nEpoch 738/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0841\nEpoch 739/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0840\nEpoch 740/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0839\nEpoch 741/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0838\nEpoch 742/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0837\nEpoch 743/1000\n4/4 [==============================] - 0s 953us/step - loss: 0.0836\nEpoch 744/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0835\nEpoch 745/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0833\nEpoch 746/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0832\nEpoch 747/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0831\nEpoch 748/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0830\nEpoch 749/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0829\nEpoch 750/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0828\nEpoch 751/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0827\nEpoch 752/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0826\nEpoch 753/1000\n4/4 [==============================] - 0s 884us/step - loss: 0.0825\nEpoch 754/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0824\nEpoch 755/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0822\nEpoch 756/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0821\nEpoch 757/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0820\nEpoch 758/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0819\nEpoch 759/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0818\nEpoch 760/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0817\nEpoch 761/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0816\nEpoch 762/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0815\nEpoch 763/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0814\nEpoch 764/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0813\nEpoch 765/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0812\nEpoch 766/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0811\nEpoch 767/1000\n4/4 [==============================] - 0s 911us/step - loss: 0.0810\nEpoch 768/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0809\nEpoch 769/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0808\nEpoch 770/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0807\nEpoch 771/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0805\nEpoch 772/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0804\nEpoch 773/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0803\nEpoch 774/1000\n4/4 [==============================] - 0s 919us/step - loss: 0.0802\nEpoch 775/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0801\nEpoch 776/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0800\nEpoch 777/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0799\nEpoch 778/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0798\nEpoch 779/1000\n4/4 [==============================] - 0s 916us/step - loss: 0.0797\nEpoch 780/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0796\nEpoch 781/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0795\nEpoch 782/1000\n4/4 [==============================] - 0s 980us/step - loss: 0.0794\nEpoch 783/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0793\nEpoch 784/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0792\nEpoch 785/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0791\nEpoch 786/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0790\nEpoch 787/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0789\nEpoch 788/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0788\nEpoch 789/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0787\nEpoch 790/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0786\nEpoch 791/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0785\nEpoch 792/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0784\nEpoch 793/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0783\nEpoch 794/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0782\nEpoch 795/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0781\nEpoch 796/1000\n4/4 [==============================] - 0s 995us/step - loss: 0.0780\nEpoch 797/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.0779\nEpoch 798/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0778\nEpoch 799/1000\n4/4 [==============================] - 0s 860us/step - loss: 0.0777\nEpoch 800/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0776\nEpoch 801/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0775\nEpoch 802/1000\n4/4 [==============================] - 0s 867us/step - loss: 0.0774\nEpoch 803/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0773\nEpoch 804/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0772\nEpoch 805/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0771\nEpoch 806/1000\n4/4 [==============================] - 0s 875us/step - loss: 0.0770\nEpoch 807/1000\n4/4 [==============================] - 0s 826us/step - loss: 0.0769\nEpoch 808/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0769\nEpoch 809/1000\n4/4 [==============================] - 0s 925us/step - loss: 0.0768\nEpoch 810/1000\n4/4 [==============================] - 0s 885us/step - loss: 0.0767\nEpoch 811/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0766\nEpoch 812/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0765\nEpoch 813/1000\n4/4 [==============================] - 0s 987us/step - loss: 0.0764\nEpoch 814/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0763\nEpoch 815/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0762\nEpoch 816/1000\n4/4 [==============================] - 0s 958us/step - loss: 0.0761\nEpoch 817/1000\n4/4 [==============================] - 0s 848us/step - loss: 0.0760\nEpoch 818/1000\n4/4 [==============================] - 0s 789us/step - loss: 0.0759\nEpoch 819/1000\n4/4 [==============================] - 0s 861us/step - loss: 0.0758\nEpoch 820/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0757\nEpoch 821/1000\n4/4 [==============================] - 0s 980us/step - loss: 0.0756\nEpoch 822/1000\n4/4 [==============================] - 0s 951us/step - loss: 0.0755\nEpoch 823/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0754\nEpoch 824/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0754\nEpoch 825/1000\n4/4 [==============================] - 0s 959us/step - loss: 0.0753\nEpoch 826/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0752\nEpoch 827/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0751\nEpoch 828/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0750\nEpoch 829/1000\n4/4 [==============================] - 0s 994us/step - loss: 0.0749\nEpoch 830/1000\n4/4 [==============================] - 0s 892us/step - loss: 0.0748\nEpoch 831/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0747\nEpoch 832/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0746\nEpoch 833/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0745\nEpoch 834/1000\n4/4 [==============================] - 0s 989us/step - loss: 0.0744\nEpoch 835/1000\n4/4 [==============================] - 0s 915us/step - loss: 0.0744\nEpoch 836/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0743\nEpoch 837/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0742\nEpoch 838/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0741\nEpoch 839/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0740\nEpoch 840/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0739\nEpoch 841/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0738\nEpoch 842/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0737\nEpoch 843/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0737\nEpoch 844/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0736\nEpoch 845/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0735\nEpoch 846/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0734\nEpoch 847/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0733\nEpoch 848/1000\n4/4 [==============================] - 0s 923us/step - loss: 0.0732\nEpoch 849/1000\n4/4 [==============================] - 0s 965us/step - loss: 0.0731\nEpoch 850/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0730\nEpoch 851/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0730\nEpoch 852/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0729\nEpoch 853/1000\n4/4 [==============================] - 0s 896us/step - loss: 0.0728\nEpoch 854/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0727\nEpoch 855/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0726\nEpoch 856/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0725\nEpoch 857/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0724\nEpoch 858/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0724\nEpoch 859/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0723\nEpoch 860/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0722\nEpoch 861/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0721\nEpoch 862/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0720\nEpoch 863/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0719\nEpoch 864/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0719\nEpoch 865/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0718\nEpoch 866/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0717\nEpoch 867/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0716\nEpoch 868/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0715\nEpoch 869/1000\n4/4 [==============================] - 0s 822us/step - loss: 0.0714\nEpoch 870/1000\n4/4 [==============================] - 0s 857us/step - loss: 0.0713\nEpoch 871/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0713\nEpoch 872/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0712\nEpoch 873/1000\n4/4 [==============================] - 0s 773us/step - loss: 0.0711\nEpoch 874/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0710\nEpoch 875/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0709\nEpoch 876/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0709\nEpoch 877/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0708\nEpoch 878/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0707\nEpoch 879/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0706\nEpoch 880/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0705\nEpoch 881/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0705\nEpoch 882/1000\n4/4 [==============================] - 0s 930us/step - loss: 0.0704\nEpoch 883/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0703\nEpoch 884/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0702\nEpoch 885/1000\n4/4 [==============================] - 0s 903us/step - loss: 0.0701\nEpoch 886/1000\n4/4 [==============================] - 0s 779us/step - loss: 0.0701\nEpoch 887/1000\n4/4 [==============================] - 0s 865us/step - loss: 0.0700\nEpoch 888/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0699\nEpoch 889/1000\n4/4 [==============================] - 0s 934us/step - loss: 0.0698\nEpoch 890/1000\n4/4 [==============================] - 0s 999us/step - loss: 0.0697\nEpoch 891/1000\n4/4 [==============================] - 0s 945us/step - loss: 0.0697\nEpoch 892/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0696\nEpoch 893/1000\n4/4 [==============================] - 0s 948us/step - loss: 0.0695\nEpoch 894/1000\n4/4 [==============================] - ETA: 0s - loss: 0.097 - 0s 1ms/step - loss: 0.0694\nEpoch 895/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0693\nEpoch 896/1000\n4/4 [==============================] - 0s 954us/step - loss: 0.0693\nEpoch 897/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0692\nEpoch 898/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0691\nEpoch 899/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0690\nEpoch 900/1000\n4/4 [==============================] - 0s 955us/step - loss: 0.0689\nEpoch 901/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0689\nEpoch 902/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0688\nEpoch 903/1000\n4/4 [==============================] - 0s 797us/step - loss: 0.0687\nEpoch 904/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0686\nEpoch 905/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0686\nEpoch 906/1000\n4/4 [==============================] - 0s 898us/step - loss: 0.0685\nEpoch 907/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0684\nEpoch 908/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0683\nEpoch 909/1000\n4/4 [==============================] - 0s 996us/step - loss: 0.0683\nEpoch 910/1000\n4/4 [==============================] - 0s 866us/step - loss: 0.0682\nEpoch 911/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0681\nEpoch 912/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0680\nEpoch 913/1000\n4/4 [==============================] - 0s 910us/step - loss: 0.0680\nEpoch 914/1000\n4/4 [==============================] - 0s 932us/step - loss: 0.0679\nEpoch 915/1000\n4/4 [==============================] - 0s 799us/step - loss: 0.0678\nEpoch 916/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0677\nEpoch 917/1000\n4/4 [==============================] - 0s 830us/step - loss: 0.0677\nEpoch 918/1000\n4/4 [==============================] - 0s 876us/step - loss: 0.0676\nEpoch 919/1000\n4/4 [==============================] - 0s 876us/step - loss: 0.0675\nEpoch 920/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0674\nEpoch 921/1000\n4/4 [==============================] - 0s 978us/step - loss: 0.0674\nEpoch 922/1000\n4/4 [==============================] - 0s 781us/step - loss: 0.0673\nEpoch 923/1000\n4/4 [==============================] - 0s 940us/step - loss: 0.0672\nEpoch 924/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0671\nEpoch 925/1000\n4/4 [==============================] - 0s 816us/step - loss: 0.0671\nEpoch 926/1000\n4/4 [==============================] - 0s 824us/step - loss: 0.0670\nEpoch 927/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0669\nEpoch 928/1000\n4/4 [==============================] - 0s 855us/step - loss: 0.0668\nEpoch 929/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0668\nEpoch 930/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0667\nEpoch 931/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0666\nEpoch 932/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0666\nEpoch 933/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0665\nEpoch 934/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0664\nEpoch 935/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0663\nEpoch 936/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0663\nEpoch 937/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0662\nEpoch 938/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0661\nEpoch 939/1000\n4/4 [==============================] - 0s 917us/step - loss: 0.0661\nEpoch 940/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0660\nEpoch 941/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0659\nEpoch 942/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0658\nEpoch 943/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0658\nEpoch 944/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0657\nEpoch 945/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0656\nEpoch 946/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0656\nEpoch 947/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0655\nEpoch 948/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0654\nEpoch 949/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0654\nEpoch 950/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0653\nEpoch 951/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0652\nEpoch 952/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0651\nEpoch 953/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0651\nEpoch 954/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0650\nEpoch 955/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0649\nEpoch 956/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0649\nEpoch 957/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0648\nEpoch 958/1000\n4/4 [==============================] - 0s 849us/step - loss: 0.0647\nEpoch 959/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0647\nEpoch 960/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0646\nEpoch 961/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0645\nEpoch 962/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0645\nEpoch 963/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0644\nEpoch 964/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0643\nEpoch 965/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0643\nEpoch 966/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0642\nEpoch 967/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0641\nEpoch 968/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0641\nEpoch 969/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0640\nEpoch 970/1000\n4/4 [==============================] - 0s 963us/step - loss: 0.0639\nEpoch 971/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0639\nEpoch 972/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0638\nEpoch 973/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0637\nEpoch 974/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0637\nEpoch 975/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0636\nEpoch 976/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0635\nEpoch 977/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0635\nEpoch 978/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0634\nEpoch 979/1000\n4/4 [==============================] - 0s 878us/step - loss: 0.0633\nEpoch 980/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0633\nEpoch 981/1000\n4/4 [==============================] - ETA: 0s - loss: 0.042 - 0s 1ms/step - loss: 0.0632\nEpoch 982/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0631\nEpoch 983/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0631\nEpoch 984/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0630\nEpoch 985/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0629\nEpoch 986/1000\n4/4 [==============================] - 0s 2ms/step - loss: 0.0629\nEpoch 987/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0628\nEpoch 988/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0627\nEpoch 989/1000\n4/4 [==============================] - 0s 832us/step - loss: 0.0627\nEpoch 990/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0626\nEpoch 991/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0625\nEpoch 992/1000\n4/4 [==============================] - 0s 801us/step - loss: 0.0625\nEpoch 993/1000\n4/4 [==============================] - 0s 897us/step - loss: 0.0624\nEpoch 994/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0624\nEpoch 995/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0623\nEpoch 996/1000\n4/4 [==============================] - 0s 930us/step - loss: 0.0622\nEpoch 997/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0622\nEpoch 998/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0621\nEpoch 999/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0620\nEpoch 1000/1000\n4/4 [==============================] - 0s 1ms/step - loss: 0.0620\n\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n\n\n&lt;keras.callbacks.callbacks.History at 0x7f1622c6e210&gt;\n\n\n\nmodel.get_weights()\n\n[array([[  2.0119758],\n        [  5.108962 ],\n        [-11.173091 ],\n        [  3.2969708]], dtype=float32), array([-2.396014], dtype=float32)]"
  },
  {
    "objectID": "notebooks/ridge-calculation.html",
    "href": "notebooks/ridge-calculation.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\n\n\nmu = 100\n\n\nX = np.array([[1, 1], [1, 2]])\n\n\ny = np.array([1, 2])-1.5\n\n\nX.T@X\n\narray([[2, 3],\n       [3, 5]])\n\n\n\nI_star = np.eye(2, 2)\n\n\n#I_star[0, 0] = 0.\n\n\nnp.linalg.inv(X.T@X + mu*I_star)@(X.T@y)\n\narray([-0.00014017,  0.00476591])\n\n\n\nX = np.array([[1, 1, 2], [1, 2, 4], [1, 3, 6]])\n\n\nX.T@X\n\narray([[ 3,  6, 12],\n       [ 6, 14, 28],\n       [12, 28, 56]])"
  },
  {
    "objectID": "notebooks/Linear Regression Notebook.html",
    "href": "notebooks/Linear Regression Notebook.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "notebooks/gym.html",
    "href": "notebooks/gym.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "%pip install gym\n\nCollecting gym\n  Downloading gym-0.18.0.tar.gz (1.6 MB)\n     |████████████████████████████████| 1.6 MB 3.4 MB/s eta 0:00:01\nRequirement already satisfied: scipy in /usr/local/anaconda3/lib/python3.8/site-packages (from gym) (1.5.0)\nRequirement already satisfied: numpy&gt;=1.10.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from gym) (1.18.5)\nCollecting pyglet&lt;=1.5.0,&gt;=1.4.0\n  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n     |████████████████████████████████| 1.0 MB 4.5 MB/s eta 0:00:01\nRequirement already satisfied: Pillow&lt;=7.2.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from gym) (7.2.0)\nRequirement already satisfied: cloudpickle&lt;1.7.0,&gt;=1.2.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from gym) (1.5.0)\nRequirement already satisfied: future in /usr/local/anaconda3/lib/python3.8/site-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym) (0.18.2)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (setup.py) ... done\n  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656450 sha256=e214d705fc702140c22f2108ea689891952646c1adee4140d10132d22b088ed0\n  Stored in directory: /Users/nipun/Library/Caches/pip/wheels/d8/e7/68/a3f0f1b5831c9321d7523f6fd4e0d3f83f2705a1cbd5daaa79\nSuccessfully built gym\nInstalling collected packages: pyglet, gym\nSuccessfully installed gym-0.18.0 pyglet-1.5.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport gym\n\n\nenv = gym.make??\n\n\nSignature: gym.make(id, **kwargs)\nDocstring: &lt;no docstring&gt;\nSource:   \ndef make(id, **kwargs):\n    return registry.make(id, **kwargs)\nFile:      /usr/local/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\nType:      function\n\n\n\n\n\nenv = gym.make\n\n\nenv = gym.make\n\n\nenv = gym.make\n\n\nenv = gym.make\n\n\nenv = gym.make\n\n\nenv = gym.make(\"CartPole-v1\")\n\n\nenv.action_space\n\nDiscrete(2)\n\n\n\nenv.reward_range\n\n(-inf, inf)\n\n\n\nenv.reset()\n\narray([-0.04141238,  0.02879778,  0.04665526, -0.03694293])\n\n\n\ndone = False\nwhile not done:\n    action = 1\n    new_state, reward, done, info = env.step(action)\n    env.render()\n\n\n\n\nTrue\n\n\n\nimport pygame\n\nModuleNotFoundError: No module named 'pygame'\n\n\n\n%pip install pygame\n\nCollecting pygame\n  Downloading pygame-2.0.1-cp38-cp38-macosx_10_9_intel.whl (6.9 MB)\n     |████████████████████████████████| 6.9 MB 924 kB/s eta 0:00:01\nInstalling collected packages: pygame\nSuccessfully installed pygame-2.0.1\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "notebooks/decision-tree-real-output.html",
    "href": "notebooks/decision-tree-real-output.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-real-output.csv\", index_col=[0])\n\n\ndf\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWind\nMinutes Played\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\n20\n\n\nD2\nSunny\nHot\nHigh\nStrong\n24\n\n\nD3\nOvercast\nHot\nHigh\nWeak\n40\n\n\nD4\nRain\nMild\nHigh\nWeak\n50\n\n\nD5\nRain\nCool\nNormal\nWeak\n60\n\n\nD6\nRain\nCool\nNormal\nStrong\n10\n\n\nD7\nOvercast\nCool\nNormal\nStrong\n4\n\n\nD8\nSunny\nMild\nHigh\nWeak\n10\n\n\nD9\nSunny\nCool\nNormal\nWeak\n60\n\n\nD10\nRain\nMild\nNormal\nWeak\n40\n\n\nD11\nSunny\nMild\nHigh\nStrong\n45\n\n\nD12\nOvercast\nMild\nHigh\nStrong\n40\n\n\nD13\nOvercast\nHot\nNormal\nWeak\n35\n\n\nD14\nRain\nMild\nHigh\nStrong\n20\n\n\n\n\n\n\n\n\nmean_mins = df[\"Minutes Played\"].mean()\nprint(mean_mins)\n\n32.714285714285715\n\n\n\ninitial_mse = ((df[\"Minutes Played\"] - mean_mins) ** 2).mean()\nprint(initial_mse)\n\n311.3469387755102\n\n\n\n# Explore MSE for different splits based on the \"Outlook\" attribute\nweighted_total_mse = 0.0\nfor category in df[\"Wind\"].unique():\n    subset = df[df[\"Wind\"] == category]\n    \n    # Calculate MSE for the subset\n    mse_subset = ((subset[\"Minutes Played\"] - subset[\"Minutes Played\"].mean()) ** 2).mean()\n    \n    # Calculate the weighted MSE\n    weighted_mse = (len(subset) / len(df)) * mse_subset\n    weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    print(subset[\"Minutes Played\"].values)\n    print(f\"Wind: {category}\")\n    print(\"Subset MSE:\", mse_subset)\n    print(f\"Weighted MSE = {len(subset)}/{len(df)} * {mse_subset:0.4} = {weighted_mse:0.4}\")\n    print(\"\\n\")\n\nprint(\"Weighted total MSE:\", weighted_total_mse)\n\n[20 40 50 60 10 60 40 35]\nWind: Weak\nSubset MSE: 277.734375\nWeighted MSE = 8/14 * 277.7 = 158.7\n\n\n[24 10  4 45 40 20]\nWind: Strong\nSubset MSE: 218.13888888888889\nWeighted MSE = 6/14 * 218.1 = 93.49\n\n\nWeighted total MSE: 252.19345238095235\n\n\n\nreduction_mse_wind = initial_mse - weighted_total_mse\nprint(reduction_mse_wind)\n\n59.15348639455783\n\n\n\ndef reduction_mse(df_dataset, input_attribute, target_attribute):\n    # Calculate the initial MSE\n    mean_target = df_dataset[target_attribute].mean()\n    initial_mse = ((df_dataset[target_attribute] - mean_target) ** 2).mean()\n    weighted_total_mse = 0.0\n\n    for category in df_dataset[input_attribute].unique():\n        subset = df_dataset[df_dataset[input_attribute] == category]\n        mse_subset = ((subset[target_attribute] - subset[target_attribute].mean()) ** 2).mean()\n        \n        weighted_mse = (len(subset) / len(df_dataset)) * mse_subset\n        weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    return initial_mse - weighted_total_mse\n\n    \n\n\nreduction = {}\nfor attribute in [\"Outlook\", \"Temp\", \"Humidity\", \"Wind\"]:\n    reduction[attribute] = reduction_mse(df, attribute, \"Minutes Played\")\n    \nreduction_ser = pd.Series(reduction)\n\n\nlatexify()\n\n\nbars = reduction_ser.plot(kind='bar', rot=0, color='k')\nformat_axes(plt.gca())\n\n# Add values on top of the bars\nfor bar in bars.patches:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\nplt.xlabel(\"Attribute\")\nplt.ylabel(\"Reduction in MSE\")\nplt.savefig(\"../figures/decision-trees/discrete-input-real-output-level-1.pdf\")\n\n\n\n\n\ndf.groupby(\"Outlook\").groupsa\n\nAttributeError: 'DataFrameGroupBy' object has no attribute 'groupsa'\n\n\n\nfrom latexify import latexify, format_axes\nfrom scipy.special import xlogy\n\n# Function to calculate entropy\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Generate data\nx_values = np.linspace(0.000, 1.0, 100)  # Avoid log(0) in the calculation\ny_values = entropy(x_values)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_19079/845472961.py:6: RuntimeWarning: divide by zero encountered in log2\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_19079/845472961.py:6: RuntimeWarning: invalid value encountered in multiply\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\n\ny_values\n\narray([       nan, 0.08146203, 0.14257333, 0.19590927, 0.24414164,\n       0.28853851, 0.32984607, 0.36855678, 0.40502013, 0.43949699,\n       0.47218938, 0.50325833, 0.53283506, 0.56102849, 0.58793037,\n       0.61361902, 0.63816195, 0.66161791, 0.68403844, 0.70546904,\n       0.72595015, 0.74551784, 0.76420451, 0.78203929, 0.79904852,\n       0.81525608, 0.83068364, 0.84535094, 0.85927598, 0.87247521,\n       0.88496364, 0.89675502, 0.90786192, 0.91829583, 0.92806728,\n       0.93718586, 0.9456603 , 0.95349858, 0.9607079 , 0.96729478,\n       0.97326507, 0.97862399, 0.98337619, 0.98752571, 0.99107606,\n       0.99403021, 0.99639062, 0.99815923, 0.9993375 , 0.9999264 ,\n       0.9999264 , 0.9993375 , 0.99815923, 0.99639062, 0.99403021,\n       0.99107606, 0.98752571, 0.98337619, 0.97862399, 0.97326507,\n       0.96729478, 0.9607079 , 0.95349858, 0.9456603 , 0.93718586,\n       0.92806728, 0.91829583, 0.90786192, 0.89675502, 0.88496364,\n       0.87247521, 0.85927598, 0.84535094, 0.83068364, 0.81525608,\n       0.79904852, 0.78203929, 0.76420451, 0.74551784, 0.72595015,\n       0.70546904, 0.68403844, 0.66161791, 0.63816195, 0.61361902,\n       0.58793037, 0.56102849, 0.53283506, 0.50325833, 0.47218938,\n       0.43949699, 0.40502013, 0.36855678, 0.32984607, 0.28853851,\n       0.24414164, 0.19590927, 0.14257333, 0.08146203,        nan])\n\n\n\n# Replace NaN values with 0\ny_values = np.nan_to_num(y_values, nan=0.0)\n\n\nlatexify(columns=1)\n\n\nplt.plot(x_values, y_values, color='black')\n\n# Set labels and title\nplt.xlabel('$P(+)$')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. $P(+)$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/entropy.pdf\")\n\n\n\n\n\n# Function to calculate entropy with numerical stability\ndef entropy_numerically_stable(p):\n    return (-xlogy(p, p) - xlogy(1 - p, 1 - p))/np.log(2)\n\ny_values = entropy_numerically_stable(x_values)\n\n\nplt.plot(x_values, y_values)\n\n\n\n\nHow does xlogy handle the corner case?\n\nxlogy??\n\nCall signature:  xlogy(*args, **kwargs)\nType:            ufunc\nString form:     &lt;ufunc 'xlogy'&gt;\nFile:            ~/miniconda3/lib/python3.9/site-packages/numpy/__init__.py\nDocstring:      \nxlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nxlogy(x, y, out=None)\n\nCompute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n\nParameters\n----------\nx : array_like\n    Multiplier\ny : array_like\n    Argument\nout : ndarray, optional\n    Optional output array for the function results\n\nReturns\n-------\nz : scalar or ndarray\n    Computed x*log(y)\n\nNotes\n-----\nThe log function used in the computation is the natural log.\n\n.. versionadded:: 0.13.0\n\nExamples\n--------\nWe can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n.. math::\n    L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i))\n\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.special import xlogy\n&gt;&gt;&gt; y = np.array([0, 1, 0, 1, 1, 0])\n&gt;&gt;&gt; y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\n&gt;&gt;&gt; n = len(y)\n&gt;&gt;&gt; loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\n&gt;&gt;&gt; loss /= n\n&gt;&gt;&gt; loss\n0.29597052165495025\n\nA lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilties are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.\nClass docstring:\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\n**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays."
  },
  {
    "objectID": "notebooks/Maths for ML-2.html",
    "href": "notebooks/Maths for ML-2.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n&lt;matplotlib.colors.LinearSegmentedColormap at 0x24c0bddf978&gt;\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2 + Y**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Filled Contours Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-1.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-2.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(np.abs(X) + np.abs(Y))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-3.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-2.0, 8.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(Y*(X**2))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-4.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X*Y)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-5.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\n# Contour Plot\nX, Y = np.mgrid[-1:1:100j, -1:1:100j]\nZ = X**2 + Y**2 \ncp = plt.contour(X, Y, Z)\n# cb = plt.colorbar(cp)\n\n# Vector Field\nY, X = np.mgrid[-1:1:30j, -1:1:30j]\nU = 2*X\nV = 2*Y\nspeed = np.sqrt(U**2 + V**2)\nUN = U/speed\nVN = V/speed\nquiv = plt.quiver(X, Y, UN, VN,  # assign to var\n           color='Teal', \n           headlength=7)\nplt.savefig(\"gradient-field.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncp = ax.contour(X, Y, Z,levels=[1,2,3,4,5,6,7,8,9])\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(0.11,4.5,600)\nplt.plot(d,0.5/d)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-6.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nfor i in np.linspace(0,2,100):\n    print (i,0.5 - i**2 + 1/i**2)\n\n0.0 inf\n0.020202020202020204 2450.7495918783793\n0.04040404040404041 613.0608675135189\n0.06060606060606061 272.74632690541773\n0.08080808080808081 153.63409505407608\n0.10101010101010102 98.4997969594939\n0.12121212121212122 68.54780762167124\n0.14141414141414144 50.4851040814244\n0.16161616161616163 38.75903646630445\n0.18181818181818182 30.71694214876033\n0.20202020202020204 24.96168783797571\n0.22222222222222224 20.700617283950617\n0.24242424242424243 17.45685548668503\n0.26262626262626265 14.929548156238129\n0.2828282828282829 12.92128367263648\n0.30303030303030304 11.298172635445361\n0.32323232323232326 9.966809927717833\n0.3434343434343435 8.860426554171964\n0.36363636363636365 7.930268595041323\n0.38383838383838387 7.1400642169759925\n0.4040404040404041 6.462376351902866\n0.42424242424242425 5.876140814452502\n0.4444444444444445 5.364969135802469\n0.4646464646464647 4.9159562148764175\n0.48484848484848486 4.518828196740127\n0.5050505050505051 4.165323987348229\n0.5252525252525253 3.848739962230637\n0.5454545454545455 3.5635904499540856\n0.5656565656565657 3.305351527280638\n0.5858585858585859 3.0702655556635308\n0.6060606060606061 2.8551905417814507\n0.6262626262626263 2.65748294812874\n0.6464646464646465 2.474905726496339\n0.6666666666666667 2.305555555555555\n0.686868686868687 2.1478048326048214\n0.7070707070707072 2.0002550968351827\n0.7272727272727273 1.8616993801652892\n0.7474747474747475 1.7310915822381832\n0.7676767676767677 1.6075214108402638\n0.787878787878788 1.4901937611727818\n0.8080808080808082 1.3784116576114678\n0.8282828282828284 1.27156207154134\n0.8484848484848485 1.1691040741365415\n0.8686868686868687 1.0705588948578604\n0.888888888888889 0.9755015432098765\n0.9090909090909092 0.8835537190082641\n0.9292929292929294 0.7943777895623855\n0.9494949494949496 0.7076716541475031\n0.9696969696969697 0.6231643494605139\n0.98989898989899 0.5406122763442311\n1.0101010101010102 0.4597959493929188\n1.0303030303030305 0.3805171882397417\n1.0505050505050506 0.302596683242079\n1.0707070707070707 0.22587187959584054\n1.090909090909091 0.15019513314967814\n1.1111111111111112 0.07543209876543189\n1.1313131313131315 0.0014603183062319447\n1.1515151515151516 -0.07183201951522311\n1.1717171717171717 -0.14454717092494984\n1.191919191919192 -0.2167788004559923\n1.2121212121212122 -0.2886128328741967\n1.2323232323232325 -0.36012820815496915\n1.2525252525252526 -0.4313975519179223\n1.272727272727273 -0.5024877719682923\n1.292929292929293 -0.5734605901083917\n1.3131313131313131 -0.6443730171236\n1.3333333333333335 -0.7152777777777782\n1.3535353535353536 -0.7862236917418948\n1.373737373737374 -0.8572560156014735\n1.393939393939394 -0.9284167504222499\n1.4141414141414144 -0.9997449187817161\n1.4343434343434345 -1.0712768146824043\n1.4545454545454546 -1.143046229338843\n1.474747474747475 -1.2150846554637709\n1.494949494949495 -1.2874214723620951\n1.5151515151515154 -1.3600841138659328\n1.5353535353535355 -1.4330982209048717\n1.5555555555555556 -1.5064877802973042\n1.575757575757576 -1.58027525116686\n1.595959595959596 -1.6544816802290594\n1.6161616161616164 -1.729126807054128\n1.6363636363636365 -1.8042291602897669\n1.6565656565656568 -1.8798061457204203\n1.676767676767677 -1.9558741269450486\n1.696969696969697 -2.032448499372201\n1.7171717171717173 -2.1095437581575784\n1.7373737373737375 -2.187173560644274\n1.7575757575757578 -2.2653507838082487\n1.777777777777778 -2.344087577160494\n1.7979797979797982 -2.423395411511965\n1.8181818181818183 -2.503285123966943\n1.8383838383838385 -2.583766959474586\n1.8585858585858588 -2.664850609236279\n1.878787878787879 -2.7465452462378015\n1.8989898989898992 -2.828859558149688\n1.9191919191919193 -2.9118017778162164\n1.9393939393939394 -2.9953797115329435\n1.9595959595959598 -3.079600765294187\n1.97979797979798 -3.1644719691753447\n2.0 -3.25\n\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n  \n\n\n\nxlist = np.linspace(-2.0, 2.0, 100)\nylist = np.linspace(-2.0, 2.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncontours = ax.contour(X, Y, Z,levels=[0.1,0.2,.3,.4,.5,.6,.7,.8,.9,1])#,levels=[.5**(.5),2,3,4,5,6,7,8,9])\nplt.clabel(contours, inline=True, fontsize=8)\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(-4,5,600)\nplt.plot(d,1-d)\nplt.xlim(-1,1)\nplt.ylim(-1,1)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-7.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n(2*0.5**2)**(.5)\n\n0.7071067811865476"
  },
  {
    "objectID": "bayesian/readme.html",
    "href": "bayesian/readme.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "Slides for Bayesian Machine Learning"
  },
  {
    "objectID": "ml-maths/Maths for ML-2.html",
    "href": "ml-maths/Maths for ML-2.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n&lt;matplotlib.colors.LinearSegmentedColormap at 0x7f01d40eb590&gt;\n\n\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height &gt; MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'text.latex.preamble': ['\\\\usepackage{gensymb}'],\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'font.size': 8, # was 10\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': True,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\nlatexify()\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2 + Y**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title(r'$x^2+y^2=K$')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nax.set_aspect(\"equal\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nformat_axes(ax)\nplt.savefig(\"contour-plot-1.pdf\",transparent=True,bbox_inches=\"tight\")\n#plt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title(r'$x^2=K$')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nax.set_aspect(\"equal\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nformat_axes(ax)\nplt.savefig(\"contour-plot-2.pdf\",transparent=True,bbox_inches=\"tight\")\n#plt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(np.abs(X) + np.abs(Y))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-3.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-2.0, 8.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(Y*(X**2))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-4.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\envs\\nilmtk_env\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X*Y)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-5.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\envs\\nilmtk_env\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\n# Contour Plot\nX, Y = np.mgrid[-1:1:100j, -1:1:100j]\nZ = X**2 + Y**2 \ncp = plt.contour(X, Y, Z)\n# cb = plt.colorbar(cp)\n\n# Vector Field\nY, X = np.mgrid[-1:1:30j, -1:1:30j]\nU = 2*X\nV = 2*Y\nspeed = np.sqrt(U**2 + V**2)\nUN = U/speed\nVN = V/speed\nquiv = plt.quiver(X, Y, UN, VN,  # assign to var\n           color='Teal', \n           headlength=7)\nplt.savefig(\"gradient-field.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z,levels=[1,4,9])\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-6.eps\", format='eps',transparent=True)\nd = np.linspace(0.1,6,600)\nplt.plot(d,0.5/d)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\n\nplt.show()\n\n\n\n\n\nfor i in np.linspace(0,2,100):\n    print (i,0.5 - i**2 + 1/i**2)\n\n0.0 inf\n0.020202020202020204 2450.7495918783793\n0.04040404040404041 613.0608675135189\n0.06060606060606061 272.74632690541773\n0.08080808080808081 153.63409505407608\n0.10101010101010102 98.4997969594939\n0.12121212121212122 68.54780762167124\n0.14141414141414144 50.4851040814244\n0.16161616161616163 38.75903646630445\n0.18181818181818182 30.71694214876033\n0.20202020202020204 24.96168783797571\n0.22222222222222224 20.700617283950617\n0.24242424242424243 17.45685548668503\n0.26262626262626265 14.929548156238129\n0.2828282828282829 12.92128367263648\n0.30303030303030304 11.298172635445361\n0.32323232323232326 9.966809927717833\n0.3434343434343435 8.860426554171964\n0.36363636363636365 7.930268595041323\n0.38383838383838387 7.1400642169759925\n0.4040404040404041 6.462376351902866\n0.42424242424242425 5.876140814452502\n0.4444444444444445 5.364969135802469\n0.4646464646464647 4.9159562148764175\n0.48484848484848486 4.518828196740127\n0.5050505050505051 4.165323987348229\n0.5252525252525253 3.848739962230637\n0.5454545454545455 3.5635904499540856\n0.5656565656565657 3.305351527280638\n0.5858585858585859 3.0702655556635308\n0.6060606060606061 2.8551905417814507\n0.6262626262626263 2.65748294812874\n0.6464646464646465 2.474905726496339\n0.6666666666666667 2.305555555555555\n0.686868686868687 2.1478048326048214\n0.7070707070707072 2.0002550968351827\n0.7272727272727273 1.8616993801652892\n0.7474747474747475 1.7310915822381832\n0.7676767676767677 1.6075214108402638\n0.787878787878788 1.4901937611727818\n0.8080808080808082 1.3784116576114678\n0.8282828282828284 1.27156207154134\n0.8484848484848485 1.1691040741365415\n0.8686868686868687 1.0705588948578604\n0.888888888888889 0.9755015432098765\n0.9090909090909092 0.8835537190082641\n0.9292929292929294 0.7943777895623855\n0.9494949494949496 0.7076716541475031\n0.9696969696969697 0.6231643494605139\n0.98989898989899 0.5406122763442311\n1.0101010101010102 0.4597959493929188\n1.0303030303030305 0.3805171882397417\n1.0505050505050506 0.302596683242079\n1.0707070707070707 0.22587187959584054\n1.090909090909091 0.15019513314967814\n1.1111111111111112 0.07543209876543189\n1.1313131313131315 0.0014603183062319447\n1.1515151515151516 -0.07183201951522311\n1.1717171717171717 -0.14454717092494984\n1.191919191919192 -0.2167788004559923\n1.2121212121212122 -0.2886128328741967\n1.2323232323232325 -0.36012820815496915\n1.2525252525252526 -0.4313975519179223\n1.272727272727273 -0.5024877719682923\n1.292929292929293 -0.5734605901083917\n1.3131313131313131 -0.6443730171236\n1.3333333333333335 -0.7152777777777782\n1.3535353535353536 -0.7862236917418948\n1.373737373737374 -0.8572560156014735\n1.393939393939394 -0.9284167504222499\n1.4141414141414144 -0.9997449187817161\n1.4343434343434345 -1.0712768146824043\n1.4545454545454546 -1.143046229338843\n1.474747474747475 -1.2150846554637709\n1.494949494949495 -1.2874214723620951\n1.5151515151515154 -1.3600841138659328\n1.5353535353535355 -1.4330982209048717\n1.5555555555555556 -1.5064877802973042\n1.575757575757576 -1.58027525116686\n1.595959595959596 -1.6544816802290594\n1.6161616161616164 -1.729126807054128\n1.6363636363636365 -1.8042291602897669\n1.6565656565656568 -1.8798061457204203\n1.676767676767677 -1.9558741269450486\n1.696969696969697 -2.032448499372201\n1.7171717171717173 -2.1095437581575784\n1.7373737373737375 -2.187173560644274\n1.7575757575757578 -2.2653507838082487\n1.777777777777778 -2.344087577160494\n1.7979797979797982 -2.423395411511965\n1.8181818181818183 -2.503285123966943\n1.8383838383838385 -2.583766959474586\n1.8585858585858588 -2.664850609236279\n1.878787878787879 -2.7465452462378015\n1.8989898989898992 -2.828859558149688\n1.9191919191919193 -2.9118017778162164\n1.9393939393939394 -2.9953797115329435\n1.9595959595959598 -3.079600765294187\n1.97979797979798 -3.1644719691753447\n2.0 -3.25\n\n\nC:\\Users\\Hp\\Anaconda3\\envs\\nilmtk_env\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars"
  },
  {
    "objectID": "cnn/visualisation.html",
    "href": "cnn/visualisation.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n%matplotlib inline\n\n\nsomeX, someY = 0.5, 0.5\nfig,ax = plt.subplots()\nax.set_aspect(\"equal\")\nax.add_patch(patches.Rectangle((0.5, 0.5), 0.1, 0.1,\n                      alpha=1, facecolor='gray'))\n\n&lt;matplotlib.patches.Rectangle at 0x7fb7e4060b00&gt;\n\n\n\n\n\n\nimport sys\nsys.path.append(\"convnet-drawer/\")\n\n\nfrom convnet_drawer import Model, Conv2D, MaxPooling2D, Flatten, Dense\nfrom matplotlib_util import save_model_to_file\nchannel_scale = 1/5\n\nmodel = Model(input_shape=(32, 32, 1))\nmodel.add(Conv2D(6, (3, 3), (1, 1)))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(256, (5, 5), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(256, (3, 3), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(4096))\nmodel.add(Dense(4096))\nmodel.add(Dense(1000))\n\n# save as svg file\nmodel.save_fig(\"example.svg\")\n\n\n\n# save via matplotlib\nsave_model_to_file(model, \"example.pdf\")\n\n\n\n\nmodel\n\n##### from mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef cuboid_data(o, size=(1,1,1)):\n    # code taken from\n    # https://stackoverflow.com/a/35978146/4124317\n    # suppose axis direction: x: to left; y: to inside; z: to upper\n    # get the length, width, and height\n    l, w, h = size\n    x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  \n    y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1], o[1], o[1]],          \n         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]   \n    z = [[o[2], o[2], o[2], o[2], o[2]],                       \n         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],   \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]],               \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]]]               \n    return np.array(x), np.array(y), np.array(z)\n\ndef plotCubeAt(pos=(0,0,0), size=(1,1,1), ax=None,**kwargs):\n    # Plotting a cube element at position pos\n    if ax !=None:\n        X, Y, Z = cuboid_data( pos, size )\n        ax.plot_surface(X, Y, Z, rstride=1, cstride=1, **kwargs)\n\nsizes = [(32,32,1), (28, 28, 6), (14, 14, 6), (10, 10, 16), (5, 5, 16), (1, 120, 1)]\npositions = [(0, 0, 0)]*len(sizes)\nfor i in range(1, len(sizes)):\n    positions[i] = (positions[i-1][0] + sizes[i-1][0]+10, 0, 0)\ncolors = [\"grey\"]*len(sizes)\n\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.view_init(84, -90)\nax.set_aspect('equal')\nax.set_axis_off()\nax.set_xlabel('X')\nax.set_xlim(-5, positions[-1][0]+10)\nax.set_ylabel('Y')\nax.set_ylim(-1, 130)\nax.set_zlabel('Z')\nax.set_zlim(-1, 5)\n#ax.set_visible(False)\nfor p,s,c in zip(positions,sizes,colors):\n    plotCubeAt(pos=p, size=s, ax=ax, color=c)\nax.w_zaxis.line.set_lw(0.)\nax.set_zticks([])\n\nfor i in range(len(positions)):\n    ax.text(positions[i][0], -5, 0, \"X\".join(str(x) for x in sizes[i]), color='black', fontsize=4)\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1)\nfig.tight_layout()\nplt.tight_layout()\nplt.savefig(\"lenet.pdf\", bbox_inches=\"tight\", transparent=True, dpi=600)"
  },
  {
    "objectID": "cnn/tensor-factorisation.html",
    "href": "cnn/tensor-factorisation.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import tensorly as tl\n\nUsing numpy backend.\n\n\n\nimport cvxpy as cp\nimport numpy as np\n\n# Ensure repeatably random problem data.\nnp.random.seed(0)\n\n# Generate random data matrix A.\nm = 10\nn = 10\no = 5\nk = 2\nD = 10*np.ones((m, n, o)) + np.random.randn(m, n, o)\n\n# Initialize Y randomly.\nA_init = 10*np.ones((m, k))\nB_init = 10*np.ones((n, k))\nC_init = 10*np.ones((o, k))\n\n\n\n\nPred_A = np.einsum('ir, jr, kr -&gt;ijk', A_init, B_init, C_init)\n\n\n# Ensure same initial random Y, rather than generate new one\n# when executing this cell.\nB = B_init\nC = C_init\n\n# Perform alternating minimization.\nMAX_ITERS = 100\nresidual = np.zeros(MAX_ITERS)\nfor iter_num in range(0, 1+MAX_ITERS):\n\n    if iter_num % 3 == 0:\n        A = cp.Variable(shape=(n, k))\n        constraint = [A &gt;= 0]\n        prediction = A@tl.tenalg.khatri_rao([C, B]).T\n    elif iter_num % 3 == 1:\n        B = cp.Variable(shape=(m, k))\n        constraint = [B &gt;= 0]\n        prediction = B@tl.tenalg.khatri_rao([A, C]).T\n    elif iter_num % 3 == 2:\n        C = cp.Variable(shape=(o, k))\n        constraint = [C &gt;= 0]\n        prediction = C@tl.tenalg.khatri_rao([B, A]).T\n\n    obj = cp.Minimize(cp.norm(D.reshape(prediction.shape) - prediction, 'fro')/D.size)\n    prob = cp.Problem(obj, constraint)\n    prob.solve(solver=cp.SCS, max_iters=10000)\n\n    if prob.status != cp.OPTIMAL:\n        raise Exception(\"Solver did not converge!\")\n\n    print('Iteration {}, residual norm {}'.format(iter_num, prob.value))\n    residual[iter_num-1] = prob.value\n\n    # Convert variable to NumPy array constant for next iteration.\n    if iter_num % 3 == 0:\n        A = A.value\n    elif iter_num%3 == 1:\n        B = B.value\n    else:\n        C = C.value\n\nIteration 0, residual norm 0.044229038768601577\nIteration 1, residual norm 0.04438975125966638\nIteration 2, residual norm 0.04485089174072711\nIteration 3, residual norm 0.0446730384004453\nIteration 4, residual norm 0.044526862069177754\nIteration 5, residual norm 0.04484264445045543\nIteration 6, residual norm 0.044478708695822676\nIteration 7, residual norm 0.04447482085818169\nIteration 8, residual norm 0.045090403949033964\nIteration 9, residual norm 0.04454108258260972\nIteration 10, residual norm 0.04409564845828368\nIteration 11, residual norm 0.04513476609369982\nIteration 12, residual norm 0.04452491993393663\nIteration 13, residual norm 0.04378836637021587\nIteration 14, residual norm 0.045190029913906464\nIteration 15, residual norm 0.04437455947694065\nIteration 16, residual norm 0.04376389953499031\nIteration 17, residual norm 0.04474118548369649\nIteration 18, residual norm 0.04485265629997688\nIteration 19, residual norm 0.04423721109971314\nIteration 20, residual norm 0.04463836313028598\nIteration 21, residual norm 0.04509480034427943\nIteration 22, residual norm 0.04449824189344165\nIteration 23, residual norm 0.0446297404908044\nIteration 24, residual norm 0.044916791718091605\nIteration 25, residual norm 0.04475629136811872\nIteration 26, residual norm 0.044530205009593614\nIteration 27, residual norm 0.04481551774462266\nIteration 28, residual norm 0.04450448414700286\nIteration 29, residual norm 0.04473161074066948\nIteration 30, residual norm 0.044760915844488124\nIteration 31, residual norm 0.04447831568798033\nIteration 32, residual norm 0.04458463344810684\nIteration 33, residual norm 0.04486246216584251\nIteration 34, residual norm 0.04441553594150665\nIteration 35, residual norm 0.04465894246345131\nIteration 36, residual norm 0.04504337082596328\nIteration 37, residual norm 0.044526299849176346\nIteration 38, residual norm 0.04482460772221398\nIteration 39, residual norm 0.04500800584243032\nIteration 40, residual norm 0.04450100433007423\nIteration 41, residual norm 0.04484822280377619\nIteration 42, residual norm 0.04482311584413178\nIteration 43, residual norm 0.044522803791676675\nIteration 44, residual norm 0.044772484551616504\nIteration 45, residual norm 0.04458696318356589\nIteration 46, residual norm 0.044520256516783666\nIteration 47, residual norm 0.04482402095966313\nIteration 48, residual norm 0.04426750617419428\nIteration 49, residual norm 0.044306760469399145\nIteration 50, residual norm 0.04488415609564971\nIteration 51, residual norm 0.04462703244119978\nIteration 52, residual norm 0.04393438972821902\nIteration 53, residual norm 0.04496061133340096\nIteration 54, residual norm 0.04474813896252984\nIteration 55, residual norm 0.04412129577027855\nIteration 56, residual norm 0.04476074674202858\nIteration 57, residual norm 0.04493986609489073\nIteration 58, residual norm 0.04433894515459506\nIteration 59, residual norm 0.04479359793862809\nIteration 60, residual norm 0.04507730499950036\nIteration 61, residual norm 0.04443726027910054\nIteration 62, residual norm 0.04482400811580273\nIteration 63, residual norm 0.044904850352243696\nIteration 64, residual norm 0.04455396547559336\nIteration 65, residual norm 0.04468608722592516\nIteration 66, residual norm 0.04458327810705387\nIteration 67, residual norm 0.0445511237555968\nIteration 68, residual norm 0.044703743776719096\nIteration 69, residual norm 0.04439204969221895\nIteration 70, residual norm 0.04419479288919463\nIteration 71, residual norm 0.04496926843613955\nIteration 72, residual norm 0.04456014579787714\nIteration 73, residual norm 0.04390674892508923\nIteration 74, residual norm 0.04471211816232015\nIteration 75, residual norm 0.04477670162347586\nIteration 76, residual norm 0.044259574198538376\nIteration 77, residual norm 0.044866509022554346\nIteration 78, residual norm 0.04499103987882674\nIteration 79, residual norm 0.04443576055416042\nIteration 80, residual norm 0.04494346910320409\nIteration 81, residual norm 0.04480069447165729\nIteration 82, residual norm 0.044256568121726014\nIteration 83, residual norm 0.044744496252969\nIteration 84, residual norm 0.04448314930970543\nIteration 85, residual norm 0.044155459981671225\nIteration 86, residual norm 0.04521780706935539\nIteration 87, residual norm 0.04446147094598418\nIteration 88, residual norm 0.04404940602099275\nIteration 89, residual norm 0.04516034088743775\nIteration 90, residual norm 0.04457572511910495\nIteration 91, residual norm 0.044197230653923565\nIteration 92, residual norm 0.045118201306481545\nIteration 93, residual norm 0.04421183315228502\nIteration 94, residual norm 0.04375429174860549\nIteration 95, residual norm 0.04441022241202105\nIteration 96, residual norm 0.04401508759046129\nIteration 97, residual norm 0.04393321896924147\nIteration 98, residual norm 0.04435341885307329\nIteration 99, residual norm 0.04411809866902199\nIteration 100, residual norm 0.044273830016112556\n\n\n\nA\n\narray([[ 1.23599056e-01,  4.95799879e-02],\n       [-6.88417340e-12,  4.89807637e-02],\n       [ 1.97771012e-01,  5.00354889e-02],\n       [ 2.97660174e-11,  4.86588823e-02],\n       [ 1.16488475e-01,  4.82973254e-02],\n       [-1.07464945e-11,  4.91216434e-02],\n       [ 2.41321912e-11,  4.81319502e-02],\n       [ 1.23285226e-01,  4.79485790e-02],\n       [ 8.32457971e-12,  4.89433882e-02],\n       [ 1.25287913e-11,  4.91104662e-02]])\n\n\n\nB\n\narray([[ 9.47630862e+00,  1.31432989e+01],\n       [ 2.21280935e+00,  1.29931743e+01],\n       [ 5.84494148e-10,  1.33764365e+01],\n       [ 1.08369019e+01,  1.28286679e+01],\n       [ 5.85242755e+00,  1.28260099e+01],\n       [-2.53650233e-09,  1.30457870e+01],\n       [ 2.49310302e+00,  1.27637902e+01],\n       [-9.13914054e-10,  1.27890647e+01],\n       [ 1.19440017e+00,  1.29894214e+01],\n       [-5.49265493e-10,  1.30485092e+01]])\n\n\n\nC\n\narray([[ 1.52713520e-01,  1.58149369e+01],\n       [ 4.52202894e-01,  1.58153740e+01],\n       [ 5.73957401e-10,  1.56441552e+01],\n       [-1.73226998e-12,  1.54265628e+01],\n       [-1.68743313e-10,  1.57148110e+01]])\n\n\n\nnp.einsum('ir, jr, kr -&gt;ijk', A, B, C)\n\narray([[[10.48458589, 10.83565147, 10.19442928, 10.05263637,\n         10.24047173],\n        [10.22977218, 10.31196443, 10.07798707,  9.93781374,\n         10.12350361],\n        [10.48852243, 10.4888123 , 10.37525943, 10.23095138,\n         10.42211858],\n        [10.26356388, 10.66498719,  9.95038983,  9.81199123,\n          9.99533009],\n        [10.16739655, 10.38431148,  9.94832818,  9.80995826,\n          9.99325913],\n        [10.22925871, 10.22954142, 10.11879543,  9.97805451,\n         10.16449628],\n        [10.05520169, 10.1477645 ,  9.90006825,  9.76236957,\n          9.94478124],\n        [10.02796162, 10.02823877,  9.9196721 ,  9.78170076,\n          9.96447363],\n        [10.20760682, 10.25210095, 10.07507622,  9.93494338,\n         10.12057961],\n        [10.2313932 , 10.23167597, 10.12090687,  9.98013658,\n         10.16661726]],\n\n       [[10.18116321, 10.18144459, 10.07121931,  9.93114011,\n         10.11670529],\n        [10.06487253, 10.0651507 ,  9.95618442,  9.81770523,\n         10.00115085],\n        [10.36175805, 10.36204442, 10.24986394, 10.1073    ,\n         10.29615676],\n        [ 9.93744133,  9.93771598,  9.83012932,  9.69340341,\n          9.87452643],\n        [ 9.93538237,  9.93565696,  9.82809259,  9.69139501,\n          9.87248051],\n        [10.10562779, 10.10590708,  9.99649957,  9.85745964,\n         10.04164808],\n        [ 9.88718524,  9.88745849,  9.78041593,  9.64438148,\n          9.82458851],\n        [ 9.90676358,  9.90703738,  9.79978285,  9.66347903,\n          9.84404291],\n        [10.06196547, 10.06224355,  9.95330875,  9.81486955,\n          9.99826219],\n        [10.10773648, 10.10801583,  9.99858549,  9.85951655,\n         10.04374343]],\n\n       [[10.68660524, 11.24817743, 10.28808747, 10.14499189,\n         10.33455292],\n        [10.34843597, 10.47978553, 10.17057548, 10.02911436,\n         10.2165102 ],\n        [10.58488251, 10.58517505, 10.47057894, 10.32494511,\n         10.5178686 ],\n        [10.47872824, 11.12088193, 10.04180598,  9.90213589,\n         10.08715912],\n        [10.32608231, 10.67300395, 10.0397254 ,  9.90008425,\n         10.08506914],\n        [10.32323689, 10.32352219, 10.21175876, 10.06972482,\n         10.25787948],\n        [10.17538798, 10.3233344 ,  9.99102209,  9.85205835,\n         10.03614586],\n        [10.12009045, 10.12037014, 10.01080605,  9.87156714,\n         10.05601918],\n        [10.314708  , 10.38573678, 10.16763789, 10.02621762,\n         10.21355934],\n        [10.32539099, 10.32567635, 10.2138896 , 10.07182603,\n         10.26001994]],\n\n       [[10.11425681, 10.11453634, 10.00503541,  9.86587676,\n         10.05022247],\n        [ 9.99873034,  9.99900668,  9.89075648,  9.75318732,\n          9.93542741],\n        [10.29366485, 10.29394934, 10.18250606, 10.040879  ,\n         10.22849466],\n        [ 9.87213657,  9.87240941,  9.76552976,  9.62970236,\n          9.80963512],\n        [ 9.87009114,  9.87036392,  9.76350642,  9.62770716,\n          9.80760264],\n        [10.03921777, 10.03949523,  9.9308067 ,  9.79268048,\n          9.97565852],\n        [ 9.82221073,  9.82248219,  9.71614307,  9.58100258,\n          9.76002537],\n        [ 9.84166042,  9.84193242,  9.73538272,  9.59997463,\n          9.77935192],\n        [ 9.99584238,  9.99611864,  9.88789971,  9.75037028,\n          9.93255774],\n        [10.04131261, 10.04159012,  9.93287891,  9.79472388,\n          9.97774009]],\n\n       [[10.20768094, 10.53855895,  9.93069359,  9.79256895,\n          9.97554489],\n        [ 9.96379984, 10.04127254,  9.8172638 ,  9.68071684,\n          9.86160281],\n        [10.21717838, 10.21746076, 10.10684556,  9.96627084,\n         10.15249244],\n        [ 9.99156385, 10.36990232,  9.69296758,  9.55814943,\n          9.73674521],\n        [ 9.90086298, 10.10530773,  9.69095927,  9.55616906,\n          9.73472783],\n        [ 9.96462196,  9.96489736,  9.85701643,  9.71991656,\n          9.90153498],\n        [ 9.7935781 ,  9.88082458,  9.64394785,  9.50981151,\n          9.68750408],\n        [ 9.76853255,  9.76880252,  9.66304454,  9.52864259,\n          9.70668703],\n        [ 9.94281649,  9.98475981,  9.81442826,  9.67792073,\n          9.85875446],\n        [ 9.96670123,  9.96697669,  9.85907325,  9.72194476,\n          9.90360109]],\n\n       [[10.21044653, 10.21072872, 10.1001864 ,  9.95970431,\n         10.14580321],\n        [10.09382137, 10.09410034,  9.98482065,  9.84594316,\n         10.02991642],\n        [10.3915608 , 10.39184799, 10.27934486, 10.13637088,\n         10.32577082],\n        [ 9.96602365,  9.96629909,  9.85840299,  9.72128382,\n          9.9029278 ],\n        [ 9.96395877,  9.96423415,  9.8563604 ,  9.71926965,\n          9.90087599],\n        [10.13469385, 10.13497395, 10.02525176,  9.88581192,\n         10.07053013],\n        [ 9.91562301,  9.91589705,  9.80854661,  9.67212089,\n          9.85284625],\n        [ 9.93525767,  9.93553225,  9.82796924,  9.69127337,\n          9.87235659],\n        [10.09090595, 10.09118483,  9.98193671,  9.84309933,\n         10.02701945],\n        [10.13680861, 10.13708877, 10.02734368,  9.88787475,\n         10.0726315 ]],\n\n       [[10.00472845, 10.00500495,  9.89668982,  9.75903813,\n          9.94138755],\n        [ 9.89045303,  9.89072638,  9.78364843,  9.64756902,\n          9.82783562],\n        [10.18219366, 10.18247507, 10.07223863,  9.93214525,\n         10.11772921],\n        [ 9.76523015,  9.76550004,  9.65977781,  9.5254213 ,\n          9.70340554],\n        [ 9.76320687,  9.7634767 ,  9.65777638,  9.5234477 ,\n          9.70139507],\n        [ 9.93050202,  9.93077647,  9.82326494,  9.68663451,\n          9.86763105],\n        [ 9.71584497,  9.71611349,  9.61092593,  9.47724889,\n          9.65433302],\n        [ 9.73508404,  9.73535309,  9.62995723,  9.49601549,\n          9.67345028],\n        [ 9.88759634,  9.88786961,  9.78082259,  9.64478249,\n          9.82499702],\n        [ 9.93257417,  9.93284868,  9.82531472,  9.68865578,\n          9.86969008]],\n\n       [[10.14502635, 10.4951919 ,  9.85898582,  9.72185855,\n          9.90351326],\n        [ 9.89443406,  9.97640907,  9.74637509,  9.61081411,\n          9.79039394],\n        [10.14340196, 10.1436823 , 10.03386583,  9.89430618,\n         10.0791831 ],\n        [ 9.93205681, 10.33245243,  9.62297639,  9.48913174,\n          9.66643791],\n        [ 9.83619694, 10.05255267,  9.62098258,  9.48716567,\n          9.6644351 ],\n        [ 9.8926692 ,  9.89294261,  9.78584067,  9.64973077,\n          9.83003776],\n        [ 9.7257684 ,  9.81808778,  9.57431062,  9.44114286,\n          9.61755234],\n        [ 9.69799572,  9.69826374,  9.59326942,  9.45983796,\n          9.63659677],\n        [ 9.87241434,  9.91678695,  9.74356002,  9.60803819,\n          9.78756615],\n        [ 9.89473346,  9.89500692,  9.78788264,  9.65174433,\n          9.83208895]],\n\n       [[10.17339432, 10.17367549, 10.06353431,  9.92356201,\n         10.10898558],\n        [10.05719238, 10.05747033,  9.9485872 ,  9.81021368,\n          9.99351932],\n        [10.35385135, 10.3541375 , 10.24204263, 10.09958747,\n         10.28830012],\n        [ 9.92985842,  9.93013285,  9.82262829,  9.68600671,\n          9.86699153],\n        [ 9.92780103,  9.92807541,  9.82059312,  9.68399985,\n          9.86494716],\n        [10.09791654, 10.09819562,  9.98887159,  9.84993776,\n         10.03398565],\n        [ 9.87964067,  9.87991372,  9.77295283,  9.63702219,\n          9.81709171],\n        [ 9.89920408,  9.89947766,  9.79230498,  9.65610517,\n          9.83653126],\n        [10.05428753, 10.0545654 ,  9.94571372,  9.80738017,\n          9.99063287],\n        [10.10002362, 10.10030276,  9.99095592,  9.8519931 ,\n         10.0360794 ]],\n\n       [[10.20812322, 10.20840535, 10.09788818,  9.95743805,\n         10.14349461],\n        [10.0915246 , 10.0918035 ,  9.98254868,  9.84370279,\n         10.02763418],\n        [10.38919627, 10.3894834 , 10.27700587, 10.13406442,\n         10.32342127],\n        [ 9.96375596,  9.96403133,  9.85615978,  9.71907182,\n          9.90067446],\n        [ 9.96169154,  9.96196686,  9.85411766,  9.7170581 ,\n          9.89862311],\n        [10.13238778, 10.13266781, 10.02297059,  9.88356248,\n         10.06823865],\n        [ 9.91336678,  9.91364076,  9.80631475,  9.66992007,\n          9.8506043 ],\n        [ 9.93299697,  9.93327149,  9.82573295,  9.68906819,\n          9.87011021],\n        [10.08860984, 10.08888866,  9.97966539,  9.84085961,\n         10.02473787],\n        [10.13450206, 10.13478215, 10.02506203,  9.88562483,\n         10.07033954]]])"
  },
  {
    "objectID": "cnn/lenet.html",
    "href": "cnn/lenet.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel = keras.Sequential()\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(units=120, activation='relu'))\nmodel.add(layers.Dense(units=84, activation='relu'))\nmodel.add(layers.Dense(units=10, activation = 'softmax'))\n\n\nmodel\n\n&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x7f96fa3cd4e0&gt;\n\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n&lt;matplotlib.image.AxesImage at 0x7f96c1be8668&gt;\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n&lt;matplotlib.image.AxesImage at 0x7f96c16642b0&gt;\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f96c0af1da0&gt;\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "ensemble/binomial.html",
    "href": "ensemble/binomial.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nSPINE_COLOR = 'gray'\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\ndef bino(n, p, k)\n\n\n choose(n, k) * p**k * (1-p)**(n-k)\n\n\nstats.binom.pmf(n=21, p=0.3, k=4)\n\n0.11277578372328753\n\n\n\nfrom scipy import stats\na=range(21)\nfor error in [0.3, 0.6]:\n    fig, ax = plt.subplots(figsize=(4,3))\n    ax.bar(a,stats.binom.pmf(n=21, p=error, k=a), color='grey', alpha=0.3)\n    ax.bar(a[11:],stats.binom.pmf(n=21, p=error, k=a[11:]), color='grey', alpha=0.9)\n\n\n    ax.set_ylabel(r'$P(X=k)$')\n    ax.set_xlabel(r'$k$')\n    #ax.set_ylim((0,0.4))\n    #ax.legend()\n    format_axes(ax)\n    #plt.fill_betweenx(3, 11, 20)\n    plt.axvline(10.5, color='k',lw=2, label=r'$k=11, \\epsilon={}$'.format(error))\n    plt.title(\"Probability that majority vote \\n (11 out of 21) is wrong = {}\".format(sum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(3)))\n    plt.legend()\n    import tikzplotlib\n\n    tikzplotlib.save(\"test-{}.tex\".format(error), )\n\n\n\n\n\n\n\n\n!cat test.tex\n\n% This file was created by tikzplotlib v0.9.0.\n\\begin{tikzpicture}\n\n\\definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}\n\n\\begin{axis}[\ntick align=outside,\ntick pos=left,\ntitle={Simple plot \\(\\displaystyle \\frac{\\alpha}{2}\\)},\nx grid style={white!69.0196078431373!black},\nxlabel={time (s)},\nxmin=-0.75, xmax=15.75,\nxtick style={color=black},\ny grid style={white!69.0196078431373!black},\nylabel={Voltage (mV)},\nymin=0, ymax=0.24892,\nytick style={color=black}\n]\n\\draw[draw=none,fill=color0] (axis cs:0,0) rectangle (axis cs:1.5,0.00293333333333333);\n\\draw[draw=none,fill=color0] (axis cs:1.5,0) rectangle (axis cs:3,0.0157333333333333);\n\\draw[draw=none,fill=color0] (axis cs:3,0) rectangle (axis cs:4.5,0.110866666666667);\n\\draw[draw=none,fill=color0] (axis cs:4.5,0) rectangle (axis cs:6,0.116866666666667);\n\\draw[draw=none,fill=color0] (axis cs:6,0) rectangle (axis cs:7.5,0.237066666666667);\n\\draw[draw=none,fill=color0] (axis cs:7.5,0) rectangle (axis cs:9,0.0889333333333333);\n\\draw[draw=none,fill=color0] (axis cs:9,0) rectangle (axis cs:10.5,0.0760666666666667);\n\\draw[draw=none,fill=color0] (axis cs:10.5,0) rectangle (axis cs:12,0.012);\n\\draw[draw=none,fill=color0] (axis cs:12,0) rectangle (axis cs:13.5,0.00593333333333333);\n\\draw[draw=none,fill=color0] (axis cs:13.5,0) rectangle (axis cs:15,0.000266666666666667);\n\\end{axis}\n\n\\end{tikzpicture}\n\n\n\nsum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(2)\n\n0.83"
  },
  {
    "objectID": "knn/knn/knn.html",
    "href": "knn/knn/knn.html",
    "title": "IRIS Dataset",
    "section": "",
    "text": "# https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html\n\nimport numpy as np\nimport pylab as plt\nfrom sklearn import neighbors, datasets\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import train_test_split\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "knn/knn/knn.html#random-blobs",
    "href": "knn/knn/knn.html#random-blobs",
    "title": "IRIS Dataset",
    "section": "Random Blobs",
    "text": "Random Blobs\n\nX, Y = datasets.make_blobs(n_samples=500, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('big.pdf'.format(K, B), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .02 # step size in the mesh\nknn=neighbors.KNeighborsClassifier()\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.show()\n\n\n\n\n\nK = 1000\nB = 2\n\nh = .1 # step size in the mesh\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = np.array([probDist(temp, K, B) for temp in np.c_[xx.ravel(), yy.ravel()]])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.contourf(xx, yy, Z, 20, cmap='viridis')\nplt.colorbar();\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n\n\n\n\nExample 1\n\nX = np.array([[1,5],[2,5],[3,5],[4,5],[1,3],[2,3],[3,3],[4,3],[2.5,1]])\nY = np.array([0,0,0,0,1,1,1,1,0])\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.savefig('exp.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nh = .1 # step size in the mesh\nfor K in [ 1, 3, 9]:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=K)\n    knn.fit(X, Y)\n    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.set_cmap(plt.cm.Paired)\n    plt.pcolormesh(xx, yy, Z)\n\n    # Plot also the training points\n    plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    plt.savefig('exp_knn_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n    #plt.show()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\nExample 2\n\n# X, Y = datasets.make_blobs(n_samples=50, centers=3, n_features=2, random_state=12, cluster_std=2)\n\n# plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n# plt.xlabel('X')\n# plt.ylabel('Y')\n\n\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrain_err = []\ntest_err = []\nk_list = [t for t in range(1,100)]\nfor k in k_list:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n    Z_train = knn.predict(X_train)\n    Z_test = knn.predict(X_test)\n    \n    train_err.append((Z_train != Y_train).sum()/Y_train.size)\n    test_err.append((Z_test != Y_test).sum()/Y_test.size)\n    #print(k,(Z_train != Y_train).sum()/Y_train.size, (Z_test != Y_test).sum()/Y_test.size)\n\n\nplt.plot(k_list, train_err)\nplt.plot(k_list, test_err)"
  },
  {
    "objectID": "knn/knn/knn.html#other-distance-metrics",
    "href": "knn/knn/knn.html#other-distance-metrics",
    "title": "IRIS Dataset",
    "section": "Other Distance Metrics",
    "text": "Other Distance Metrics\n\nX, Y = datasets.make_blobs(n_samples=100, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('iris_knn_data.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K)\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_def_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"manhattan\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_man_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"chebyshev\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_che_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\nCurse of Dimensionality"
  },
  {
    "objectID": "ridge/Ridge.html",
    "href": "ridge/Ridge.html",
    "title": "Question",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom mpl_toolkits import mplot3d\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n\n%matplotlib inline\n# Based on: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\n\nimport sys\nsys.path.append(\"../\")\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify()\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nformat_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\ncoef_list = []\nfor i,deg in enumerate([1,3,6,11]):\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=False, fit_intercept=False)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    coef_list.append(abs(max(regressor.coef_, key=abs)))\n\n    plt.scatter(data['x'],data['y'], label='Train')\n    plt.plot(data['x'], y_pred,'k', label='Prediction')\n    plt.plot(data['x'], y_true,'g.', label='True Function')\n    format_axes(plt.gca())\n    plt.legend() \n    plt.title(f\"Degree: {deg} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n    plt.savefig('lin_plot_{}.pdf'.format(deg), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\nplt.semilogy([1,3,6,11],coef_list,'o-k')\nplt.xticks([1,3,6,11])\nplt.xlabel('Degree')\nplt.ylabel('Max Coef')\nformat_axes(plt.gca())\nplt.savefig('lin_plot_coef.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\nfrom matplotlib.patches import Circle\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.4)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Circle((0, 0), 3, color='g', label=r'$\\theta_0^2+\\theta_1^2=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig('ridge_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes(projection='3d')\n\nax = plt.axes(projection='3d')\nax.plot_surface(x_grid, y_grid, cost_matrix,cmap='viridis', edgecolor='none')\nax.set_title('Least squares objective function');\nax.set_xlabel(r\"$\\theta_0$\")\nax.set_ylabel(r\"$\\theta_1$\")\nax.set_xlim([-4,15])\nax.set_ylim([-4,15])\n\nu = np.linspace(0, np.pi, 30)\nv = np.linspace(0, 2 * np.pi, 30)\n\n# x = np.outer(500*np.sin(u), np.sin(v))\n# y = np.outer(500*np.sin(u), np.cos(v))\n# z = np.outer(500*np.cos(u), np.ones_like(v))\n# ax.plot_wireframe(x, y, z)\n\nax.view_init(45, 120)\nplt.savefig('ridge_base_surface.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nlatexify(fig_width=5, fig_height=2.5)\nfrom sklearn.linear_model import Ridge\n\nfor alpha in [1, 10, 1000]:\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Ridge(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    format_axes(ax[0])\n    format_axes(ax[1])\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    p1 = Circle((0, 0), np.sqrt(regressor.coef_.T@regressor.coef_), alpha=0.6, color='g', label=r'$\\theta_0^2+\\theta_1^2={:.2f}$'.format(np.sqrt(regressor.coef_.T@regressor.coef_)))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.3)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Ridge Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n    ax[1].set_aspect('equal')\n    plt.savefig('ridge_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\nlatexify()\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        format_axes(plt.gca())\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('ridge_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.62699e-21): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\n# from sklearn.linear_model import Ridge\n\n# for i,deg in enumerate([2,4,8,16]):\n#   predictors = ['x']\n#   if deg &gt;= 2:\n#     predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n#   fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20, 4))\n\n#   for i,alpha in enumerate([1e-15,1e-4,1,20]):\n#     regressor = Ridge(alpha=alpha,normalize=True)\n#     regressor.fit(data[predictors],data['y'])\n#     y_pred = regressor.predict(data[predictors])\n#     ax[i].scatter(data['x'],data['y'], label='Train')\n#     ax[i].plot(data['x'], y_pred,'k', label='Prediction')\n#     ax[i].plot(data['x'], y_true,'g.', label='True Function')\n#     ax[i].legend() \n#     ax[i].set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"data.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-2,10,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Ridge(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nformat_axes(plt.gca())\n\nlim = True\nif lim:\n    plt.ylim((10e-20, 100))\n    plt.savefig('rid_reg-without-lim.pdf', transparent=True, bbox_inches=\"tight\")\nelse:\n    plt.savefig('rid_reg-with-lim.pdf', transparent=True, bbox_inches=\"tight\")\n\n# plt.set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\n\n\n\nplt.style.use('seaborn-whitegrid')\n\nx = [1,2,3,4]\ny = [1,2,3,0]\ny_1 = [(2-i/5) for i in x]\ny_2 = [(0.5+0.4*i) for i in x]\nplt.ylim(-0.2,3.3)\nplt.plot(x,y,'.')\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\n#plt.plot(x,y_1, label=\"unreg\")\n#plt.plot(x,y_2, label=\"reg\")\n#plt.legend()\n#format_axes(plt.gca())\nplt.savefig('temp.pdf', transparent=True)\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nx_new = np.vstack([np.ones_like(x), x]).T\nregressor = LinearRegression(normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_unreg.pdf', transparent=True)\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nregressor = Ridge(alpha=4, normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred_r = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\ndef ridge_func(x):\n    return 0.56 + 0.26*np.array(x)\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.plot(x,ridge_func(x),'-b', label='Regularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_reg.pdf', transparent=True)\n\n\n\n\n\nregressor.coef_\n\narray([0.37209302, 0.30232558])\n\n\n\nRetrying with a better example\n\nnp.linalg.inv([[4, 10], [10, 34]])@np.array([6, 14])\n\narray([ 1.77777778, -0.11111111])\n\n\n\nnp.linalg.inv([[8, 10], [10, 34]])@np.array([6, 14])\n\narray([0.37209302, 0.30232558])\n\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,600,10)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,5,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n#format_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([17]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1e-3, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        #format_axes(plt.gca())\n        #print(regressor.coef_)\n        plt.ylim([0,60])\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha}\")\n        plt.savefig('ridge_new_{}_{}.pdf'.format(i, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.11479e-28): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "accuracy_convention.pdf\n\n\n12/26/23, 4:24:44 AM\n\n\n\n\n\n\n \n\n\n\n\ndecision-tree-1.pdf\n\n\n12/26/23, 4:24:44 AM\n\n\n\n\n\n\n \n\n\n\n\ndecision-tree-2-bias-variance-1.pdf\n\n\n12/26/23, 4:24:44 AM\n\n\n\n\n\n\n \n\n\n\n\ngp.pdf\n\n\n12/26/23, 4:24:44 AM\n\n\n\n\n\n\n \n\n\n\n\ntemp.pdf\n\n\n12/26/23, 4:24:44 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Gaussian Naive Bayes\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFitting linear model\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLikelihood\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nXOR using feature transformation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "gradient-descent/Gradient Descent.html",
    "href": "gradient-descent/Gradient Descent.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "#import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nsns.despine()\n\nNameError: name 'sns' is not defined\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\nplt.plot(x,y)\nplt.title(\"Cost Function\")\n\nText(0.5, 1.0, 'Cost Function')\n\n\n\n\n\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "linear-reg/Linear Regression Notebook.html",
    "href": "linear-reg/Linear Regression Notebook.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Home page"
  },
  {
    "objectID": "Lasso/Lasso_Regression.html",
    "href": "Lasso/Lasso_Regression.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom mpl_toolkits import mplot3d\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n%matplotlib inline\n# Based on: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\n\ny_true = 4*x + 7\nmax_deg = 20\n\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.savefig('true_function.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\ndef cost_lasso(theta_0, theta_1, x, y, lamb):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2 + lamb*(abs(theta_0) + abs(theta_1))\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\n\n\n#lambda  = 1000 cost curve tends to lasso objective. \nfig = plt.figure(figsize=(7,7))\nlamb_list = [10,100,1000]\nfor lamb in lamb_list:\n    lasso_cost_matrix = np.zeros_like(x_grid)\n    for i in range(x_grid.shape[0]):\n        for j in range(x_grid.shape[1]):\n            lasso_cost_matrix[i, j] = cost_lasso(x_grid[i, j], y_grid[i, j], data['x'], data['y'],lamb)\n\n\n    ax = plt.axes(projection='3d')\n    ax.plot_surface(x_grid, y_grid, lasso_cost_matrix,cmap='viridis', edgecolor='none')\n\n    ax.set_title('Least squares objective function');\n    ax.set_xlabel(r\"$\\theta_0$\")\n    ax.set_ylabel(r\"$\\theta_1$\")\n    ax.set_xlim([-4,15])\n    ax.set_ylim([-4,15])\n\n    u = np.linspace(0, np.pi, 30)\n    v = np.linspace(0, 2 * np.pi, 30)\n\n    # x = np.outer(500*np.sin(u), np.sin(v))\n    # y = np.outer(500*np.sin(u), np.cos(v))\n    # z = np.outer(500*np.cos(u), np.ones_like(v))\n    # ax.plot_wireframe(x, y, z)\n\n    ax.view_init(45, 120)\n    plt.savefig('lasso_lamb_{}_surface.pdf'.format(lamb), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\ndef yy(p,soln):\n    xx = np.linspace(-soln,soln,100)\n    xx_final = []\n    yy_final = []\n    for x in xx:\n        if(x&gt;0):\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - x**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n            \n        else:\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - (-x)**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n    return xx_final, yy_final\n\n\nfig,ax = plt.subplots()\nax.fill(xx_final,yy_final)\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\nsoln = 3\np  = 0.5\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nxx = np.linspace(-soln,soln,100)\ny1  = yy1(xx,soln,p)\ny2  = yy2(xx,soln,p)\n\nx_final = np.hstack((xx,xx))\ny_final = np.hstack((y1,y2))\n\nxx_final, yy_final = yy(p,soln)\n\nplt.fill(xx_final,yy_final)\n\n\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n  import sys\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n  \"\"\"\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nplt.scatter(xx, y1, s=0.1,color='k',)\nplt.scatter(xx, y2, s=0.1,color='k',)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\nregressor.coef_[0] + regressor.coef_[1]\n\n5.74196012460497\n\n\n\n## Function generator. \niterations = 60\np = 4\nq = 4\nalpha  = 0.1\n\nx = np.linspace(-5,5,1000)\ny1 = x**2\ny2 = abs(x)\n\n\nfor i in range(iterations):\n    fig,ax = plt.subplots(1,2)\n    ax[0].plot(x,y1)\n    ax[1].plot(x,y2)\n    prev = p\n    qrev = q\n    p = p - 2*alpha*p\n    q = q - alpha\n    val = p\n    \n    ax[0].arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    ax[1].arrow(qrev,abs(qrev),q - qrev ,abs(q) - abs(qrev),head_width = 0.5)\n    ax[0].scatter([prev],[prev**2],s=100)\n    ax[1].scatter([qrev],abs(qrev),s=100)\n    ax[0].set_xlabel(\"x\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_ylabel(\"Cost\")\n    ax[0].set_ylabel(\"Cost\")\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(0,5)\n    ax[1].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    ax[0].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    if(i==0):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    if(i%10==9):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1,y1= 5**0.5,5\ny2,x2 = 5,5\nx_shift = 0\ny_shift = -0.5\niterations = 11\nfor i in range(iterations):\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n   \n    ax[0].set_ylim(-1,10)\n    ax[0].set_xlim(-5,5)\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(-1,10)\n    ax[0].plot(x,x**2, color = 'blue')\n    ax[1].plot(x,abs(x),color = 'red')\n    ax[0].scatter(x1,y1,color = 'black')\n    \n    ax[0].annotate(str(round(y1,3)), (x1 + x_shift, y1+y_shift))\n    ax[1].annotate(str(y2), (x2 + x_shift, y2 + y_shift))\n    ax[1].scatter(x2,y2,color = 'black')\n    fig.suptitle('Iteration {}'.format(i))\n    if(iteratio)\n    plt.savefig('GD_Iteration_{}.pdf'.format(i))\n    \n    y1 = y1 - alpha*y1\n    y2 = y2 - 0.5\n    x2 = y2\n    x1 = y1**0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nfrom matplotlib.patches import Rectangle\n\n\nfor alpha in np.linspace(1,2,5):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Lasso(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    total = abs(regressor.coef_[0]) + abs(regressor.coef_[1])\n    p1 = Rectangle((-total, 0), np.sqrt(2)*total, np.sqrt(2)*total, angle = -45, alpha=0.6, color='g', label=r'$|\\theta_0|+|\\theta_1|={:.2f}$'.format(total))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Lasso Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n\n    plt.savefig('lasso_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nfrom sklearn.linear_model import Lasso\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e10]):\n        regressor = Lasso(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('lasso_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 659.2329891662157, tolerance: 2.566256097809531\n  positive)\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5017.444529811921, tolerance: 2.566256097809531\n  positive)\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"dataset.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-5,1,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Lasso(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nplt.savefig('lasso_reg.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "bias-variance/Charts.html",
    "href": "bias-variance/Charts.html",
    "title": "Bias New",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.legend()\nplt.savefig('images/true.pdf', transparent=True)\n\n\n\n\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\nplt.savefig('images/data.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n# plt.fill_between(data['x'], y_true-dy, y_true+dy, color='green',alpha=0.2, label='Variance')\nplt.errorbar(data['x'][15], y_true[15], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\n\nplt.legend()\n\n\nplt.savefig('images/data_var.pdf', transparent=True)\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\n\nplt.savefig('images/biasn_1.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.legend()\n\nplt.savefig('images/biasn_2.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.fill_between(x, y_true, [data['y'].mean() for _ in data['x']], color='green',alpha=0.2, label='Bias')\nplt.legend()\n\nplt.savefig('images/biasn_3.pdf', transparent=True)"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-old",
    "href": "bias-variance/Charts.html#bias-old",
    "title": "Bias New",
    "section": "Bias Old",
    "text": "Bias Old\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\n\n\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nplt.savefig('images/bias1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\nax[1].plot(x, [y2.mean() for _ in x], 'r:', label='Prediction')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train4)}$')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias4.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\n\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias5.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nplt.plot(x, y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nfit = np.array([(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x])\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nplt.fill_between(x, y_true, fit, color='green',alpha=0.2, label='Bias')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias6.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nVarying Degree on Bias\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nax[0].plot(x, [y.mean() for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nax[0].plot(data['x'], data['y'], '.b', label='Actual Prices')\nax[0].plot(data['x'], y_true,'g', label='True Function')\nax[0].fill_between(x, y_true, [y.mean() for _ in x], color='green',alpha=0.2, label='Bias')\nax[0].set_title(f\"Degree = 0\")\nfor i,deg in enumerate([1]):\n    i=i+1\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias7.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nfor i,deg in enumerate([2,3]):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias8.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#variance",
    "href": "bias-variance/Charts.html#variance",
    "title": "Bias New",
    "section": "Variance",
    "text": "Variance\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 25\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\n# plt.errorbar(x[::3], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::3], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\nplt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nVaraince Variation\n\nfrom sklearn.linear_model import LinearRegression\n\nfig, ax = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(10, 8))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles = []\n\nfor i,seed in enumerate([2,4,8,16]):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n    \n    ax[int(i/2)][i%2].plot(data_s['x'],data_s['y'], '.b', label='Data Point')\n#     ax[i].plot(data_n['x'],data_n['y'], 'ok', label='UnSelected Points')\n    ax[int(i/2)][i%2].plot(data_s['x'], y_pred,'r-.', label='Prediction')\n    ax[int(i/2)][i%2].plot(data['x'], y_true,'g-', label='True Function')\n#     ax[i].set_title(f\"{deg} : {max(regressor.coef_, key=abs):.2f}\")\n    \n\nhandles, labels = ax[0][0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nmodles = []\n\nfor i,seed in enumerate(range(1,50)):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(8, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles=np.array(modles)\n\n# ax[0].plot(x, modles.mean(axis=0), 'r-.', label='Average Fit')\n# ax[0].plot(data['x'], y_true,'g-', label='True Function')\n# ax[0].set_xlabel('Size (sq.ft)')\n# ax[0].set_ylabel('Price (\\$)')\n\n# ax[1].errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\n# ax[1].plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\n# ax[1].plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\n# ax[1].plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\n# ax[1].plot(data['x'], y_true,'g-', label='True Function')\n# ax[1].set_xlabel('Size (sq.ft)')\n\nax.errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\nax.plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\nax.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\nax.plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\nax.plot(data['x'], y_true,'g-', label='True Function')\nax.set_xlabel('Size (sq.ft)')\n\n# plt.plot(x, modles.mean(axis=0), 'k.-', label=r'Average Fit')\n# plt.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train3)}$')\n# plt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\n# handles, labels = [(a + b) for a, b in zip(ax[0].get_legend_handles_labels(), ax[1].get_legend_handles_labels())]\nhandles, labels = ax.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var4.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-variance-tradeoff",
    "href": "bias-variance/Charts.html#bias-variance-tradeoff",
    "title": "Bias New",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nx = x = np.linspace(0, 4*np.pi, 201)\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 1\np = np.poly1d([1, 2, 3])\ny = np.sin(x) + 0.5*x -  0.05*x**2 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5*x - 0.05*x**2\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\n# plt.xlabel('Size (sq.ft)')\n# plt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\n# plt.ylim(-2,2)\n# plt.xlim(0,4*np.pi)\nplt.plot(data['x'], data['y'], '.', label='Data Points')\nplt.legend()\nplt.savefig('images/bv-1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(10, 5))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,4*np.pi))\n\ndegs = [1,3,7]\nfor i,deg in enumerate(degs):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n    \n#     print(predictors)\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n#     ax[0][i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[0][i].plot(data['x'], y_pred,'-.r', label='Prediction')\n    ax[0][i].plot(data['x'], y_true,'g', label='True Function')\n    ax[0][i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[0][i].set_title(f\"Degree = {deg}\")\n\nfor i,deg in enumerate(degs):    \n    predictors = ['x']\n    models=[]\n    \n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for t,seed in enumerate(range(1,50)):\n        np.random.seed(seed)\n        y_random = np.sin(x) + 0.5*x - 0.05*x**2 + np.random.normal(0,var,len(x))\n        data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n        data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n        data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n\n        predictors = ['x']\n        if deg &gt;= 2:\n            predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n        regressor = LinearRegression(normalize=True)  \n        regressor.fit(data_s[predictors],data_s['y'])\n        y_pred = regressor.predict(data_s[predictors])\n\n        models.append(y_pred)\n    \n    models=np.array(models)\n    ax[1][i].errorbar(x[::7], models.mean(axis=0)[::7], yerr=2*models.std(axis=0)[::7], fmt=':k', capsize=3, label='Variance')\n    ax[1][i].plot(data['x'], y_true,'g-', label='True Function')\n\nhandles, labels = [(a + b) for a, b in zip(ax[0][0].get_legend_handles_labels(), ax[1][0].get_legend_handles_labels())]\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bv-2.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "knn/knn/curse_dimensionality.html",
    "href": "knn/knn/curse_dimensionality.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n%matplotlib inline\n\n\nnp.random.seed(0)\n\n\nnum_points = 10\n\n\ndata = {}\nfor num_dimensions in range(1, 20):\n    data[num_dimensions] = np.random.uniform(low=0.0, high=1.0, size=(num_points, num_dimensions))\n\n\nplt.scatter(data[1], np.zeros_like(data[1]))\n\n&lt;matplotlib.collections.PathCollection at 0x7f4479630890&gt;\n\n\n\n\n\n\nplt.scatter(data[2][:, 0], data[2][:, 1])\n\n&lt;matplotlib.collections.PathCollection at 0x7f44795acbd0&gt;\n\n\n\n\n\n\ndist = {}\nfor num_dimensions in range(1, 20):\n    dist[num_dimensions] = pd.DataFrame(euclidean_distances(data[num_dimensions], data[num_dimensions]))\n\n\nmean_distances = pd.Series({num_dimensions: dist[num_dimensions].mean().mean() for num_dimensions in range(1, 20)})\n\n\nmean_distances.plot(style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Mean distance between two points\")\nplt.savefig('curse_dist.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\nratio_max_min = pd.Series({num_dimensions:(dist[num_dimensions].replace({0:np.NAN}).max()/dist[num_dimensions].replace({0:np.NAN}).min()).mean() \n                           for num_dimensions in range(1, 20) })\n\n\nratio_max_min.plot(logy=True, style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Ratio of max to min distances\")\nplt.ylim((-1, 180))\nplt.savefig('curse_spread.pdf', transparent=True, bbox_inches=\"tight\")\n\n/home/prof/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Attempted to set non-positive bottom ylim on a log-scaled axis.\nInvalid limit will be ignored.\n  after removing the cwd from sys.path."
  },
  {
    "objectID": "ensemble/representation-ensemble.html",
    "href": "ensemble/representation-ensemble.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "%matplotlib inline\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\nmd = 1\n\nnames = [\"Decision Tree (Depth %d)\" %md, \"Random Forest\"]\n\nclassifiers = [\n    DecisionTreeClassifier(max_depth=md),\n    RandomForestClassifier(max_depth=md, n_estimators=200, max_features=1),\n   ]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            ]\n\nfigure = plt.figure(figsize=(8, 4))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\n#plt.savefig(\"abc.svg\")\n#plt.tight_layout()\n#fig.savefig(\"2-representation.pdf\")\n#import tikzplotlib\nplt.savefig(str(md)+\"-representation.pdf\" , transparent=True, bbox_inches=\"tight\")\n#tikzplotlib.save(\"ensemble-vs-stump.tex\")"
  },
  {
    "objectID": "ensemble/boosting-explanation.html",
    "href": "ensemble/boosting-explanation.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\n\nx = np.linspace(0.04, 0.51, 1000)\n\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height &gt; MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': True,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=3)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\n\nText(0, 0.5, '$\\\\alpha_m$')\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=2)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\nformat_axes(plt.gca())\nplt.savefig(\"alpha-boosting.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, np.exp(0.5*np.log((1-x)/x)), color='r', linewidth=2, label=r'$e^{\\alpha_m}$ ')\nplt.plot(x, np.exp(-0.5*np.log((1-x)/x)), color='g', linewidth=2, label=r'$e^{-\\alpha_m}$')\n\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(\"Weight Multiplier\")\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"alpha-boosting-weight.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "cnn/convolution-operation.html",
    "href": "cnn/convolution-operation.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n&lt;matplotlib.image.AxesImage at 0x7f8f0ce58a20&gt;\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\nfrom scipy.signal import convolve2d\n\n\nconvolve2d(inp, filter_conv, mode='valid')\n\narray([[ 2, -3, -4],\n       [ 4,  8, -9],\n       [ 0, 14, -1]])\n\n\n\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt; from scipy.misc import lena as lena\n\n&gt;&gt;&gt; scharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n...                    [-10+0j, 0+ 0j, +10 +0j],\n...                    [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy\n&gt;&gt;&gt; grad = signal.convolve2d(lena, scharr, boundary='symm', mode='same')\n\nImportError: cannot import name 'lena' from 'scipy.misc' (/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/scipy/misc/__init__.py)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 1\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.array([[ 1,0, -1], [1, 0,-1], [ 1,0, -1]])\nf = kernel.shape[0]\n\npadding = True\n\nif padding:\n    # visualization array (2 bigger in each direction)\n    va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n    va[p:-p,p:-p] = a\n    va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n    va_color[p:-p,p:-p] = 0.5\nelse:\n    va = a\n    va_color = np.zeros_like(a)\n\n#output array\nres = np.zeros((n-f+1+2*p, n-f+1+2*p))\n\n\n\n#####################\n# Create inital plot\n#####################\nfig = plt.figure(figsize=(8,4))\n\ndef add_axes_inches(fig, rect):\n    w,h = fig.get_size_inches()\n    return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\naxwidth = 3.\ncellsize = axwidth/va.shape[1]\naxheight = cellsize*va.shape[0]\n\nax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\nax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                   (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                   kernel.shape[1]*cellsize,  \n                                   kernel.shape[0]*cellsize])\nax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                               2*cellsize, \n                               res.shape[1]*cellsize,  \n                               res.shape[0]*cellsize])\nax_kernel.set_title(\"Kernel\", size=12)\n\nim_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\nax_va.set_title(\"Image size: {}X{}\\n Padding: {}\".format(n, n, p))\nfor i in range(va.shape[0]):\n    for j in range(va.shape[1]):\n        ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\nax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\nfor i in range(kernel.shape[0]):\n    for j in range(kernel.shape[1]):\n        ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\nim_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\nres_texts = []\nfor i in range(res.shape[0]):\n    row = []\n    for j in range(res.shape[1]):\n        row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n    res_texts.append(row)    \n\nax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\nfor ax  in [ax_va, ax_kernel, ax_res]:\n    ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.grid(color=\"k\")\n\n###############\n# Animation\n###############\ndef init():\n    for row in res_texts:\n        for text in row:\n            text.set_text(\"\")\n\ndef animate(ij):\n    i,j=ij\n    o = kernel.shape[1]//2\n    # calculate result\n    \n   \n    res_ij = (kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]).sum()\n    \n    res_texts[i][j].set_text(res_ij)\n    # make colors\n    c = va_color.copy()\n    c[1+i-o:1+i+o+1, 1+j-o:1+j+o+1] = 1.\n    im_va.set_array(c)\n\n    r = res.copy()\n    r[i,j] = 1\n    im_res.set_array(r)\n    \n\n\ni,j = np.indices(res.shape)\nani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                         frames=zip(i.flat, j.flat), interval=5)\nani.save(\"algo.gif\", writer=\"imagemagick\")\n\n\n\n\n\nva\n\narray([[0, 2, 2, 2, 0],\n       [4, 3, 0, 2, 2],\n       [1, 3, 3, 4, 2],\n       [3, 0, 0, 0, 2],\n       [0, 3, 4, 2, 3]])\n\n\n\ni\n\narray([[0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4]])\n\n\n\ni = 0\nj = 3\no =kernel.shape[1]//2\n(kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1])\n\nValueError: operands could not be broadcast together with shapes (3,3) (3,2) \n\n\n\n(kernel)\n\narray([[ 1,  0, -1],\n       [ 1,  0, -1],\n       [ 1,  0, -1]])\n\n\n\nva[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]\n\narray([[3, 2],\n       [2, 1],\n       [3, 0]])"
  },
  {
    "objectID": "cnn/vgg-minst.html",
    "href": "cnn/vgg-minst.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nmodel = VGG16()\n\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 362s 1us/step\n\n\n\nmodel.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n&lt;matplotlib.image.AxesImage at 0x7f96c1be8668&gt;\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n&lt;matplotlib.image.AxesImage at 0x7f96c16642b0&gt;\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f96c0af1da0&gt;\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "cnn/convolution-operation-stride.html",
    "href": "cnn/convolution-operation-stride.html",
    "title": "CIFAR",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n&lt;matplotlib.image.AxesImage at 0x7f7132292e80&gt;\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_animation(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n    ani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                             frames=zip(i.flat, j.flat), interval=frate)\n    ani.save(fname, writer=\"imagemagick\")\n\n\ncreate_animation(a, kernel, p, s, 'demo.gif', 400)\n\n\n\n\n\nfrom keras.datasets import mnist\n\nUsing TensorFlow backend.\n\n\n\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\ncreate_animation(x_train[0], kernel, 0, 1, 'mnist.gif', 2, (20, 4))\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_static(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n     \n    frames=zip(i.flat, j.flat)\n    animate(frames)\n    fig.savefig(fname)\n\n\nfrom keras import backend as K\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel_vertical_edge = keras.Sequential()\nmodel_vertical_edge.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='linear', input_shape=(28, 28, 1)))\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_vertical_edge.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-linear.pdf\", transparent=True)\n\n\n\n\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap((x_train[2:3]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-4.pdf\", transparent=True)\n\n\n\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_conv.T.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap((x_train[0:1]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-5.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap((x_train[5:6]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-2.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap((x_train[6:7]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-1.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\nfrom keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 91s 1us/step\n\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nmodel_horizontal_edge_relu.layers[0].get_weights()[0].shape\n\n(3, 3, 3, 1)\n\n\n\nfilter_3d_horizontal = np.empty((3, 3, 3))\nfilter_3d_horizontal[:] = filter_conv.T\n\n\nfilter_3d_horizontal\n\narray([[[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]]])\n\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_horizontal.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow(x_train[4])\nplt.title(y_train[4])\nplt.savefig(\"cifar-10-car.pdf\", transparent=True)\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 0], cmap='Reds')\nplt.savefig(\"cifar-10-car-red.pdf\", transparent=True)\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 1], cmap='Greens')\nplt.savefig(\"cifar-10-car-green.pdf\", transparent=True)\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 2], cmap='Blues')\nplt.savefig(\"cifar-10-car-blue.pdf\", transparent=True)\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f70d40ae278&gt;\n\n\n\n\n\n\nx_train.shape[1:]\n\n(32, 32, 3)\n\n\n\nmodel_horizontal_edge_relu.predict(x_train[4:5]).shape\n\n(1, 30, 30, 1)\n\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nfilter_3d_vertical = np.empty((3, 3, 3))\nfilter_3d_vertical[:] = filter_conv\nfilter_3d_vertical = filter_3d_vertical\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_vertical.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow((filter_3d_horizontal+1)/2)\n\n&lt;matplotlib.image.AxesImage at 0x7f711ae48438&gt;\n\n\n\n\n\n\nplt.imshow(((filter_3d_vertical+1)/2).T)\n\n&lt;matplotlib.image.AxesImage at 0x7f711a4deb38&gt;\n\n\n\n\n\n\nfilter_3d_vertical\n\narray([[[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]]])\n\n\n\n(filter_3d_vertical+1)/2\n\narray([[[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]]])\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f711814d6a0&gt;\n\n\n\n\n\n\nmodel_vertical_edge_relu.layers[0].get_weights()[0][0].shape\n\n(3, 3, 1)\n\n\n\nimport scipy\nimg = x_train[6:7].reshape(32, 32, 3)\nfrom skimage import color\nimg = color.rgb2gray(img)\nsharpen_kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\nimage_sharpen = scipy.signal.convolve2d(img, sharpen_kernel, 'valid')\n\n\nfrom skimage import io\n\n\nbeach = io.imread(\"beach.jpg\")\n\n\nbeach.shape\n\n(1704, 2272, 3)\n\n\n\nbuildings = io.imread(\"buildings.jpg\")\n\n\nbuildings.shape\n\n(1704, 2272, 3)\n\n\n\nplt.imshow(beach)\nplt.axis('OFF')\n\n(-0.5, 2271.5, 1703.5, -0.5)\n\n\n\n\n\n\nplt.imshow(beach[:, :, 0], cmap='Reds')\nplt.axis('off')\n\n(-0.5, 2271.5, 1703.5, -0.5)\n\n\n\n\n\n\nplt.imshow(beach[:, :, 1], cmap='Greens')\n\n&lt;matplotlib.image.AxesImage at 0x7f711bad72e8&gt;\n\n\n\n\n\n\nplt.imshow(beach[:, :, 2], cmap='Blues')\n\n&lt;matplotlib.image.AxesImage at 0x7f711baf92e8&gt;\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_red = scipy.signal.convolve2d(beach[:, :, 0], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_red, cmap='Greys')\n\n&lt;matplotlib.image.AxesImage at 0x7f7117df2080&gt;\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_green = scipy.signal.convolve2d(beach[:, :, 1], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_green, cmap='Greens')\nplt.axis('off')\n\n(-0.5, 2269.5, 1701.5, -0.5)\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_blue = scipy.signal.convolve2d(beach[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_blue, cmap='Blues')\nplt.axis('off')\n\n(-0.5, 2269.5, 1701.5, -0.5)\n\n\n\n\n\n\nimage_out_buildings_blue = scipy.signal.convolve2d(buildings[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_buildings_blue, cmap='Greys')\nplt.axis('off')\n\n(-0.5, 2269.5, 1701.5, -0.5)\n\n\n\n\n\n\nhorizontal_kernel = vertical_kernel.T\nhorizontal_kernel\n\narray([[ 1,  1,  1],\n       [ 0,  0,  0],\n       [-1, -1, -1]])\n\n\n\nimage_out_buildings_blue_horizontal = scipy.signal.convolve2d(buildings[:, :, 2], horizontal_kernel, 'valid')\nplt.imshow(image_out_buildings_blue_horizontal, cmap='Greys')\nplt.axis('off')\n\n(-0.5, 2269.5, 1701.5, -0.5)"
  },
  {
    "objectID": "bayesian/linear-mle-map-blr/readme.html",
    "href": "bayesian/linear-mle-map-blr/readme.html",
    "title": "Slides from Maths for ML",
    "section": "",
    "text": "Slides from Maths for ML"
  },
  {
    "objectID": "bayesian/RidgeLasso/readme.html",
    "href": "bayesian/RidgeLasso/readme.html",
    "title": "Slides for Ridge and Lasso from the Bayesian Perspective",
    "section": "",
    "text": "Slides for Ridge and Lasso from the Bayesian Perspective"
  },
  {
    "objectID": "notebooks/perceptron-learning.html",
    "href": "notebooks/perceptron-learning.html",
    "title": "XOR using feature transformation",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_and = np.array([0, 0, 0, 1])\ny_or = np.array([0, 1, 1, 1])\ny_xor = np.array([0, 1, 1, 0])\n\n\nclass Perceptron(object):\n    def __init__(self, lr=0.01, iterations=100):\n        self.lr = lr\n        self.iterations = iterations\n        \n    def activation(self, z):\n        ac = np.zeros_like(z)\n        ac[z&gt;0] = 1\n        return ac\n    \n    def fit(self, X, y):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        self.W = np.zeros((X_with_one.shape[1], 1))\n        for i in range(self.iterations):\n            for j in range(len(X)):\n                summation = (X_with_one@self.W).flatten()\n                y_hat = self.activation(summation)\n                err = y - y_hat.flatten() \n                self.W = self.W + (self.lr*err[j]*X_with_one[j]).reshape(*(self.W.shape))\n        \n    def predict(self, X):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        summation = (X_with_one@self.W).flatten()\n        y_hat = self.activation(summation)           \n        return y_hat\n\n    \n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X, y_or)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 1., 1., 1.])\n\n\n\nperceptron.fit(X, y_and)\n\n\nperceptron.W\n\narray([[-0.02],\n       [ 0.02],\n       [ 0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 0., 0., 1.])\n\n\n\nperceptron.fit(X, y_xor)\n\n\nperceptron.W\n\narray([[ 0.01],\n       [-0.01],\n       [ 0.  ]])\n\n\n\nperceptron.predict(X)\n\narray([1., 1., 0., 0.])\n\n\n\n# Transformation: 1 \n# x1, x2, x1x2\nX_xor_1 = np.append(X, (X[:, 0]*X[:, 1]).reshape(-1, 1), axis=1)\n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X_xor_1, y_xor)\n\n\nperceptron.W\n\narray([[ 0.  ],\n       [ 0.01],\n       [ 0.01],\n       [-0.04]])\n\n\n\nnp.allclose(perceptron.predict(X_xor_1), y_xor)\n\nTrue\n\n\n\n(X[:, 0]*X[:, 1]).reshape(-1, 1)\n\narray([[0],\n       [0],\n       [0],\n       [1]])\n\n\n\n# Transformation: 1 \n# x1, x2, x1x2\n\n\nX_xor_2 = np.array([(1-X[:, 0])*X[:,1], (1-X[:, 1])*X[:,0]]).T\n\n\nperceptron = Perceptron()\nperceptron.fit(X_xor_2, y_xor)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])"
  },
  {
    "objectID": "notebooks/Gradient Descent.html",
    "href": "notebooks/Gradient Descent.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000) y = x**2 plt.plot(x,y) plt.title(“Cost Function”)\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/dt.html",
    "href": "notebooks/dt.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "\\[\\operatorname{Entropy} \\equiv \\sum_i(-p_i\\log_2{p_i})\n\\]\n\\[\\operatorname{Gain}(S, A) \\equiv \\text { Entropy }(S)-\\sum_{v \\in V a l u e s(A)} \\frac{\\left|S_{v}\\right|}{|S|} \\text {Entropy}\\left(S_{v}\\right)\n\\]\n\nimport pandas as pd\nimport numpy as np\n\n\ndef entropy(s):\n    \"\"\"\n    s: pd.Series of dtype category\n    returns: entropy\n    \"\"\"\n    p = s.value_counts()/len(s)\n    entropy_df = pd.DataFrame({\"p\":p, \"-plogp\":-p*np.log2(p)})\n    return entropy_df['-plogp'].sum()\n\n\ns = pd.Series(['a','a','c','d'], dtype='category')\n\n\n\nentropy(s)\n\n1.5\n\n\n\n\n\n0.0\n\n\n\ndf = pd.read_csv(\"/Users/nipun/Desktop/tennis.csv\", index_col=0)\n\n\nattribute = 'Outlook'\noutput_variable = 'PlayTennis'\n\n\ndef ig(df, attribute, output_variable):\n    l = df.groupby(attribute)\n    df2 = l.agg({attribute:'count', output_variable:entropy})\n    df2['WeightedEntropy'] = (df2[attribute]/df2[attribute].sum())*df2[output_variable]\n    return entropy(df[output_variable])- df2['WeightedEntropy'].sum()\n\n\nig2 = pd.Series({attribute: ig(df, attribute,'PlayTennis') for attribute in df.columns if attribute!=output_variable})\n\n\nig2.argmax()\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: \nThe current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\ninstead.\nThe behavior of 'argmax' will be corrected to return the positional\nmaximum in the future. For now, use 'series.values.argmax' or\n'np.argmax(np.array(values))' to get the position of the maximum\nrow.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n'Outlook'\n\n\n\npd.crosstab(df[attribute], df[output_variable])\n\n\n\n\n\n\n\nPlayTennis\nNo\nYes\n\n\nOutlook\n\n\n\n\n\n\nOvercast\n0\n4\n\n\nRain\n2\n3\n\n\nSunny\n3\n2\n\n\n\n\n\n\n\n\nl = df.groupby(attribute)\n\n\n%matplotlib inline\n\n\ndf2 = l.agg({attribute:'count', output_variable:entropy})\n\n\ndf2['WeightedEntropy'] = (df2['Outlook']/df2['Outlook'].sum())*df2[output_variable]\n\n\ndf2['WeightedEntropy'].sum()\n\n0.6935361388961918"
  },
  {
    "objectID": "notebooks/assignment3.html",
    "href": "notebooks/assignment3.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nnp.random.seed(0)\n\n\nx = np.linspace(0, 10, 30)\n\n\ny = np.sin(x) + 10*x**2 - 3*x**3 + 0.2*x**4 + 15*np.random.randn(*x.shape)\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.scatter(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n\ndeg = 6\npoly = PolynomialFeatures(deg, include_bias=False)\n\n\nlr = LinearRegression()\nlr.fit(poly.fit_transform(x.reshape(-1, 1)), y)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\n\nplt.plot(x, lr.predict(poly.fit_transform(x.reshape(-1, 1))),color='k',label='Fit (degree: {})'.format(deg))\nplt.scatter(x, y)\nlr.coef_\n\narray([ 3.06126005e+01, -1.75210076e+01,  8.20343045e+00, -2.00647784e+00,\n        2.03804932e-01, -7.05035168e-03])\n\n\n\n\n\n\npoly.fit_transform(x.reshape(-1, 1)).shape\n\n(1000, 30)"
  },
  {
    "objectID": "notebooks/logistic-iris.html",
    "href": "notebooks/logistic-iris.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n\n\nfrom sklearn.datasets import load_iris\n\n\nd = load_iris()\nX = d['data'][:, :2]\ny = d['target']\n\n\nd['feature_names']\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\nlatexify()\ncolours = ['blue', 'red', 'green']\nspecies = ['I. setosa', 'I. versicolor', 'I. virginica']\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i] ,\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\n\n\nplt.savefig(\"logisitic-iris.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(X, y)\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n  \"this warning.\", FutureWarning)\n\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\nclf.coef_\n\narray([[-82.05203507,  70.39156348],\n       [  0.129441  ,  -3.21271681],\n       [  2.60252889,  -0.74578564]])\n\n\n\nX.shape\n\n(150, 2)\n\n\n\ny.shape\n\n(150,)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\n#plt.scatter(X[:, 0], X[:, 1], c=y)\nlatexify()\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i],\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\nplt.savefig(\"logisitic-iris-prediction.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/logistic-regression-cost.html",
    "href": "notebooks/logistic-regression-cost.html",
    "title": "Likelihood",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\n\n\nX = np.array([\n    [1],\n    [2],\n    [3],\n    [4],\n    [5],\n    [6]\n])\n\ny = np.array([1, 1, 1, 0, 0, 0])\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nlr = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nlr.fit(X, y)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\nlr.coef_\n\narray([[-18.33148189]])\n\n\n\nlr.intercept_\n\narray([64.11147504])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(z))\n\n\ntheta_0_li, theta_1_li = np.meshgrid(np.linspace(-10, 10, 200), np.linspace(-10, 10, 200))\n\n\ndef cost_rmse(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    err = np.sum((y-y_hat)**2)\n    return err\n\n\nz = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z[i, j] = cost_rmse(theta_0_li[i, j], theta_1_li[i, j])\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z)\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nplt.colorbar()\nplt.title('RMSE contour plot')\nformat_axes(plt.gca())\nplt.savefig(\"logistic-sse-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z)\nax.set_title('RMSE surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"logistic-sse-loss-3d.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\nimport pandas as pd\n\n\npd.DataFrame(z).min().min()\n\n9.01794626038055\n\n\n\ndef cost_2(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    \n    err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n    return err\n\n\nz2 = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z2[i, j] = cost_2(theta_0_li[i, j], theta_1_li[i, j])\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n  after removing the cwd from sys.path.\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n  after removing the cwd from sys.path.\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z2)\nax.set_title('Cross-entropy surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"logistic-cross-loss-surface.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Z contains NaN values. This may result in rendering artifacts.\n  import sys\n\n\n\n\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z2)\nplt.title('Cross-entropy contour plot')\nplt.colorbar()\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nformat_axes(plt.gca())\nplt.savefig(\"logistic-cross-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n#y = 0\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 0\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"logistic-cross-cost-0.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n  This is separate from the ipykernel package so we can avoid doing imports until\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n  This is separate from the ipykernel package so we can avoid doing imports until\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\n\n\n\n#y = 1\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 1\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"logistic-cross-cost-1.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n  This is separate from the ipykernel package so we can avoid doing imports until\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log\n  This is separate from the ipykernel package so we can avoid doing imports until\n/Users/nipun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\n\n\n\nX_with_one = np.hstack((np.ones_like(X), X))\n\n\n\\[\\begin{align*}\nP(y | X, \\theta) &= \\prod_{i=1}^{n} P(y_{i} | x_{i}, \\theta) \\\\ &= \\prod_{i=1}^{n} \\Big\\{\\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{y_{i}}\\Big\\{1 - \\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{1 - y_{i}} \\\\\n\\end{align*}\\]\n\nX_with_one[1]\n\narray([1, 2])\n\n\n\ndef likelihood(theta_0, theta_1):\n    s = 1\n\n    for i in range(len(X)):\n        y_i_hat = sigmoid(-X_with_one[i]@np.array([theta_0, theta_1]))\n        s = s* ((y_i_hat**y[i])*(1-y_i_hat)**(1-y[i]))\n    \n    \n    return s\n\nx_grid_2, y_grid_2 = np.mgrid[-5:100:0.5, -30:10:.1]\n\nli = np.zeros_like(x_grid_2)\nfor i in range(x_grid_2.shape[0]):\n    for j in range(x_grid_2.shape[1]):\n        li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j])\n        \n\n\nplt.contourf(x_grid_2, y_grid_2, li)\n#plt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.colorbar()\nplt.scatter(lr.intercept_[0], lr.coef_[0], s=200, marker='*', color='r', label='MLE')\nplt.title(r\"Likelihood as a function of ($\\theta_0, \\theta_1$)\")\n#plt.gca().set_aspect('equal')\nplt.legend()\nplt.savefig(\"logistic-likelihood.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/SFS_and_BFS.html",
    "href": "notebooks/SFS_and_BFS.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score as acc\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.datasets import fetch_california_housing\n\n\n\nfrom mlxtend.feature_selection import Sequential\n\n\n\n# Read data\ndata =  fetch_california_housing()\nX = data['data']\ny = data['target']\n\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.25,\n    random_state=42)\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\nprint('Training dataset shape:', X_train.shape, y_train.shape)\nprint('Testing dataset shape:', X_test.shape, y_test.shape)\n\nTraining dataset shape: (15480, 8) (15480,)\nTesting dataset shape: (5160, 8) (5160,)\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsfs1 = sfs(clf,\n           k_features=6,\n           forward=True,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sfs1.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.1s finished\n\n[2020-01-18 12:14:41] Features: 1/6 -- score: -0.9799855743872914[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.4s finished\n\n[2020-01-18 12:14:42] Features: 2/6 -- score: -0.6329184295861372[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.6s finished\n\n[2020-01-18 12:14:44] Features: 3/6 -- score: -0.6522227062026185[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:45] Features: 4/6 -- score: -0.6627208539646012[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:47] Features: 5/6 -- score: -0.6800772168838566[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.7s finished\n\n[2020-01-18 12:14:49] Features: 6/6 -- score: -0.6988981779449276\n\n\n\nfeat_cols = list(sfs1.k_feature_idx_)\n\n# data['features']\n\n\nnp.array(data['feature_names'])[feat_cols]\n\narray(['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'Latitude',\n       'Longitude'], dtype='&lt;U10')\n\n\n\ndata['feature_names']\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsbs = sfs(clf,\n           k_features=1,\n           forward=False,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sbs.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    5.2s finished\n\n[2020-01-18 12:26:50] Features: 7/1 -- score: -0.7097202737310716[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    3.9s finished\n\n[2020-01-18 12:26:54] Features: 6/1 -- score: -0.6985180206966245[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.8s finished\n\n[2020-01-18 12:26:57] Features: 5/1 -- score: -0.6766309576585353[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n\n[2020-01-18 12:26:58] Features: 4/1 -- score: -0.6683220592138266[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.2s finished\n\n[2020-01-18 12:27:00] Features: 3/1 -- score: -0.6613854217987167[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n\n[2020-01-18 12:27:00] Features: 2/1 -- score: -0.6320510743499647[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n\n[2020-01-18 12:27:00] Features: 1/1 -- score: -0.9799855743872914"
  },
  {
    "objectID": "notebooks/anscombe.html",
    "href": "notebooks/anscombe.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/entropy.html",
    "href": "notebooks/entropy.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\nfrom scipy.special import xlogy\n\n# Function to calculate entropy\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Generate data\nx_values = np.linspace(0.000, 1.0, 100)  # Avoid log(0) in the calculation\ny_values = entropy(x_values)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_19079/845472961.py:6: RuntimeWarning: divide by zero encountered in log2\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_19079/845472961.py:6: RuntimeWarning: invalid value encountered in multiply\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\n\ny_values\n\narray([       nan, 0.08146203, 0.14257333, 0.19590927, 0.24414164,\n       0.28853851, 0.32984607, 0.36855678, 0.40502013, 0.43949699,\n       0.47218938, 0.50325833, 0.53283506, 0.56102849, 0.58793037,\n       0.61361902, 0.63816195, 0.66161791, 0.68403844, 0.70546904,\n       0.72595015, 0.74551784, 0.76420451, 0.78203929, 0.79904852,\n       0.81525608, 0.83068364, 0.84535094, 0.85927598, 0.87247521,\n       0.88496364, 0.89675502, 0.90786192, 0.91829583, 0.92806728,\n       0.93718586, 0.9456603 , 0.95349858, 0.9607079 , 0.96729478,\n       0.97326507, 0.97862399, 0.98337619, 0.98752571, 0.99107606,\n       0.99403021, 0.99639062, 0.99815923, 0.9993375 , 0.9999264 ,\n       0.9999264 , 0.9993375 , 0.99815923, 0.99639062, 0.99403021,\n       0.99107606, 0.98752571, 0.98337619, 0.97862399, 0.97326507,\n       0.96729478, 0.9607079 , 0.95349858, 0.9456603 , 0.93718586,\n       0.92806728, 0.91829583, 0.90786192, 0.89675502, 0.88496364,\n       0.87247521, 0.85927598, 0.84535094, 0.83068364, 0.81525608,\n       0.79904852, 0.78203929, 0.76420451, 0.74551784, 0.72595015,\n       0.70546904, 0.68403844, 0.66161791, 0.63816195, 0.61361902,\n       0.58793037, 0.56102849, 0.53283506, 0.50325833, 0.47218938,\n       0.43949699, 0.40502013, 0.36855678, 0.32984607, 0.28853851,\n       0.24414164, 0.19590927, 0.14257333, 0.08146203,        nan])\n\n\n\n# Replace NaN values with 0\ny_values = np.nan_to_num(y_values, nan=0.0)\n\n\nlatexify(columns=1)\n\n\nplt.plot(x_values, y_values, color='black')\n\n# Set labels and title\nplt.xlabel('$P(+)$')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. $P(+)$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/entropy.pdf\")\n\n\n\n\n\n# Function to calculate entropy with numerical stability\ndef entropy_numerically_stable(p):\n    return (-xlogy(p, p) - xlogy(1 - p, 1 - p))/np.log(2)\n\ny_values = entropy_numerically_stable(x_values)\n\n\nplt.plot(x_values, y_values)\n\n\n\n\nHow does xlogy handle the corner case?\n\nxlogy??\n\nCall signature:  xlogy(*args, **kwargs)\nType:            ufunc\nString form:     &lt;ufunc 'xlogy'&gt;\nFile:            ~/miniconda3/lib/python3.9/site-packages/numpy/__init__.py\nDocstring:      \nxlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nxlogy(x, y, out=None)\n\nCompute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n\nParameters\n----------\nx : array_like\n    Multiplier\ny : array_like\n    Argument\nout : ndarray, optional\n    Optional output array for the function results\n\nReturns\n-------\nz : scalar or ndarray\n    Computed x*log(y)\n\nNotes\n-----\nThe log function used in the computation is the natural log.\n\n.. versionadded:: 0.13.0\n\nExamples\n--------\nWe can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n.. math::\n    L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i))\n\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.special import xlogy\n&gt;&gt;&gt; y = np.array([0, 1, 0, 1, 1, 0])\n&gt;&gt;&gt; y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\n&gt;&gt;&gt; n = len(y)\n&gt;&gt;&gt; loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\n&gt;&gt;&gt; loss /= n\n&gt;&gt;&gt; loss\n0.29597052165495025\n\nA lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilties are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.\nClass docstring:\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\n**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays."
  },
  {
    "objectID": "notebooks/ML Notebook.html",
    "href": "notebooks/ML Notebook.html",
    "title": "Nipun Batra Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n\na = np.linspace(-10,10,1000)\n\n\npoints = np.linspace(0,10,100)\n\n\nplt.figure(figsize=(20,8))\nplt.plot(a,a**2)\nplt.annotate(\"Minima\", xy=(0,0), xytext=(20,0), arrowprops=dict(arrowstyle=\"-&gt;\"))\nplt.annotate(\"Starting point\", xy=(np.sqrt(80),80), xytext=(20,80), arrowprops=dict(arrowstyle=\"-&gt;\"))\n# for i in range(len(points)-1):\n#     #print (points[i],points[i]**2)\n#     plt.arrow(points[i], f(points[i]), points[i+1], f(points[i+1])-f(points[i]), shape='full', lw=0, length_includes_head=True, head_width=.05)\nfor i in range(1,10):\n    plt.arrow(i,f(i),i,f(i),shape='full', lw=0, length_includes_head=True, head_width=.5)\nplt.xlim(-20,40)\n\n(-20, 40)\n\n\n\n\n\n\ndef f(t): return t**2\n\nt = np.linspace(-2, 2, 100)\nplt.plot(t, f(t))\nplt.arrow(0, f(0), 0.01, f(0.01)-f(0), shape='full', lw=0, length_includes_head=True, head_width=.05)\nplt.show()"
  },
  {
    "objectID": "notebooks/05_05_Naive_Bayes.html",
    "href": "notebooks/05_05_Naive_Bayes.html",
    "title": "Gaussian Naive Bayes",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom latexify import *\nPerhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes. In this classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution. Imagine that you have the following data:\nlatexify()\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='RdBu');\nformat_axes(plt.gca())\nax.set_aspect('equal')\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n\nfig, ax = plt.subplots()\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='RdBu')\nax.set_title('Naive Bayes Model', size=14)\n\nxlim = (-8, 8)\nylim = (-15, 5)\n\nxg = np.linspace(xlim[0], xlim[1], 60)\nyg = np.linspace(ylim[0], ylim[1], 40)\nxx, yy = np.meshgrid(xg, yg)\nXgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n\nfor label, color in enumerate(['red', 'blue']):\n    mask = (y == label)\n    mu, std = X[mask].mean(0), X[mask].std(0)\n    P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n    Pm = np.ma.masked_array(P, P &lt; 0.03)\n    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5,\n                  cmap=color.title() + 's')\n    ax.contour(xx, yy, P.reshape(xx.shape),\n               levels=[0.01, 0.1, 0.5, 0.9],\n               colors=color, alpha=0.2)\n    \nax.set(xlim=xlim, ylim=ylim)\nformat_axes(ax)\nax.set_aspect('equal')\nOne extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution. The result of this naive Gaussian assumption is shown in the following figure:\nfigure source in Appendix\nThe ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses. With this generative model in place for each class, we have a simple recipe to compute the likelihood \\(P({\\rm features}~|~L_1)\\) for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point.\nThis procedure is implemented in Scikit-Learn’s sklearn.naive_bayes.GaussianNB estimator:\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);\nNow let’s generate some new data and predict the label:\nrng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\nNow we can plot this new data to get an idea of where the decision boundary is:\nplt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap='RdBu')\nlim = plt.axis()\nplt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=5, cmap='RdBu', alpha=0.2)\nplt.axis(lim);\nformat_axes(plt.gca())\nax.set_aspect('equal')\nWe see a slightly curved boundary in the classifications—in general, the boundary in Gaussian naive Bayes is quadratic.\nA nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the predict_proba method:\nyprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n\narray([[ 0.89,  0.11],\n       [ 1.  ,  0.  ],\n       [ 1.  ,  0.  ],\n       [ 1.  ,  0.  ],\n       [ 1.  ,  0.  ],\n       [ 1.  ,  0.  ],\n       [ 0.  ,  1.  ],\n       [ 0.15,  0.85]])\nThe columns give the posterior probabilities of the first and second label, respectively. If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a useful approach.\nOf course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results. Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method."
  },
  {
    "objectID": "notebooks/05_05_Naive_Bayes.html#multinomial-naive-bayes",
    "href": "notebooks/05_05_Naive_Bayes.html#multinomial-naive-bayes",
    "title": "Gaussian Naive Bayes",
    "section": "Multinomial Naive Bayes",
    "text": "Multinomial Naive Bayes\nThe Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label. Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution. The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\nThe idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution.\n\nExample: Classifying Text\nOne place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified. We discussed the extraction of such features from text in Feature Engineering; here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.\nLet’s download the data and take a look at the target names:\n\nfrom sklearn.datasets import fetch_20newsgroups\n\ndata = fetch_20newsgroups()\ndata.target_names\n\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']\n\n\nFor simplicity here, we will select just a few of these categories, and download the training and testing set:\n\ncategories = ['talk.religion.misc', 'soc.religion.christian',\n              'sci.space', 'comp.graphics']\ntrain = fetch_20newsgroups(subset='train', categories=categories)\ntest = fetch_20newsgroups(subset='test', categories=categories)\n\nHere is a representative entry from the data:\n\nprint(train.data[5])\n\nFrom: dmcgee@uluhe.soest.hawaii.edu (Don McGee)\nSubject: Federal Hearing\nOriginator: dmcgee@uluhe\nOrganization: School of Ocean and Earth Science and Technology\nDistribution: usa\nLines: 10\n\n\nFact or rumor....?  Madalyn Murray O'Hare an atheist who eliminated the\nuse of the bible reading and prayer in public schools 15 years ago is now\ngoing to appear before the FCC with a petition to stop the reading of the\nGospel on the airways of America.  And she is also campaigning to remove\nChristmas programs, songs, etc from the public schools.  If it is true\nthen mail to Federal Communications Commission 1919 H Street Washington DC\n20054 expressing your opposition to her request.  Reference Petition number\n\n2493.\n\n\n\nIn order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers. For this we will use the TF-IDF vectorizer (discussed in Feature Engineering), and create a pipeline that attaches it to a multinomial naive Bayes classifier:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(TfidfVectorizer(), MultinomialNB())\n\nWith this pipeline, we can apply the model to the training data, and predict labels for the test data:\n\nmodel.fit(train.data, train.target)\nlabels = model.predict(test.data)\n\nNow that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, here is the confusion matrix between the true and predicted labels for the test data:\n\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(test.target, labels)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=train.target_names, yticklabels=train.target_names)\nplt.xlabel('true label')\nplt.ylabel('predicted label');\n\n\n\n\nEvidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity. This is perhaps an expected area of confusion!\nThe very cool thing here is that we now have the tools to determine the category for any string, using the predict() method of this pipeline. Here’s a quick utility function that will return the prediction for a single string:\n\ndef predict_category(s, train=train, model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n\nLet’s try it out:\n\npredict_category('sending a payload to the ISS')\n\n'sci.space'\n\n\n\npredict_category('discussing islam vs atheism')\n\n'soc.religion.christian'\n\n\n\npredict_category('determining the screen resolution')\n\n'comp.graphics'\n\n\nRemember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking. Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective."
  },
  {
    "objectID": "notebooks/05_05_Naive_Bayes.html#when-to-use-naive-bayes",
    "href": "notebooks/05_05_Naive_Bayes.html#when-to-use-naive-bayes",
    "title": "Gaussian Naive Bayes",
    "section": "When to Use Naive Bayes",
    "text": "When to Use Naive Bayes\nBecause naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages:\n\nThey are extremely fast for both training and prediction\nThey provide straightforward probabilistic prediction\nThey are often very easily interpretable\nThey have very few (if any) tunable parameters\n\nThese advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification. If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem. If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\nNaive Bayes classifiers tend to perform especially well in one of the following situations:\n\nWhen the naive assumptions actually match the data (very rare in practice)\nFor very well-separated categories, when model complexity is less important\nFor very high-dimensional data, when model complexity is less important\n\nThe last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful.\n\n&lt; Feature Engineering | Contents | In Depth: Linear Regression &gt;"
  }
]