[
  {
    "objectID": "ridge/Ridge.html",
    "href": "ridge/Ridge.html",
    "title": "Question",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom mpl_toolkits import mplot3d\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n\n%matplotlib inline\n# Based on: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\n\nimport sys\nsys.path.append(\"../\")\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify()\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nformat_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\ncoef_list = []\nfor i,deg in enumerate([1,3,6,11]):\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=False, fit_intercept=False)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    coef_list.append(abs(max(regressor.coef_, key=abs)))\n\n    plt.scatter(data['x'],data['y'], label='Train')\n    plt.plot(data['x'], y_pred,'k', label='Prediction')\n    plt.plot(data['x'], y_true,'g.', label='True Function')\n    format_axes(plt.gca())\n    plt.legend() \n    plt.title(f\"Degree: {deg} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n    plt.savefig('lin_plot_{}.pdf'.format(deg), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\nplt.semilogy([1,3,6,11],coef_list,'o-k')\nplt.xticks([1,3,6,11])\nplt.xlabel('Degree')\nplt.ylabel('Max Coef')\nformat_axes(plt.gca())\nplt.savefig('lin_plot_coef.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\nfrom matplotlib.patches import Circle\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.4)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Circle((0, 0), 3, color='g', label=r'$\\theta_0^2+\\theta_1^2=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig('ridge_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes(projection='3d')\n\nax = plt.axes(projection='3d')\nax.plot_surface(x_grid, y_grid, cost_matrix,cmap='viridis', edgecolor='none')\nax.set_title('Least squares objective function');\nax.set_xlabel(r\"$\\theta_0$\")\nax.set_ylabel(r\"$\\theta_1$\")\nax.set_xlim([-4,15])\nax.set_ylim([-4,15])\n\nu = np.linspace(0, np.pi, 30)\nv = np.linspace(0, 2 * np.pi, 30)\n\n# x = np.outer(500*np.sin(u), np.sin(v))\n# y = np.outer(500*np.sin(u), np.cos(v))\n# z = np.outer(500*np.cos(u), np.ones_like(v))\n# ax.plot_wireframe(x, y, z)\n\nax.view_init(45, 120)\nplt.savefig('ridge_base_surface.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nlatexify(fig_width=5, fig_height=2.5)\nfrom sklearn.linear_model import Ridge\n\nfor alpha in [1, 10, 1000]:\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Ridge(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    format_axes(ax[0])\n    format_axes(ax[1])\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    p1 = Circle((0, 0), np.sqrt(regressor.coef_.T@regressor.coef_), alpha=0.6, color='g', label=r'$\\theta_0^2+\\theta_1^2={:.2f}$'.format(np.sqrt(regressor.coef_.T@regressor.coef_)))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.3)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Ridge Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n    ax[1].set_aspect('equal')\n    plt.savefig('ridge_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\nlatexify()\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        format_axes(plt.gca())\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('ridge_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.62699e-21): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\n# from sklearn.linear_model import Ridge\n\n# for i,deg in enumerate([2,4,8,16]):\n#   predictors = ['x']\n#   if deg &gt;= 2:\n#     predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n#   fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20, 4))\n\n#   for i,alpha in enumerate([1e-15,1e-4,1,20]):\n#     regressor = Ridge(alpha=alpha,normalize=True)\n#     regressor.fit(data[predictors],data['y'])\n#     y_pred = regressor.predict(data[predictors])\n#     ax[i].scatter(data['x'],data['y'], label='Train')\n#     ax[i].plot(data['x'], y_pred,'k', label='Prediction')\n#     ax[i].plot(data['x'], y_true,'g.', label='True Function')\n#     ax[i].legend() \n#     ax[i].set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"data.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-2,10,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Ridge(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nformat_axes(plt.gca())\n\nlim = True\nif lim:\n    plt.ylim((10e-20, 100))\n    plt.savefig('rid_reg-without-lim.pdf', transparent=True, bbox_inches=\"tight\")\nelse:\n    plt.savefig('rid_reg-with-lim.pdf', transparent=True, bbox_inches=\"tight\")\n\n# plt.set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\n\n\n\n\n\n\n\nQuestion\n\nplt.style.use('seaborn-whitegrid')\n\nx = [1,2,3,4]\ny = [1,2,3,0]\ny_1 = [(2-i/5) for i in x]\ny_2 = [(0.5+0.4*i) for i in x]\nplt.ylim(-0.2,3.3)\nplt.plot(x,y,'.')\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\n#plt.plot(x,y_1, label=\"unreg\")\n#plt.plot(x,y_2, label=\"reg\")\n#plt.legend()\n#format_axes(plt.gca())\nplt.savefig('temp.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nx_new = np.vstack([np.ones_like(x), x]).T\nregressor = LinearRegression(normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_unreg.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nregressor = Ridge(alpha=4, normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred_r = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\ndef ridge_func(x):\n    return 0.56 + 0.26*np.array(x)\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.plot(x,ridge_func(x),'-b', label='Regularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_reg.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nregressor.coef_\n\narray([0.37209302, 0.30232558])\n\n\n\n\nRetrying with a better example\n\nnp.linalg.inv([[4, 10], [10, 34]])@np.array([6, 14])\n\narray([ 1.77777778, -0.11111111])\n\n\n\nnp.linalg.inv([[8, 10], [10, 34]])@np.array([6, 14])\n\narray([0.37209302, 0.30232558])\n\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,600,10)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,5,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n#format_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([17]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1e-3, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        #format_axes(plt.gca())\n        #print(regressor.coef_)\n        plt.ylim([0,60])\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha}\")\n        plt.savefig('ridge_new_{}_{}.pdf'.format(i, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.11479e-28): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;"
  },
  {
    "objectID": "knn/knn/curse_dimensionality.html",
    "href": "knn/knn/curse_dimensionality.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n%matplotlib inline\n\n\nnp.random.seed(0)\n\n\nnum_points = 10\n\n\ndata = {}\nfor num_dimensions in range(1, 20):\n    data[num_dimensions] = np.random.uniform(low=0.0, high=1.0, size=(num_points, num_dimensions))\n\n\nplt.scatter(data[1], np.zeros_like(data[1]))\n\n\n\n\n\n\n\n\n\nplt.scatter(data[2][:, 0], data[2][:, 1])\n\n\n\n\n\n\n\n\n\ndist = {}\nfor num_dimensions in range(1, 20):\n    dist[num_dimensions] = pd.DataFrame(euclidean_distances(data[num_dimensions], data[num_dimensions]))\n\n\nmean_distances = pd.Series({num_dimensions: dist[num_dimensions].mean().mean() for num_dimensions in range(1, 20)})\n\n\nmean_distances.plot(style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Mean distance between two points\")\nplt.savefig('curse_dist.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nratio_max_min = pd.Series({num_dimensions:(dist[num_dimensions].replace({0:np.NAN}).max()/dist[num_dimensions].replace({0:np.NAN}).min()).mean() \n                           for num_dimensions in range(1, 20) })\n\n\nratio_max_min.plot(logy=True, style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Ratio of max to min distances\")\nplt.ylim((-1, 180))\nplt.savefig('curse_spread.pdf', transparent=True, bbox_inches=\"tight\")\n\n/home/prof/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Attempted to set non-positive bottom ylim on a log-scaled axis.\nInvalid limit will be ignored.\n  after removing the cwd from sys.path."
  },
  {
    "objectID": "gradient-descent/Gradient Descent.html",
    "href": "gradient-descent/Gradient Descent.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "#import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nsns.despine()\n\nNameError: name 'sns' is not defined\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\nplt.plot(x,y)\nplt.title(\"Cost Function\")\n\nText(0.5, 1.0, 'Cost Function')\n\n\n\n\n\n\n\n\n\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/anscombe.html",
    "href": "notebooks/anscombe.html",
    "title": "Anscombe’s Quartet",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/Gradient Descent-2d.html",
    "href": "notebooks/Gradient Descent-2d.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib inline\n\n\nimport numpy as np\n\n\n4.1*4.1\n\n16.81\n\n\n\n4.1-0.2*2*4.1\n\n2.46\n\n\n\nx = 4.1\nalpha = 0.2\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    print(\"(\",round(x, 2),\",\" ,round(x*x, 2),\")\")\n\n( 2.46 , 6.05 )\n( 1.48 , 2.18 )\n( 0.89 , 0.78 )\n( 0.53 , 0.28 )\n( 0.32 , 0.1 )\n( 0.19 , 0.04 )\n( 0.11 , 0.01 )\n( 0.07 , 0.0 )\n( 0.04 , 0.0 )\n( 0.02 , 0.0 )\n\n\n\nx = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    st = \"\"\"\n    \n    \\begin{frame}{Iteration %d}\n    \\begin{columns}\n\n\n        \\begin{column}{0.6\\textwidth}\n            \\begin{adjustbox}{max totalsize={\\textwidth},center}\n                \\begin{tikzpicture}\n\n                    \\begin{axis}[\n                        xlabel=$x$,\n                        ylabel=$y$,\n                        xmin=-4.2,\n                        xmax=4.2,\n                        axis x line*=bottom,\n                        axis y line*=left,\n                        xtick align=outside,\n                        ytick align=outside,\n                        legend pos=outer north east\n                        ]\n                        \\addplot[mark=none, gray] {x^2};\\addlegendentry{$y=x^2$}\n                        \\addplot[only marks, mark=*]\n                        coordinates{ % plot 1 data set\n                            (%s,%s)\n                            }; \n\n\n\n                        \\end{axis}\n\n                \\end{tikzpicture}\n            \\end{adjustbox}\n        \\end{column}\n    \\begin{column}{0.5\\textwidth}\n    \\begin{adjustbox}{max totalsize={\\textwidth},center}\n        \\begin{tikzpicture}\n        \\begin{axis}\n        [\n        title={Contour plot, view from top},\n        view={0}{90},\n        xlabel=$x$,\n        ylabel=$y$,\n        axis x line*=bottom,\n        axis y line*=left,\n        xtick align=outside,\n        ytick align=outside,\n        unit vector ratio*=1 1 1,\n        ]\n        \\addplot3[\n        contour gnuplot={number=14,}\n        ]\n        {x^2};\n        \\addplot[only marks, mark=*]\n        coordinates{ % plot 1 data set\n            (%f,%f)\n        }; \n        \\end{axis}\n        \\end{tikzpicture}\n        \\end{adjustbox}\n    \\end{column}\n    \\end{columns}\n\n\n    \\end{frame}\n    \"\"\" %(i, i, i, i, i)\n\nValueError: unsupported format character 'p' (0x70) at index 793\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\n8.2*4.1\n\n33.62\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\nlatexify()\nval = -7.2\n\nplt.scatter([val],func(np.array([val])), color='k')\nax.annotate('Local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='grey', shrink=0.0001))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y, color='grey')\nformat_axes(plt.gca())\nplt.xlabel(\"x\")\nplt.ylabel(\"y=f(x)\")\nplt.savefig(\"../gradient-descent/local-minima.eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nimport sys\nsys.path.append(\"../\")\n\n\nfrom latexify import format_axes, latexify\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = 0.95\niterations = 10\nlatexify()\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5, color='grey')\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/basis.html",
    "href": "notebooks/basis.html",
    "title": "Basis Expansion in Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n# Interactive widget\nfrom ipywidgets import interact\n\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\n# Download CO2 data from NOAA\nurl = 'https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv'\n\nnames = 'year,month,decimal date,average,deseasonalized,ndays,sdev,unc'.split(',')\n\n# no index\ndf = pd.read_csv(url, skiprows=72, names=names, index_col=False)\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal date\naverage\ndeseasonalized\nndays\nsdev\nunc\n\n\n\n\n0\n1960\n10\n1960.7896\n313.83\n316.83\n-1\n-9.99\n-0.99\n\n\n1\n1960\n11\n1960.8743\n315.00\n316.88\n-1\n-9.99\n-0.99\n\n\n2\n1960\n12\n1960.9563\n316.19\n316.96\n-1\n-9.99\n-0.99\n\n\n3\n1961\n1\n1961.0411\n316.89\n316.86\n-1\n-9.99\n-0.99\n\n\n4\n1961\n2\n1961.1260\n317.70\n317.08\n-1\n-9.99\n-0.99\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n754\n2023\n8\n2023.6250\n419.68\n421.57\n21\n0.45\n0.19\n\n\n755\n2023\n9\n2023.7083\n418.51\n421.96\n18\n0.30\n0.14\n\n\n756\n2023\n10\n2023.7917\n418.82\n422.11\n27\n0.47\n0.17\n\n\n757\n2023\n11\n2023.8750\n420.46\n422.43\n21\n0.91\n0.38\n\n\n758\n2023\n12\n2023.9583\n421.86\n422.58\n20\n0.69\n0.29\n\n\n\n\n759 rows × 8 columns\n\n\n\n\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\n/tmp/ipykernel_3219587/1556785423.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  df.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\n\n\ndf.average.plot(figsize=(6, 4), title='CO2 Levels at Mauna Loa Observatory')\nplt.xlabel('Year')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')\n\n\n\n\n\n\n\n\n\n\n# Create X and y\n\n# X = months since first measurement\nX = np.array(range(len(df)))\ny = df.average.values\n\n\nplt.plot(X, y)\nplt.xlabel('Months since first measurement')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')\n\n\n\n\n\n\n\n\n\n\n# Normalize X and y\n\ns1 = StandardScaler()\ns2 = StandardScaler()\n\nX_norm = s1.fit_transform(X.reshape(-1, 1))\ny_norm = s2.fit_transform(y.reshape(-1, 1))\n\n\nX_norm.mean(), X_norm.std()\n\n(0.0, 1.0)\n\n\n\ndf = pd.DataFrame({\"x\":X.flatten(), \"transformed\":X_norm.flatten()})\ndf\n\n\n\n\n\n\n\n\nx\ntransformed\n\n\n\n\n0\n0\n-1.729770\n\n\n1\n1\n-1.725206\n\n\n2\n2\n-1.720642\n\n\n3\n3\n-1.716078\n\n\n4\n4\n-1.711514\n\n\n...\n...\n...\n\n\n754\n754\n1.711514\n\n\n755\n755\n1.716078\n\n\n756\n756\n1.720642\n\n\n757\n757\n1.725206\n\n\n758\n758\n1.729770\n\n\n\n\n759 rows × 2 columns\n\n\n\n\ndf[\"re-transformed\"] = s1.inverse_transform(df[\"transformed\"].values.reshape(-1, 1))\ndf\n\n\n\n\n\n\n\n\nx\ntransformed\nre-transformed\n\n\n\n\n0\n0\n-1.729770\n0.0\n\n\n1\n1\n-1.725206\n1.0\n\n\n2\n2\n-1.720642\n2.0\n\n\n3\n3\n-1.716078\n3.0\n\n\n4\n4\n-1.711514\n4.0\n\n\n...\n...\n...\n...\n\n\n754\n754\n1.711514\n754.0\n\n\n755\n755\n1.716078\n755.0\n\n\n756\n756\n1.720642\n756.0\n\n\n757\n757\n1.725206\n757.0\n\n\n758\n758\n1.729770\n758.0\n\n\n\n\n759 rows × 3 columns\n\n\n\n\ndf.mean()\n\nx              379.0\ntransformed      0.0\ndtype: float64\n\n\n\ndf.std()\n\nx              219.248717\ntransformed      1.000659\ndtype: float64\n\n\n\nx_test = np.array([800])\ns1.transform(x_test.reshape(-1, 1))\n\narray([[1.92145988]])\n\n\n\ny_norm.mean(), y_norm.std()\n\n(1.647635329298783e-15, 1.0)\n\n\n\nplt.plot(X_norm, y_norm)\nplt.xlabel('(Normalized) Months since first measurement')\nplt.ylabel('(Normalized) CO2 Level')\n\nText(0, 0.5, '(Normalized) CO2 Level')\n\n\n\n\n\n\n\n\n\n\nTask 1: Interpolation\n\nnp.random.seed(42)\ntrain_idx = np.random.choice(range(len(X_norm)), size=int(len(X_norm) * 0.7), replace=False)\ntest_idx = list(set(range(len(X_norm))) - set(train_idx))\n\nX_train = X[train_idx]\ny_train = y[train_idx]\n\nX_test = X[test_idx]\ny_test = y[test_idx]\n\nX_norm_train = X_norm[train_idx]\ny_norm_train = y_norm[train_idx]\n\nX_norm_test = X_norm[test_idx]\ny_norm_test = y_norm[test_idx]\n\n\nplt.plot(X_norm_train, y_norm_train, 'o', label='train',markersize=1)\nplt.plot(X_norm_test, y_norm_test, 'o', label='test', ms=3)\nplt.xlabel('(Normalized) Months since first measurement')\nplt.ylabel('(Normalized) CO2 Level')\nplt.legend()\n\n\n\n\n\n\n\n\n\nerrors= {}\nX_lin_1d = np.linspace(X_norm.min(), X_norm.max(), 100).reshape(-1, 1)\n\n\n\nModel 1: Vanilla Linear Regression\n\ndef plot_fit_predict(model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n    model.fit(X_norm_train, y_norm_train)\n\n    y_hat_train = model.predict(X_norm_train).reshape(-1, 1)\n    y_hat_test = model.predict(X_norm_test).reshape(-1, 1)\n\n    # Transform back to original scale\n    y_hat_train = s2.inverse_transform(y_hat_train)\n    y_hat_test = s2.inverse_transform(y_hat_test)\n\n    y_hat_lin = s2.inverse_transform(model.predict(X_lin).reshape(-1, 1))\n\n    errors[title] = {\"train\": mean_squared_error(y_train, y_hat_train),\n                     \"test\": mean_squared_error(y_test, y_hat_test)}\n\n    if plot:\n        plt.plot(X_train, y_train, 'o', label='train', markersize=1)\n        plt.plot(X_test, y_test, 'o', label='test', ms=3)\n        plt.plot(s1.inverse_transform(X_lin_1d), y_hat_lin, label='model')\n        plt.xlabel('Months since first measurement')\n        plt.ylabel('CO2 Levels')\n        plt.legend()\n        plt.title('{}\\n Train MSE: {:.2f} | Test MSE: {:.2f}'.format(title, errors[title][\"train\"], errors[title][\"test\"]))\n\n    return errors[title]\n\n\nmodel = LinearRegression()\nplot_fit_predict(model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Linear Regression\")\n\n{'train': 20.261366262795633, 'test': 19.19972185975937}\n\n\n\n\n\n\n\n\n\n\n\nMLP\n\n# use sk-learn for MLP\nmlp_model = MLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter = 10000)\n\nplot_fit_predict(mlp_model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"MLP Regression\")\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n{'train': 4.752028323119785, 'test': 4.665698336426404}\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression of degree “d”\n\ndef create_poly_features(X, d):\n    \"\"\"\n    X is (N, 1) array\n    d is degree of polynomial\n    returns normalized polynomial features of X\n    \"\"\"\n    \n    X_poly = np.zeros((len(X), d))\n    X_poly[:, 0] = X[:, 0]\n    for i in range(1, d):\n        X_poly[:, i] = X[:, 0] ** (i + 1)\n    \n    # Normalize each column\n    X_poly = StandardScaler().fit_transform(X_poly)\n    return X_poly\n\n\nxs = np.linspace(-5, 5, 50).reshape(-1, 1)\npoly_f = create_poly_features(xs, 3)\nfor i in range(3):\n    plt.plot(xs, poly_f[:, i], label='x^{}'.format(i+1))\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef show_poly_features(degree):\n    X_poly = create_poly_features(X_norm, degree)\n    plt.plot(X_norm, X_poly)\n    plt.xlabel('X')\n    plt.ylabel('Polynomial Features')\n    plt.title('Degree {}'.format(degree))\n\nshow_poly_features(2)\n\n\n\n\n\n\n\n\n\ninteract(show_poly_features, degree=(1, 10, 1))\n\n\n\n\n&lt;function __main__.show_poly_features(degree)&gt;\n\n\n\nmodel2 = LinearRegression()\ndegree = 4\nXf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\nXf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\nX_lin_poly = create_poly_features(X_lin_1d, degree)\n\nplot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Polynomial Regression (d={degree})\")\n\n{'train': 4.956984807734834, 'test': 6.3743420617353515}\n\n\n\n\n\n\n\n\n\n\nmodel2.coef_, model2.intercept_\n\n(array([[0.97834069, 0.09963612, 0.01969948, 0.0283343 ]]),\n array([-0.01122412]))\n\n\n\nX_lin_poly.shape\n\n(100, 4)\n\n\n\nX_lin_poly.shape\n\n(100, 4)\n\n\n\nmodel2.coef_.shape\n\n(1, 4)\n\n\n\nX_lin_1d.shape\n\n(100, 1)\n\n\n\nplt.plot(X_lin_1d, model2.intercept_.repeat(len(X_lin_1d)), label='Degree 0')\nplt.plot(X_lin_1d, X_lin_poly[:, 0:1]@model2.coef_[:, 0], label='Degree 1')\nplt.plot(X_lin_1d, X_lin_poly[:, 1:2]@model2.coef_[:, 1], label='Degree 2')\nplt.plot(X_lin_1d, X_lin_poly[:, 2:3]@model2.coef_[:, 2], label='Degree 3')\nplt.plot(X_lin_1d, X_lin_poly[:, 3:4]@model2.coef_[:, 3], label='Degree 4')\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef show_additive(model, X_lin_1d, max_degree):\n    ys = model.intercept_.repeat(len(X_lin_1d))\n    #plt.fill_between(X_lin_1d.squeeze(), s2.inverse_transform(ys.reshape(-1, 1)).squeeze(), alpha=0.1)\n    print(ys.shape, X_lin_1d.shape)\n    label = '{:0.2f}x'.format(model.intercept_[0])\n    \n    for i in range(1, max_degree + 1):\n        yd = X_lin_poly[:, i-1:i]@model.coef_[:, i-1]\n        ys = ys + yd\n        label += ' + {:0.2f} x^{}'.format(model.coef_[:, i-1][0], i)\n    ys = s2.inverse_transform(ys.reshape(-1, 1))\n    plt.plot(X_lin_1d, ys, label = label)\n    plt.plot(X_norm_train, y_train, 'o', label='train', markersize=1)\n    plt.legend()\n\nshow_additive(model2, X_lin_1d, 3)\n\n(100,) (100, 1)\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact, fixed\nm = model2\ninteract(show_additive, model=fixed(m), X_lin_1d=fixed(X_lin_1d), max_degree=(1, len(m.coef_[0]), 1))\n\n\n\n\n&lt;function __main__.show_additive(model, X_lin_1d, max_degree)&gt;\n\n\n\nfor degree in range(1, 10):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Polynomial Regression (d={degree})\", plot=False)\n\n\nerrors_df = pd.DataFrame(errors).T\nerrors_df\n\n\n\n\n\n\n\n\ntrain\ntest\n\n\n\n\nLinear Regression\n20.261366\n19.199722\n\n\nMLP Regression\n4.752028\n4.665698\n\n\nPolynomial Regression (d=4)\n4.956985\n6.374342\n\n\nPolynomial Regression (d=2)\n5.075907\n6.678665\n\n\nPolynomial Regression (d=3)\n5.017752\n6.567621\n\n\nPolynomial Regression (d=6)\n4.805097\n6.390612\n\n\nPolynomial Regression (d=1)\n20.261366\n20.851623\n\n\nPolynomial Regression (d=5)\n4.816000\n6.407422\n\n\nPolynomial Regression (d=7)\n4.767340\n6.491631\n\n\nPolynomial Regression (d=8)\n4.750171\n6.508795\n\n\nPolynomial Regression (d=9)\n4.683092\n6.514263\n\n\n\n\n\n\n\n\nerrors_df.plot(kind='bar', figsize=(12, 6), title='MSE for Train and Test Sets')\n\n\n\n\n\n\n\n\n\n# bias variance tradeoff\n\nerrors_poly = {}\n\nfor degree in range(1, 20):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, degree, plot=False)\n\n    # geting errors for polynomial regression only for plotting\n    errors_poly[degree] = errors[degree]\n\n\nerrors_poly_df = pd.DataFrame(errors_poly).T\nbest_degree = np.argmin(errors_poly_df.test) + 1\nmin_error = errors_poly_df.test[best_degree - 1]    # index of df = degree - 1\nprint(f\"Best degree: {best_degree}, Min error: {min_error}\")\n\n# set figure size\nplt.figure(figsize=(12, 6))\nplt.plot(errors_poly_df.index.values, errors_poly_df.train.values, label='train')\nplt.plot(errors_poly_df.index.values, errors_poly_df.test.values, label='test')\nplt.axvline(best_degree, color='black', linestyle='--', label='best degree')\nplt.xticks(np.arange(min(errors_poly_df.index), max(errors_poly_df.index)+1, 1.0))\nplt.ylim(4.5, 7.5)      # set y limit - to show the difference between train and test clearly\nplt.xlabel('Degree')\nplt.ylabel('MSE')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.show()\n\nBest degree: 4, Min error: 6.567621493533483\n\n\n\n\n\n\n\n\n\n\n\nRidge Regression with polynomial basis\n\n# initiate ridge regression model\nmodel_ridge = Ridge(alpha=0.3)\n\nerrors_ridge = {}\n\nfor degree in range(1, 20):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model_ridge, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f'ridge_{degree}', plot=False)\n\n    # geting errors for polynomial regression only for plotting\n    errors_ridge[degree] = errors[f'ridge_{degree}']\n\n\nerrors_ridge_df = pd.DataFrame(errors_ridge).T\nbest_degree_ridge = np.argmin(errors_ridge_df.test) + 1\nmin_error = errors_ridge_df.test[best_degree_ridge - 1]    # index of df = degree - 1\nprint(f\"Best degree: {best_degree_ridge}, Min error: {min_error}\")\n\n# set figure size\nplt.figure(figsize=(12, 6))\nplt.plot(errors_ridge_df.index.values, errors_ridge_df.train.values, label='train')\nplt.plot(errors_ridge_df.index.values, errors_ridge_df.test.values, label='test')\nplt.axvline(best_degree_ridge, color='black', linestyle='--', label='best degree')\nplt.xticks(np.arange(min(errors_ridge_df.index), max(errors_ridge_df.index)+1, 1.0))\nplt.ylim(4.5, 7.5)      # set y limit - to show the difference between train and test clearly\nplt.xlabel('Degree')\nplt.ylabel('MSE')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.show()\n\nBest degree: 4, Min error: 6.535907094411071\n\n\n\n\n\n\n\n\n\n\nplot_fit_predict(model_ridge, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Ridge Regression with Polynomial Features (d={best_degree_ridge})\", plot=True)\n\n{'train': 4.767435271070607, 'test': 6.39886853568042}\n\n\n\n\n\n\n\n\n\n\n\nGausian Basis Function\n\ndef create_guassian_basis(X , d , std = 1):\n    \"\"\"\n    X is (N, 1) array\n    d is number of basis functions\n    \"\"\"\n    means = np.linspace(X.min(), X.max(), d)\n    X = np.repeat(X, d, axis=1)\n    means = np.repeat(means.reshape(-1, 1), len(X), axis=1).T\n\n    return np.exp(-(X - means) ** 2 / (2 * std ** 2))\n\n\ndef show_gaussian_basis(d, stdev):\n    xs = np.linspace(-5, 5, 100).reshape(-1, 1)\n    X_gauss = create_guassian_basis(xs, d, std=stdev)\n    plt.plot(xs, X_gauss)\n    plt.xlabel('X')\n    plt.ylabel('Gaussian Basis')\n    plt.title('Degree {} Stddev'.format(d, stdev))\n\nshow_gaussian_basis(3, 1)\n\n\n\n\n\n\n\n\n\ninteract(show_gaussian_basis, d=(1, 10, 1), stdev=(0.1, 10, 0.1))\n\n\n\n\n&lt;function __main__.show_gaussian_basis(d, stdev)&gt;\n\n\n\nmodel_gauss = LinearRegression()\ndegree = 4\nXf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), degree)\nXf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), degree)\n\nX_lin_poly = create_guassian_basis(X_lin_1d, degree)\n\n\nplot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d={degree})\")\n\n{'train': 5.27763506266326, 'test': 5.349399604396995}\n\n\n\n\n\n\n\n\n\n\nerrors.clear()\n\n\nfor degree in range(3, 7):\n    Xf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_guassian_basis(X_lin_1d, degree)\n\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d={degree})\", plot=False)\n\n\nerrors_df = pd.DataFrame(errors).T\n\n\nerrors_df.plot(kind='bar', figsize=(12, 6), title='MSE for Train and Test Sets')\n\n\n\n\n\n\n\n\n\nerrors.clear()\n\n\n# Bias Variance Tradeoff wrt Sigma\n\nfor std in [0.1, 0.5, 1, 2, 5, 10]:\n    Xf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), 3, std)\n    Xf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), 3, std)\n\n    X_lin_poly = create_guassian_basis(X_lin_1d, 3, std)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d=3, std={std})\", plot=False)\n\n\n# Plot the train and test errors for different values of sigma\n\nerrors_df = pd.DataFrame(errors).T\n\ntest_errors = errors_df.test.values\ntrain_errors = errors_df.train.values\n\nlog_test_errors = np.log(test_errors)\nlog_train_errors = np.log(train_errors)\n\nstds = [0.1, 0.5, 1, 2, 5, 10]\n\nplt.plot(stds , log_test_errors, label='Log Test Loss')\nplt.plot(stds , log_train_errors, label='Log Train Loss')\nplt.scatter(stds, log_test_errors)\nplt.scatter(stds, log_train_errors)\nplt.legend()\nplt.xlabel('Sigma')\nplt.ylabel('Log(MSE)')\nplt.title('Bias Variance Tradeoff wrt Sigma')\n\nText(0.5, 1.0, 'Bias Variance Tradeoff wrt Sigma')\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process\n\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\nfrom sklearn.gaussian_process.kernels import WhiteKernel\n\nlong_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n\nseasonal_kernel = (\n    2.0**2\n    * RBF(length_scale=100.0)\n    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n)\n\nirregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n\nnoise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n)\n\nco2_kernel = (\n    long_term_trend_kernel + seasonal_kernel + irregularities_kernel + noise_kernel\n)\nco2_kernel\n\n50**2 * RBF(length_scale=50) + 2**2 * RBF(length_scale=100) * ExpSineSquared(length_scale=1, periodicity=1) + 0.5**2 * RationalQuadratic(alpha=1, length_scale=1) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01)\n\n\n\n# Using GP for the interpolation problem\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\ndef plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n\n    gaussian_process = GaussianProcessRegressor(\n        kernel=co2_kernel,n_restarts_optimizer=9\n    )\n\n    gaussian_process.fit(X_norm_train, y_norm_train)\n\n    y_hat_train, std_prediction_train = gaussian_process.predict(X_norm_train, return_std=True)\n    y_hat_test , std_prediction_test = gaussian_process.predict(X_norm_test, return_std=True)\n\n    y_hat_train = y_hat_train.reshape(-1, 1)\n    y_hat_test = y_hat_test.reshape(-1, 1)\n\n    # Transform back to original scale\n    y_hat_train = s2.inverse_transform(y_hat_train)\n    y_hat_test = s2.inverse_transform(y_hat_test)\n\n    y_hat_lin , std_prediction_lin = gaussian_process.predict(X_lin , return_std=True)\n    y_hat_lin = y_hat_lin.reshape(-1, 1)\n    y_hat_lin = s2.inverse_transform(y_hat_lin)\n\n    errors[title] = {\"train\": mean_squared_error(y_train, y_hat_train),\n                     \"test\": mean_squared_error(y_test, y_hat_test)}\n\n    if plot:\n        plt.plot(X_train, y_train, 'o', label='train',markersize=1)\n        plt.plot(X_test, y_test, 'o', label='test', ms=3)\n        plt.plot(s1.inverse_transform(X_lin_1d), y_hat_lin, label='model')\n        plt.fill_between(s1.inverse_transform(X_lin_1d).reshape(-1), \n                         (y_hat_lin - 1.96*std_prediction_lin.reshape(-1,1)).reshape(-1), \n                         (y_hat_lin + 1.96*std_prediction_lin.reshape(-1,1)).reshape(-1), alpha=0.5 , label='95% Confidence interval')\n        plt.xlabel('Months since first measurement')\n        plt.ylabel('CO2 Levels')\n        plt.legend()\n\n        plt.title('{}\\n Train MSE: {:.2f} | Test MSE: {:.2f}'.format(title, errors[title][\"train\"], errors[title][\"test\"]))\n\n    return errors[title]\n\n\nplot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Regression\")\n\nKeyboardInterrupt: \n\n\n\n\nFourier Features\nReference: https://bmild.github.io/fourfeat/\n\nFourier Feature Mapping\nFor every input x, we map it to a higher dimensional space using the following function:\n\\[\\gamma(x) = [\\cos(2\\pi Bx), \\sin(2\\pi Bx)]^{T} \\]\nwhere \\(B\\) is a random Gaussian matrix, where each entry is drawn independently from a normal distribution N(0, \\(σ^{2}\\))\n\nnp.random.seed(42)\nsigma = 5\nNUM_features = 5\nfs = sigma*np.random.randn(NUM_features)\nprint(fs)\n\nfor i in range(NUM_features):\n    plt.plot(X_norm, np.sin(fs[i]*X_norm), label=f'feature {i}-sin')\n    plt.plot(X_norm, np.cos(fs[i]*X_norm), label=f'feature {i}-cos')\nplt.legend()\nplt.title('Fourier Featurization of X manually')\n\n[ 2.48357077 -0.69132151  3.23844269  7.61514928 -1.17076687]\n\n\nText(0.5, 1.0, 'Fourier Featurization of X manually')\n\n\n\n\n\n\n\n\n\n\n# Explicit implementation of RFF\n\ndef create_random_features(X, gamma, NUM_features):\n    \"\"\"\n    X is (N, 1) array\n    gamma is a scalar\n    NUM_features is a scalar\n    \"\"\"\n    \n    X_rff = np.zeros((len(X), 2*NUM_features + 1))\n    X_rff[:, 0] = X[:, 0]\n    for i in range(NUM_features):\n        b = np.random.randn()\n        X_rff[:, i+1] = np.sin(2*np.pi*gamma*b*X[:, 0])\n        X_rff[:, i + NUM_features+1] = np.cos(2*np.pi*gamma*b*X[:, 0])\n    \n    # Normalize each column\n    X_rff = StandardScaler().fit_transform(X_rff)\n    return X_rff\n\n\n\nSklearn Implementation\n\n# Sklearn's implementation of RFF\n\nfrom sklearn.kernel_approximation import RBFSampler\nRBFSampler?\n\nInit signature: RBFSampler(*, gamma=1.0, n_components=100, random_state=None)\nDocstring:     \nApproximate a RBF kernel feature map using random Fourier features.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`.\n\nParameters\n----------\ngamma : 'scale' or float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2).\n    If ``gamma='scale'`` is passed then it uses\n    1 / (n_features * X.var()) as value of gamma.\n\n    .. versionadded:: 1.2\n       The option `\"scale\"` was added in 1.2.\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\nNystroem : Approximate a kernel map using a subset of the training data.\nPolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\nSkewedChi2Sampler : Approximate feature map for\n    \"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1, 1]\n&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)\n&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)\n&gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3)\n&gt;&gt;&gt; clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n&gt;&gt;&gt; clf.score(X_features, y)\n1.0\nFile:           ~/miniforge3/lib/python3.9/site-packages/sklearn/kernel_approximation.py\nType:           type\nSubclasses:     \n\n\n\nr= RBFSampler(n_components=5)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=0.1)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=20)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\n# Implicit implementation of RFF using sklearn\n\ndef create_rff(X, gamma, NUM_features):\n    # Random Fourier Features\n    # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n    rbf_feature = RBFSampler(gamma=gamma, n_components=NUM_features, random_state=1)\n    X_features = rbf_feature.fit_transform(X)\n    return X_features\n\n\nmodel3 = LinearRegression()\ngamma = 2.0\nNUM_features = 4\n\nXf_norm_train = create_rff(X_norm_train.reshape(-1, 1), gamma, NUM_features)\nXf_norm_test = create_rff(X_norm_test.reshape(-1, 1), gamma, NUM_features)\n\nX_lin_rff = create_rff(X_lin_1d, gamma, NUM_features)\n\nplot_fit_predict(model3, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_rff, f\"Random Fourier Features (gamma={gamma}, NUM_features={NUM_features})\")\n\n{'train': 5.035023975072016, 'test': 4.782073267288203}\n\n\n\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff for Fourier Features\nw.r.t gamma\n\nmodel4 = LinearRegression()\n\n# NUM_features_values = [1, 2, 3, 4, 5, 10, 20, 50, 100]\ngamma_values = [0.01, 0.1, 1, 2, 5, 10]\n\nerrors_rff = {}\n\nfor gamma in gamma_values:\n    # gamma = 2.0\n    NUM_features_ = 100\n    Xf_norm_train = create_rff(X_norm_train.reshape(-1, 1), gamma, NUM_features_)\n    Xf_norm_test = create_rff(X_norm_test.reshape(-1, 1), gamma, NUM_features_)\n\n    X_lin_rff = create_rff(X_lin_1d, gamma, NUM_features_)\n\n    plot_fit_predict(model4, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_rff, gamma, plot=False)\n\n    errors_rff[gamma] = errors[gamma]\n\n\npd.DataFrame(errors_rff).T.plot(logy=True, logx=True)\n\n\n\n\n\n\n\n\n\n\n\nExtrapolation using Gaussian Process\n\nX_norm_train = X_norm[:int(len(X_norm)*0.7)]\nX_norm_test = X_norm[int(len(X_norm)*0.7):]\n\ny_norm_train = y_norm[:int(len(y_norm)*0.7)]\ny_norm_test = y_norm[int(len(y_norm)*0.7):]\n\nX_train = X[:int(len(X)*0.7)]\nX_test = X[int(len(X)*0.7):]\n\ny_train = y[:int(len(y)*0.7)]\ny_test = y[int(len(y)*0.7):]\n\n\nplt.plot(X_norm_train, y_norm_train, 'o', label='train',markersize=1)\nplt.plot(X_norm_test, y_norm_test, 'o', label='test', ms=3)\n\n\n\n\n\n\n\n\n\nplot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Extrapolation Regression\")\n\nNameError: name 'plot_fit_gp' is not defined\n\n\n\n\nBefore this…\n\n\nDataset 1: Sine wave with noise\n\n# Generate some data\nrng = np.random.RandomState(1)\nx = 15 * rng.rand(200)\ny = np.sin(x) + 0.1 * rng.randn(200)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# plot the data\nplt.scatter(df.x, df.y)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train test split\ntrain = df.sample(frac=0.8, random_state=1)\ntest = df.drop(train.index)\n\nplt.scatter(train.x, train.y, color='blue', label='train')\nplt.scatter(test.x, test.y, color='orange', label='test')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plot_predictions(train, test, yhat_train, yhat_test):\n        \n    # add yhat_train to train and yhat_test to test\n    train['yhat'] = yhat_train\n    test['yhat'] = yhat_test\n\n    # sort train and test by x\n    train = train.sort_values(by='x')\n    test = test.sort_values(by='x')\n\n    # Train and test error\n    train_rmse = np.sqrt(np.mean((train.yhat - train.y)**2))\n    test_rmse = np.sqrt(np.mean((test.yhat - test.y)**2))\n\n    plt.scatter(train.x, train.y, color='blue', label='train')\n    plt.scatter(test.x, test.y, color='orange', label='test')\n    plt.plot(train.x, train.yhat, color='red', label='train prediction')\n    plt.plot(test.x, test.yhat, color='green', label='test prediction')\n    plt.title('Train RMSE: {:.3f}, Test RMSE: {:.3f}'.format(train_rmse, test_rmse))\n    plt.legend()\n    plt.show()\n\n    return train_rmse, test_rmse\n\n\n# Hyperparameter tuning using grid search and showing bias variance tradeoff\ndef hyperparameter_tuning(params, train, test, model):\n    train_rmse = []\n    test_rmse = []\n\n    for d in params:\n        yhat_train, yhat_test = model(d, train, test)\n        train_rmse.append(np.sqrt(np.mean((yhat_train - train.y)**2)))\n        test_rmse.append(np.sqrt(np.mean((yhat_test - test.y)**2)))\n\n    plt.plot(params, train_rmse, label='train')\n    plt.plot(params, test_rmse, label='test')\n    plt.xlabel('params')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.show()\n\n    optimal_param = params[np.argmin(test_rmse)]\n\n    return optimal_param\n\n\nrmse_dict = {}\n\n\nModel 1: MLP\n\n# use sk-learn for MLP\nmlp_model = MLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter = 10000)\nmlp_model.fit(np.array(train.x).reshape(-1, 1), train.y)\n\nMLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressorMLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter=10000)\n\n\n\nyhat_train = mlp_model.predict(np.array(train.x).reshape(-1, 1))\nyhat_test = mlp_model.predict(np.array(test.x).reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['MLP'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\n\nModel 2: Vanilla Linear Regression\n\nlr1 = LinearRegression()\nlr1.fit(np.array(train.x).reshape(-1, 1), train.y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nyhat_train = lr1.predict(np.array(train.x).reshape(-1, 1))\nyhat_test = lr1.predict(np.array(test.x).reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Vanilla LR'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\n\nModel 3: Polynomial regression with degree d\n\ndef poly_regression(d, train, test):\n    lr = LinearRegression()\n    pf = PolynomialFeatures(degree=d)\n    \n    X_train = pf.fit_transform(train.x.values.reshape(-1, 1))\n    X_test = pf.fit_transform(test.x.values.reshape(-1, 1))\n    \n    lr.fit(X_train, train.y)\n    \n    yhat_train = lr.predict(X_train)\n    yhat_test = lr.predict(X_test)\n\n    return yhat_train, yhat_test\n\n\n# Hyperparameter tuning using grid search and showing bias variance tradeoff\ndegrees = range(1, 20)\nbest_degree = hyperparameter_tuning(degrees, train, test, poly_regression)\nyhat_train, yhat_test = poly_regression(best_degree, train, test)\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Polynomial Regression'] = (train_rmse, test_rmse)\nprint(\"Best degree: \", best_degree)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest degree:  10\n\n\n\n\nModel 4: Linear regression with sine and cosine basis functions\n\ndef sine_basis_regression(num_basis, train, test):\n    lr = LinearRegression()\n    for i in range(1, num_basis+1):\n        train[f\"sine_{i}\"] = np.sin(i*train.x)\n        train[f\"cosine_{i}\"] = np.cos(i*train.x)\n        test[f\"sine_{i}\"] = np.sin(i*test.x)\n        test[f\"cosine_{i}\"] = np.cos(i*test.x)\n    \n    X_train = train.drop(['y'], axis=1)\n    X_test = test.drop(['y'], axis=1)\n    \n    lr.fit(X_train, train.y)\n\n    yhat_train = lr.predict(X_train)\n    yhat_test = lr.predict(X_test)\n\n    return yhat_train, yhat_test\n\n\nbasis = range(1, 20)\nbest_num_basis = hyperparameter_tuning(basis, train, test, sine_basis_regression)\nyhat_train, yhat_test = sine_basis_regression(best_num_basis, train, test)\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Sine Basis Regression'] = (train_rmse, test_rmse)\nprint(\"Best number of basis: \", best_num_basis)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest number of basis:  14\n\n\n\n\nModel 5: Linear regression with Gaussian basis functions\n\n# Source: https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Gaussian-basis-functions\n\nclass GaussianFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Uniformly spaced Gaussian features for one-dimensional input\n    \n    Constructor with N centers and width_factor as hyperparameters\n    N comes from the number of basis functions\n    width_factor is the width of each basis function\n    \"\"\"\n    \n    def __init__(self, N, width_factor=2.0):\n        self.N = N\n        self.width_factor = width_factor\n    \n    @staticmethod\n    def _gauss_basis(x, y, width, axis=None):\n        arg = (x - y) / width\n        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n        \n    def fit(self, X, y=None):\n        # create N centers spread along the data range\n        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n        return self\n        \n    def transform(self, X):\n        return self._gauss_basis(X[:, :, np.newaxis], self.centers_, self.width_, axis=1)\n\n\n# Hyperparameter tuning\nbasis = range(2, 20)\n\ntrain_rmse = []\ntest_rmse = []\nfor d in basis:\n    model = make_pipeline(GaussianFeatures(d), LinearRegression())\n    model.fit(np.array(train.x).reshape(-1, 1), train.y)\n    yhat_train = model.predict(np.array(train.x).reshape(-1, 1))\n    yhat_test = model.predict(np.array(test.x).reshape(-1, 1))\n    train_rmse.append(np.sqrt(np.mean((yhat_train - train.y)**2)))\n    test_rmse.append(np.sqrt(np.mean((yhat_test - test.y)**2)))\n\nbest_num_basis = basis[np.argmin(test_rmse)]\nprint(\"Best number of basis: \", best_num_basis)\nplt.plot(basis, train_rmse, label='train')\nplt.plot(basis, test_rmse, label='test')\nplt.xlabel('degree')\nplt.ylabel('RMSE')\nplt.legend()\nplt.show()\n\nBest number of basis:  13\n\n\n\n\n\n\n\n\n\n\ngauss_model = make_pipeline(GaussianFeatures(best_num_basis), LinearRegression())\ngauss_model.fit(np.array(train.x).reshape(-1, 1), train.y)\nyhat_train = gauss_model.predict(train.x.values.reshape(-1, 1))\nyhat_test = gauss_model.predict(test.x.values.reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Gaussian Basis Regression'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\nPlotting rmse using different variants of linear regression\n\n# create a bar plot of train and test RMSE\n\ntrain_rmse = [rmse_dict[key][0] for key in rmse_dict.keys()]\ntest_rmse = [rmse_dict[key][1] for key in rmse_dict.keys()]\nlabels = [key for key in rmse_dict.keys()]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 5))\nrects1 = ax.bar(x - width/2, train_rmse, width, label='Train RMSE')\nrects2 = ax.bar(x + width/2, test_rmse, width, label='Test RMSE')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('RMSE')\nax.set_title('RMSE by model')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nDataset 2: CO2 Dataset\n\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n6.255330\n-0.020070\n\n\n1\n10.804867\n-0.920032\n\n\n2\n0.001716\n0.024965\n\n\n3\n4.534989\n-0.916051\n\n\n4\n2.201338\n0.776696\n\n\n...\n...\n...\n\n\n195\n13.979581\n1.115462\n\n\n196\n0.209274\n0.163526\n\n\n197\n3.515431\n-0.332839\n\n\n198\n9.251675\n0.161240\n\n\n199\n14.235245\n0.996049\n\n\n\n\n200 rows × 2 columns\n\n\n\n\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\nKeyError: \"None of [Index(['year', 'month'], dtype='object')] are in the [columns]\"\n\n\n\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n6.255330\n-0.020070\n\n\n1\n10.804867\n-0.920032\n\n\n2\n0.001716\n0.024965\n\n\n3\n4.534989\n-0.916051\n\n\n4\n2.201338\n0.776696\n\n\n...\n...\n...\n\n\n195\n13.979581\n1.115462\n\n\n196\n0.209274\n0.163526\n\n\n197\n3.515431\n-0.332839\n\n\n198\n9.251675\n0.161240\n\n\n199\n14.235245\n0.996049\n\n\n\n\n200 rows × 2 columns\n\n\n\n\ndf.average.plot()\n\nAttributeError: 'DataFrame' object has no attribute 'average'\n\n\n\ntrain_cutoff = 2000\ntrain = df[df.year &lt; train_cutoff]\ntest = df[df.year &gt;= train_cutoff]\ndf.average.plot()\n\ntrain.average.plot(color='blue')\ntest.average.plot(color='orange')\n\nlen(train), len(test)\n\n\nmonths_from_start = range(len(df))\nmonths_from_start = np.array(months_from_start).reshape(-1, 1)\n\n\n# use sk-learn for MLP\n\nmlp_model = MLPRegressor(hidden_layer_sizes=[512, 512, 512, 512, 512], max_iter = 5000)\nmlp_model.fit(months_from_start[:len(train)], train.average.values)\n\n\nyhat_train = mlp_model.predict(months_from_start[:len(train)])\nyhat_test = mlp_model.predict(months_from_start[len(train):])\n\nyhat_train = pd.Series(yhat_train, index=train.index)\nyhat_test = pd.Series(yhat_test, index=test.index)\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\n# normalize data\n\ntrain_scaled = (train - train.mean()) / train.std()\ntest_scaled = (test - test.mean()) / test.std()\nmonths_from_start_scaled = (months_from_start - months_from_start.mean()) / months_from_start.std()\n\n# train_scaled = (train - train.mean()) / train.std()\n# test_scaled = (test - test.mean()) / test.std()\n# months_from_start_scaled = (months_from_start - months_from_start.mean()) / months_from_start.std()\n\nmlp_model = MLPRegressor(hidden_layer_sizes=512, max_iter = 1000)\nmlp_model.fit(months_from_start_scaled[:len(train)], train_scaled.average.values)\n\nyhat_train = mlp_model.predict(months_from_start_scaled[:len(train)])\nyhat_test = mlp_model.predict(months_from_start_scaled[len(train):])\n\nyhat_train_scaled = pd.Series(yhat_train, index=train.index)\nyhat_test_scaled = pd.Series(yhat_test, index=test.index)\n\nyhat_train = yhat_train_scaled * train.std() + train.mean()\n# yhat_test = yhat_test_scaled * test.std() + test.mean()\n\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\nModel 2: Vanilla Linear Regression\n\nlr1 = LinearRegression()\nlr1.fit(months_from_start[:len(train)], train.average.values)\nyhat1_test = lr1.predict(months_from_start[len(train):])\nyhat1_train = lr1.predict(months_from_start[:len(train)])\n\nyhat_train = pd.Series(yhat1_train, index=train.index)\nyhat_test = pd.Series(yhat1_test, index=test.index)\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\n\nModel 3: Polynomial regression with degree d\n\ndef poly_regression(d, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    pf = PolynomialFeatures(degree=d)\n    X_train = pf.fit_transform(months_from_start[:len(train)])\n    X_test = pf.fit_transform(months_from_start[len(train):])\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    df.average.plot()\n    yhat_train.plot()\n    yhat_test.plot()\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_regression(2, train, test)\n\n\n\nModel 4: Linear Regression with sine and cosine basis functions\n\n### Adding sine and cosine terms\ndef sine_cosine_features(X, n):\n    \"\"\"\n    X: array of shape (n_samples, 1)\n    n: number of sine and cosine features to add\n    \"\"\"\n    for i in range(1, n+1):\n        X = np.hstack([X, np.sin(i*X), np.cos(i*X)])\n    return X\n\n\nX = np.linspace(-1, 1, 100).reshape(-1, 1)\n\n\n_ = plt.plot(X, sine_cosine_features(X, 0))\n\n\n\nModel 5: Gaussian basis functions\n\n\nModel 6: Linear Regression with polynomial and sine/cosine basis functions\n\ndef poly_sine_cosine_regression(n, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    \n    X_train = sine_cosine_features(months_from_start[:len(train)], n)\n    \n    X_test = sine_cosine_features(months_from_start[len(train):], n)\n    print(X_train.shape, X_test.shape)\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    \n    yhat_train.plot(alpha=0.2, lw=4)\n    yhat_test.plot(alpha=0.2, lw=4)\n    df.average.plot(color='k', lw=1)\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_sine_cosine_regression(6, train, test)\n\n\n\nModel 7: Random Fourier Features\n\ndef rff_featurise(X, n_components=100):\n    # Random Fourier Features\n    # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n    from sklearn.kernel_approximation import RBFSampler\n    rbf_feature = RBFSampler(gamma=1, n_components=n_components, random_state=1)\n    X_features = rbf_feature.fit_transform(X)\n    return X_features\n\n\ndef poly_rff_regression(n, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    \n    X_train = rff_featurise(months_from_start[:len(train)], n)\n    \n    X_test = rff_featurise(months_from_start[len(train):], n)\n    print(X_train.shape, X_test.shape)\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    \n    yhat_train.plot(alpha=0.2, lw=4)\n    yhat_test.plot(alpha=0.2, lw=4)\n    df.average.plot(color='k', lw=1)\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_rff_regression(440, train, test)"
  },
  {
    "objectID": "notebooks/cost-iteration-notebook.html",
    "href": "notebooks/cost-iteration-notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n\n\n\nX = np.array([1, 2, 3])\ny = np.array([1, 2, 3])\n\n\ndef y_hat(X, theta_0, theta_1):\n    return theta_0 + theta_1*X\n\n\ndef cost(X, y, theta_0, theta_1):\n    yh = y_hat(X, theta_0, theta_1)\n    return (y-yh).T@(y-yh)\n\n\ntheta_0 = 4\ntheta_1 = 0\nalpha = 0.1\ncosts = np.zeros(1000)\ntheta_0_list = np.zeros(1000)\ntheta_1_list = np.zeros(1000)\n\nfor i in range(1000):\n    costs[i] = cost(X, y, theta_0, theta_1)\n    theta_0 = theta_0 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).mean())\n    theta_1 = theta_1 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).T@X)/len(X)\n    theta_0_list[i] = theta_0\n    theta_1_list[i] = theta_1\n\n\nimport sys\nsys.path.append(\"../\")\nfrom latexify import *\n\n\nlatexify()\nplt.plot(costs[:200], 'k')\nformat_axes(plt.gca())\nplt.ylabel(\"Cost\")\nplt.xlabel(\"Iteration\")\nplt.savefig(\"../gradient-descent/gd-iter-cost.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\n\nfor i in range(0, 200, 20):\n    plt.title(label=\"Fit at iteration {}\".format(i))\n    plt.plot(X, theta_0_list[i]+theta_1_list[i]*X, color='k')\n    plt.scatter(X, y, color='k')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/fit-iteration-{}.pdf\".format(i), bbox_inches=\"tight\", transparent=True)\n    plt.cla()\n\n\n\n\n\n\n\n\n\ntheta_0 = 4\ntheta_1 = 0\n(y_hat(X, theta_0, theta_1)-y).mean()\n\n2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1)).mean()\n\n-2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1))@X\n\n-10"
  },
  {
    "objectID": "notebooks/dummy-baselines.html",
    "href": "notebooks/dummy-baselines.html",
    "title": "Comparison of Sophisticated vs Dummy Baseline ML Algorithms for Imbalanced Datasets",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nfrom latexify import latexify\n# retina\n%config InlineBackend.figure_format = 'retina'\n\nClassification\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Create an imbalanced dataset with two features\nX, y = make_blobs(\n    n_samples=[4500,500],\n    n_features=2,  # Use only two features\n    cluster_std=[4.0,4.0],\n    random_state=42\n)\n\nprint(len(y))\nprint(len(y[y==1]))\nprint(len(y[y==0]))\n\n5000\n500\n4500\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"Train set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\n\nTrain set shape: (4000, 2) (4000,)\nTest set shape: (1000, 2) (1000,)\n\n\n\ncolors = ['blue' if label == 0 else 'red' for label in y_train]\nlatexify(fig_width=7, fig_height=5)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=colors, alpha=0.8)\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.title('Training Data')\n\nText(0.5, 1.0, 'Training Data')\n\n\n\n\n\n\n\n\n\n\nprint(len(y_train[y_train==1]))\nprint(len(y_train[y_train==0]))\n\n393\n3607\n\n\n\nprint(len(y_test[y_test==1]))\nprint(len(y_test[y_test==0]))\n\n107\n893\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)\n\ny_pred = rf_classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Print the accuracy and F1 score\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1)\n\nAccuracy: 0.945\nF1 Score: 0.7208121827411167\n\n\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_classifier = DummyClassifier(strategy='stratified')\ndummy_classifier.fit(X_train, y_train)\n\ny_pred_dummy = dummy_classifier.predict(X_test)\naccuracy_dummy = accuracy_score(y_test, y_pred_dummy)\nf1_dummy = f1_score(y_test, y_pred_dummy)\n\n# Print the accuracy and F1 score for the dummy classifier\nprint(\"Dummy Classifier Accuracy:\", accuracy_dummy)\nprint(\"Dummy Classifier F1 Score:\", f1_dummy)\n\nDummy Classifier Accuracy: 0.81\nDummy Classifier F1 Score: 0.07766990291262137\n\n\nRegression\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.dummy import DummyRegressor\n\n# Generate synthetic regression dataset with noise\nnp.random.seed(42)\nX = np.linspace(0, 1, 500).reshape(-1, 1)\nslope = 1.2\ny_true = slope * X.squeeze()\ny = y_true + np.random.normal(scale=0.5, size=len(X))\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"Train set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\n\nTrain set shape: (400, 1) (400,)\nTest set shape: (100, 1) (100,)\n\n\n\n# Scatter plot of the training data\nplt.scatter(X_train, y_train, color='red', alpha=0.8, label='Actual')\nplt.plot(X, y_true, color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.legend()\nplt.title('Training Data')\n\nText(0.5, 1.0, 'Training Data')\n\n\n\n\n\n\n\n\n\n\n# RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators=10, random_state=42)\nrf_regressor.fit(X_train, y_train)\n\ny_pred_rf = rf_regressor.predict(X_test)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\n\n# Print the Mean Squared Error for the RandomForestRegressor\nprint(\"Random Forest Regressor MSE:\", mse_rf)\n\nRandom Forest Regressor MSE: 0.3446175878909235\n\n\n\nplt.scatter(X_test, y_test, color='red', alpha=0.8, label='Actual')\nplt.scatter(X_test, y_pred_rf, color='black', alpha=0.8, label='Predicted')\nplt.plot(X_test, slope*X_test.squeeze(), color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('Predictions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# DummyRegressor\ndummy_regressor = DummyRegressor(strategy='mean')\ndummy_regressor.fit(X_train, y_train)\n\ny_pred_dummy = dummy_regressor.predict(X_test)\nmse_dummy = mean_squared_error(y_test, y_pred_dummy)\n\n# Print the Mean Squared Error for the DummyRegressor\nprint(\"Dummy Regressor MSE:\", mse_dummy)\n\nDummy Regressor MSE: 0.39550009552721027\n\n\n\nplt.scatter(X_test, y_test, color='red', alpha=0.8, label='Actual')\nplt.scatter(X_test, y_pred_dummy, color='black', alpha=0.8, label='Predicted')\nplt.plot(X_test, slope*X_test.squeeze(), color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('Predictions')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/taylor.html",
    "href": "notebooks/taylor.html",
    "title": "Taylor’s Series",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nf = lambda x, y: x**2 + y**2\n\ndef f(x, y):\n    return x**2 + y**2\n\nf_dash = torch.func.grad(f, argnums=(0, 1))\n\n\ndef f2(argument):\n    x, y= argument\n    return x**2 + y**2\n\n\ntorch.func.grad(f2)(torch.tensor([1.0, 1.0]))\n\ntensor([2., 2.])\n\n\n\nf_dash\n\n&lt;function __main__.f(x, y)&gt;\n\n\n\nf_dash(torch.tensor(1.0), torch.tensor(1.0))\n\n(tensor(2.), tensor(2.))\n\n\n\nx = torch.tensor(1.0, requires_grad=True)\ny = torch.tensor(1.0, requires_grad=True)\nprint(\"Before backward: \", x.grad, y.grad)\nz = f(x, y)\nz.backward()\nprint(\"After backward: \", x.grad, y.grad)\n\nBefore backward:  None None\nAfter backward:  tensor(2.) tensor(2.)\n\n\n\nf = lambda x: torch.cos(x)\n\n\nx_range = torch.arange(-2*np.pi, 2*np.pi, 0.01)\ny_range = f(x_range)\nplt.plot(x_range, y_range)\n\n\n\n\n\n\n\n\n\ndef nth_order_appx(f, n, x0=0.0, verbose=False):\n    x0 = torch.tensor(x0)\n    derivs = {1:torch.func.grad(f)}\n    vals = {0:f(x0), 1:derivs[1](x0)}\n    if verbose:\n        print(\"f(x0) = {}\".format(vals[0]))\n        print(\"f'(x0) = {}\".format(vals[1]))\n\n    for i in range(2, n+1):\n        derivs[i] = torch.func.grad(derivs[i-1])\n        vals[i] = derivs[i](x0)\n        if verbose:\n            d = \"'\"*i\n            print(\"f{}(x0) = {}\".format(d, vals[i]))\n    \n    def g(x):\n        x_diff = x - x0\n        str_rep = \"f(x) = f(x0) + \"\n        out = vals[0].repeat(x.shape)\n        for i in range(1, n+1):\n            str_rep += f\"{vals[i]} * (x-{x0.item()})^{i} / {i}! + \"\n            out += vals[i] * x_diff**i / torch.math.factorial(i)\n        if verbose:\n            print(\"--\"*40)\n            print(str_rep)\n        return out\n\n    return g\n        \n\n\nf = lambda x: torch.cos(x)\n_ = nth_order_appx(f, 2, 0.0, verbose=True)(x_range)\n\nf(x0) = 1.0\nf'(x0) = -0.0\nf''(x0) = -1.0\n--------------------------------------------------------------------------------\nf(x) = f(x0) + -0.0 * (x-0.0)^1 / 1! + -1.0 * (x-0.0)^2 / 2! + \n\n\n\nplt.plot(x_range, f(x_range), label=\"f(x) = cos(x)\")\nfor i in range(13, 17, 2):\n    plt.plot(x_range, nth_order_appx(f,  i, 0.0)(x_range), label=f\"order {i} appx\")\nplt.ylim(-2, 2)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: torch.cos(x)\nx0 = 3.14\nplt.plot(x_range, f(x_range))\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nfor i in range(1, 11, 2):\n    \n    plt.plot(x_range, nth_order_appx(f,  i, 3.14)(x_range), label=f\"order {i}\")\nplt.ylim(-2, 2)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: x**2 + 2\nplt.plot(x_range, f(x_range))\nx0 = 2.0\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\n\n\n\n\n\n\n\n\n\nplt.plot(x_range, f(x_range), label=\"f(x) = x^2 + 2\", lw=3, alpha=0.5, ls='--')\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\nplt.xlim(1.5, 2.5)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: x**2 + 2\nplt.plot(x_range, f(x_range))\nx0 = 2.0\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\n\n\n\n\n\n\n\n\n\ndef plot_gd(alpha=0.1, iter=3):\n    x0 = torch.tensor(2.0)\n\n    xi = x0\n    plt.plot(x_range, f(x_range), label=\"f(x) = x^2 + 2\", lw=3, alpha=0.5, ls='--')\n    for i in range(iter):\n        plt.scatter(xi, f(xi), label=f'x{i} = {xi.item():0.2f}', s=100, c=f\"C{i}\")\n        plt.plot(x_range, nth_order_appx(f, 1, xi)(x_range), label=f\"order 1\", c=f\"C{i}\")\n        xi = xi - alpha * torch.func.grad(f)(xi)\n\n    plt.xlim(-2.5, 2.5)\n    plt.ylim(0, 8)\n    plt.legend()\n\n\nplot_gd(alpha=0.001, iter=4)\n\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=0.8, iter=4)\n\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4160463/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)"
  },
  {
    "objectID": "notebooks/numpy-pandas-basics.html",
    "href": "notebooks/numpy-pandas-basics.html",
    "title": "Numpy Pandas Basics",
    "section": "",
    "text": "# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nLists\n\n# Creating lists\nlist_a = [1, 2, 3, 4, 5]\nlist_b = [6, 7, 8, 9, 10]\n\n\n# Operations on lists\n# Adding lists\nlist_sum = [a + b for a, b in zip(list_a, list_b)]\nprint(\"List Sum:\", list_sum)\n\n# Vector product using lists    \nvector_product = [a * b for a, b in zip(list_a, list_b)]\nprint(\"Vector Product:\", vector_product)\n\nList Sum: [7, 9, 11, 13, 15]\nVector Product: [6, 14, 24, 36, 50]\n\n\n\n\nNumpy Array\n\n# Creating numpy arrays\nnumpy_array_a = np.array(list_a)\nnumpy_array_b = np.array(list_b)\n\n\n# Operations on numpy arrays\n# Adding numpy arrays\nnumpy_sum = numpy_array_a + numpy_array_b\nprint(\"Numpy Sum:\", numpy_sum)\n\n# Vector product using numpy arrays\nnumpy_vector_product = np.multiply(numpy_array_a, numpy_array_b)\nprint(\"Numpy Vector Product:\", numpy_vector_product)\n\nNumpy Sum: [ 7  9 11 13 15]\nNumpy Vector Product: [ 6 14 24 36 50]\n\n\n\nnp.allclose(list_sum, numpy_sum), np.allclose(vector_product, numpy_vector_product)\n\n(True, True)\n\n\n\n\nTime comparison between list and numpy array\n\n# Creating large arrays and lists for time comparison\nnumpy_array_a = np.random.randint(0, 100, size=10000)\nnumpy_array_b = np.random.randint(0, 100, size=10000)\n\nlist_a = list(numpy_array_a)\nlist_b = list(numpy_array_b)\n\n\n# Time for list addition\nstart_time = time.time()\nfor _ in range(1000):\n    list_sum = [a + b for a, b in zip(list_a, list_b)]\nend_time = time.time()\nprint(\"Time taken for lists addition:\", end_time - start_time)\n\n# Time for numpy addition\nstart_time = time.time()\nfor _ in range(1000):\n    numpy_sum = numpy_array_a + numpy_array_b\nend_time = time.time()\nprint(\"Time taken for numpy addition:\", end_time - start_time)\n\nTime taken for lists addition: 0.5500102043151855\nTime taken for numpy addition: 0.0038487911224365234\n\n\n\n# Time for list vector product\nstart_time = time.time()\nfor _ in range(10000):\n    list_product = [a * b for a, b in zip(list_a, list_b)]\n\nend_time = time.time()\nprint(\"Time taken for list vector product:\", end_time - start_time)\n\n# Time for numpy vector product \nstart_time = time.time()\nfor _ in range(10000):\n    numpy_product = np.multiply(numpy_array_a, numpy_array_b)\n\nend_time = time.time()\nprint(\"Time taken for numpy vector product:\", end_time - start_time)\n\nTime taken for list vector product: 5.371699571609497\nTime taken for numpy vector product: 0.047417640686035156\n\n\n\nnp.allclose(list_sum, numpy_sum), np.allclose(vector_product, numpy_vector_product)\n\n(True, True)\n\n\n\ntimeit_add_list = %timeit -o [a + b for a, b in zip(list_a, list_b)]\n\n542 µs ± 593 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\ntimeit_add_numpy = %timeit -o numpy_array_a + numpy_array_b\n\n3.5 µs ± 6.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode clarity\n\n# Numpy code is often more concise and readable than list comprehensions\n# Example: Calculate the element-wise product of two lists\nlist_product = [a * b for a, b in zip(list_a, list_b)]\nnumpy_product = np.multiply(numpy_array_a, numpy_array_b)\n\n\nnumpy_product\n\narray([5950, 1995,  264, ..., 2436,  928,  665])\n\n\n\nnumpy_array_a@numpy_array_b\n\n24470992\n\n\n\n\nReading CSV file using Numpy\n\n!head ../datasets/tennis-discrete-output.csv\n\nDay,Outlook,Temp,Humidity,Windy,Play\nD1,Sunny,Hot,High,Weak,No\nD2,Sunny,Hot,High,Strong,No\nD3,Overcast,Hot,High,Weak,Yes\nD4,Rain,Mild,High,Weak,Yes\nD5,Rain,Cool,Normal,Weak,Yes\nD6,Rain,Cool,Normal,Strong,No\nD7,Overcast,Cool,Normal,Strong,Yes\nD8,Sunny,Mild,High,Weak,No\nD9,Sunny,Cool,Normal,Weak,Yes\n\n\n\nnp.genfromtxt?\n\nSignature:\nnp.genfromtxt(\n    fname,\n    dtype=&lt;class 'float'&gt;,\n    comments='#',\n    delimiter=None,\n    skip_header=0,\n    skip_footer=0,\n    converters=None,\n    missing_values=None,\n    filling_values=None,\n    usecols=None,\n    names=None,\n    excludelist=None,\n    deletechars=\" !#$%&'()*+,-./:;&lt;=&gt;?@[\\\\]^{|}~\",\n    replace_space='_',\n    autostrip=False,\n    case_sensitive=True,\n    defaultfmt='f%i',\n    unpack=None,\n    usemask=False,\n    loose=True,\n    invalid_raise=True,\n    max_rows=None,\n    encoding='bytes',\n    *,\n    ndmin=0,\n    like=None,\n)\nDocstring:\nLoad data from a text file, with missing values handled as specified.\n\nEach line past the first `skip_header` lines is split at the `delimiter`\ncharacter, and characters following the `comments` character are discarded.\n\nParameters\n----------\nfname : file, str, pathlib.Path, list of str, generator\n    File, filename, list, or generator to read.  If the filename\n    extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n    that generators must return bytes or strings. The strings\n    in a list or produced by a generator are treated as lines.\ndtype : dtype, optional\n    Data type of the resulting array.\n    If None, the dtypes will be determined by the contents of each\n    column, individually.\ncomments : str, optional\n    The character used to indicate the start of a comment.\n    All the characters occurring on a line after a comment are discarded.\ndelimiter : str, int, or sequence, optional\n    The string used to separate values.  By default, any consecutive\n    whitespaces act as delimiter.  An integer or sequence of integers\n    can also be provided as width(s) of each field.\nskiprows : int, optional\n    `skiprows` was removed in numpy 1.10. Please use `skip_header` instead.\nskip_header : int, optional\n    The number of lines to skip at the beginning of the file.\nskip_footer : int, optional\n    The number of lines to skip at the end of the file.\nconverters : variable, optional\n    The set of functions that convert the data of a column to a value.\n    The converters can also be used to provide a default value\n    for missing data: ``converters = {3: lambda s: float(s or 0)}``.\nmissing : variable, optional\n    `missing` was removed in numpy 1.10. Please use `missing_values`\n    instead.\nmissing_values : variable, optional\n    The set of strings corresponding to missing data.\nfilling_values : variable, optional\n    The set of values to be used as default when the data are missing.\nusecols : sequence, optional\n    Which columns to read, with 0 being the first.  For example,\n    ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.\nnames : {None, True, str, sequence}, optional\n    If `names` is True, the field names are read from the first line after\n    the first `skip_header` lines. This line can optionally be preceded\n    by a comment delimiter. If `names` is a sequence or a single-string of\n    comma-separated names, the names will be used to define the field names\n    in a structured dtype. If `names` is None, the names of the dtype\n    fields will be used, if any.\nexcludelist : sequence, optional\n    A list of names to exclude. This list is appended to the default list\n    ['return','file','print']. Excluded names are appended with an\n    underscore: for example, `file` would become `file_`.\ndeletechars : str, optional\n    A string combining invalid characters that must be deleted from the\n    names.\ndefaultfmt : str, optional\n    A format used to define default field names, such as \"f%i\" or \"f_%02i\".\nautostrip : bool, optional\n    Whether to automatically strip white spaces from the variables.\nreplace_space : char, optional\n    Character(s) used in replacement of white spaces in the variable\n    names. By default, use a '_'.\ncase_sensitive : {True, False, 'upper', 'lower'}, optional\n    If True, field names are case sensitive.\n    If False or 'upper', field names are converted to upper case.\n    If 'lower', field names are converted to lower case.\nunpack : bool, optional\n    If True, the returned array is transposed, so that arguments may be\n    unpacked using ``x, y, z = genfromtxt(...)``.  When used with a\n    structured data-type, arrays are returned for each field.\n    Default is False.\nusemask : bool, optional\n    If True, return a masked array.\n    If False, return a regular array.\nloose : bool, optional\n    If True, do not raise errors for invalid values.\ninvalid_raise : bool, optional\n    If True, an exception is raised if an inconsistency is detected in the\n    number of columns.\n    If False, a warning is emitted and the offending lines are skipped.\nmax_rows : int,  optional\n    The maximum number of rows to read. Must not be used with skip_footer\n    at the same time.  If given, the value must be at least 1. Default is\n    to read the entire file.\n\n    .. versionadded:: 1.10.0\nencoding : str, optional\n    Encoding used to decode the inputfile. Does not apply when `fname` is\n    a file object.  The special value 'bytes' enables backward compatibility\n    workarounds that ensure that you receive byte arrays when possible\n    and passes latin1 encoded strings to converters. Override this value to\n    receive unicode arrays and pass strings as input to converters.  If set\n    to None the system default is used. The default value is 'bytes'.\n\n    .. versionadded:: 1.14.0\nndmin : int, optional\n    Same parameter as `loadtxt`\n\n    .. versionadded:: 1.23.0\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Data read from the text file. If `usemask` is True, this is a\n    masked array.\n\nSee Also\n--------\nnumpy.loadtxt : equivalent function when no data is missing.\n\nNotes\n-----\n* When spaces are used as delimiters, or when no delimiter has been given\n  as input, there should not be any missing data between two fields.\n* When the variables are named (either by a flexible dtype or with `names`),\n  there must not be any header in the file (else a ValueError\n  exception is raised).\n* Individual values are not stripped of spaces by default.\n  When using a custom converter, make sure the function does remove spaces.\n\nReferences\n----------\n.. [1] NumPy User Guide, section `I/O with NumPy\n       &lt;https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html&gt;`_.\n\nExamples\n--------\n&gt;&gt;&gt; from io import StringIO\n&gt;&gt;&gt; import numpy as np\n\nComma delimited file with mixed dtype\n\n&gt;&gt;&gt; s = StringIO(u\"1,1.3,abcde\")\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n... ('mystring','S5')], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nUsing dtype = None\n\n&gt;&gt;&gt; _ = s.seek(0) # needed for StringIO example only\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=None,\n... names = ['myint','myfloat','mystring'], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nSpecifying dtype and names\n\n&gt;&gt;&gt; _ = s.seek(0)\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n... names=['myint','myfloat','mystring'], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nAn example with fixed-width columns\n\n&gt;&gt;&gt; s = StringIO(u\"11.3abcde\")\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n...     delimiter=[1,3,5])\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('intvar', '&lt;i8'), ('fltvar', '&lt;f8'), ('strvar', 'S5')])\n\nAn example to show comments\n\n&gt;&gt;&gt; f = StringIO('''\n... text,# of chars\n... hello world,11\n... numpy,5''')\n&gt;&gt;&gt; np.genfromtxt(f, dtype='S12,S12', delimiter=',')\narray([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n  dtype=[('f0', 'S12'), ('f1', 'S12')])\nFile:      ~/miniforge3/lib/python3.9/site-packages/numpy/lib/npyio.py\nType:      function\n\n\n\ndata = np.genfromtxt('../datasets/tennis-discrete-output.csv', delimiter=',')\ndata\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan]])\n\n\nWait! What happened?\n\ndata = np.genfromtxt('../datasets/tennis-discrete-output.csv', delimiter=',', dtype=str)\ndata\n\narray([['Day', 'Outlook', 'Temp', 'Humidity', 'Windy', 'Play'],\n       ['D1', 'Sunny', 'Hot', 'High', 'Weak', 'No'],\n       ['D2', 'Sunny', 'Hot', 'High', 'Strong', 'No'],\n       ['D3', 'Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n       ['D4', 'Rain', 'Mild', 'High', 'Weak', 'Yes'],\n       ['D5', 'Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n       ['D6', 'Rain', 'Cool', 'Normal', 'Strong', 'No'],\n       ['D7', 'Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n       ['D8', 'Sunny', 'Mild', 'High', 'Weak', 'No'],\n       ['D9', 'Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n       ['D10', 'Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n       ['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n       ['D12', 'Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n       ['D13', 'Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n       ['D14', 'Rain', 'Mild', 'High', 'Strong', 'No']], dtype='&lt;U8')\n\n\n\ndata.shape\n\n(15, 6)\n\n\nQuestion: Find the outlook on D11\n\nidx = np.argwhere(data[:, 0] == 'D11')[0, 0]\nidx\n\n11\n\n\n\ndata[idx]\n\narray(['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'], dtype='&lt;U8')\n\n\n\ndata[idx][1]\n\n'Sunny'\n\n\n\n\nReading CSV file using Pandas\n\ndf = pd.read_csv('../datasets/tennis-discrete-output.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\n\n\n0\nD1\nSunny\nHot\nHigh\nWeak\nNo\n\n\n1\nD2\nSunny\nHot\nHigh\nStrong\nNo\n\n\n2\nD3\nOvercast\nHot\nHigh\nWeak\nYes\n\n\n3\nD4\nRain\nMild\nHigh\nWeak\nYes\n\n\n4\nD5\nRain\nCool\nNormal\nWeak\nYes\n\n\n5\nD6\nRain\nCool\nNormal\nStrong\nNo\n\n\n6\nD7\nOvercast\nCool\nNormal\nStrong\nYes\n\n\n7\nD8\nSunny\nMild\nHigh\nWeak\nNo\n\n\n8\nD9\nSunny\nCool\nNormal\nWeak\nYes\n\n\n9\nD10\nRain\nMild\nNormal\nWeak\nYes\n\n\n10\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\n11\nD12\nOvercast\nMild\nHigh\nStrong\nYes\n\n\n12\nD13\nOvercast\nHot\nNormal\nWeak\nYes\n\n\n13\nD14\nRain\nMild\nHigh\nStrong\nNo\n\n\n\n\n\n\n\n\ndf['Day'] == 'D11'\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10     True\n11    False\n12    False\n13    False\nName: Day, dtype: bool\n\n\n\ndf[df['Day'] == 'D11']\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\n\n\n10\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\n\n\n\n\n\n\ndf[df['Day'] == 'D11']['Outlook']\n\n10    Sunny\nName: Outlook, dtype: object\n\n\n\ndf.query('Day == \"D11\"')['Outlook']\n\n10    Sunny\nName: Outlook, dtype: object\n\n\n\ndf.shape\n\n(14, 6)\n\n\nQuestion. How many times do we play v/s not play tennis\n\nser = df['Play']\nser\n\n0      No\n1      No\n2     Yes\n3     Yes\n4     Yes\n5      No\n6     Yes\n7      No\n8     Yes\n9     Yes\n10    Yes\n11    Yes\n12    Yes\n13     No\nName: Play, dtype: object\n\n\n\nunique_play_options = df['Play'].unique()\nunique_play_options\n\narray(['No', 'Yes'], dtype=object)\n\n\n\nfor option in unique_play_options:\n    print(option, (df['Play'] == option).sum())\n\nNo 5\nYes 9\n\n\n\ndf['Play'].value_counts()\n\nPlay\nYes    9\nNo     5\nName: count, dtype: int64\n\n\n\ndf.groupby('Play').size()\n\nPlay\nNo     5\nYes    9\ndtype: int64\n\n\n\ngby = df.groupby('Play')\n\n\n{k: len(v) for k, v in gby.groups.items()}\n\n{'No': 5, 'Yes': 9}\n\n\n\npd.crosstab(index=df['Play'], columns='count')\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nPlay\n\n\n\n\n\nNo\n5\n\n\nYes\n9\n\n\n\n\n\n\n\nWhat is the distribution of any given attribute?\n\ndef distribution(df, attribute):\n    return df[attribute].value_counts()\n\n\nser = distribution(df, 'Outlook')\n\n\nser\n\nOutlook\nSunny       5\nRain        5\nOvercast    4\nName: count, dtype: int64\n\n\n\ntype(ser)\n\npandas.core.series.Series\n\n\n\nser.values\n\narray([5, 5, 4])\n\n\n\nser.index\n\nIndex(['Sunny', 'Rain', 'Overcast'], dtype='object', name='Outlook')\n\n\n\ndistribution(df, 'Temp')\n\nTemp\nMild    6\nHot     4\nCool    4\nName: count, dtype: int64\n\n\nFinding entropy for target variable\n\ntarget_attribute = 'Play'\ndist_target = distribution(df, target_attribute)\n\n\ndist_target\n\nPlay\nYes    9\nNo     5\nName: count, dtype: int64\n\n\nNormalize distribution\n\ndist_target/dist_target.sum()\n\nPlay\nYes    0.642857\nNo     0.357143\nName: count, dtype: float64\n\n\n\ndf['Play'].value_counts(normalize=True)\n\nPlay\nYes    0.642857\nNo     0.357143\nName: proportion, dtype: float64\n\n\n\nnormalized_dist_target = dist_target/dist_target.sum()\n\nFor loop way of calculating entropy\n\ne = 0.0\nfor value, p in normalized_dist_target.items():\n    e = e - p * np.log2(p + 1e-6) # 1e-6 is added to avoid log(0)\nprint(e)\n\n0.9402830732836911\n\n\n\nnormalized_dist_target.apply(lambda x: -x * np.log2(x + 1e-6))\n\nPlay\nYes    0.409775\nNo     0.530508\nName: count, dtype: float64\n\n\n\nnormalized_dist_target.apply(lambda x: -x * np.log2(x + 1e-6)).sum()\n\n0.9402830732836911\n\n\nMore on crosstab\n\npd.crosstab(index=df['Outlook'], columns=df['Play'])\n\n\n\n\n\n\n\nPlay\nNo\nYes\n\n\nOutlook\n\n\n\n\n\n\nOvercast\n0\n4\n\n\nRain\n2\n3\n\n\nSunny\n3\n2\n\n\n\n\n\n\n\n\npd.crosstab(index=df['Outlook'], columns=df['Play']).T\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0\n2\n3\n\n\nYes\n4\n3\n2\n\n\n\n\n\n\n\n\ndf_attr = pd.crosstab(index=df['Play'], columns=df['Outlook'], normalize='columns')\ndf_attr\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.0\n0.4\n0.6\n\n\nYes\n1.0\n0.6\n0.4\n\n\n\n\n\n\n\nUsing groupby\n\ndf.groupby(['Play', 'Outlook']).size()\n\nPlay  Outlook \nNo    Rain        2\n      Sunny       3\nYes   Overcast    4\n      Rain        3\n      Sunny       2\ndtype: int64\n\n\n\ndf.groupby(['Play', 'Outlook']).size().index\n\nMultiIndex([( 'No',     'Rain'),\n            ( 'No',    'Sunny'),\n            ('Yes', 'Overcast'),\n            ('Yes',     'Rain'),\n            ('Yes',    'Sunny')],\n           names=['Play', 'Outlook'])\n\n\n\ndf.groupby(['Play', 'Outlook']).size().unstack('Outlook')\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\nNaN\n2.0\n3.0\n\n\nYes\n4.0\n3.0\n2.0\n\n\n\n\n\n\n\n\ndf_attr_groupby = df.groupby(['Play', 'Outlook']).size().unstack('Outlook').fillna(0)\ndf_attr_groupby\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.0\n2.0\n3.0\n\n\nYes\n4.0\n3.0\n2.0\n\n\n\n\n\n\n\nApply\n\nneg_plogp = df_attr.apply(lambda x: -x * np.log2(x + 1e-6), axis=0)\nneg_plogp\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.000000\n0.528770\n0.442178\n\n\nYes\n-0.000001\n0.442178\n0.528770\n\n\n\n\n\n\n\n\nneg_plogp.sum(axis=0).sort_index()\n\nOutlook\nOvercast   -0.000001\nRain        0.970948\nSunny       0.970948\ndtype: float64\n\n\n\ndf_attr_dist = distribution(df, 'Outlook')\nnorm_attr_dist = df_attr_dist/df_attr_dist.sum()\nnorm_attr_dist\n\nOutlook\nSunny       0.357143\nRain        0.357143\nOvercast    0.285714\nName: count, dtype: float64\n\n\n\n(norm_attr_dist*neg_plogp.sum(axis=0).sort_index()).sum()\n\n0.6935336657070463"
  },
  {
    "objectID": "notebooks/SFS_and_BFS.html",
    "href": "notebooks/SFS_and_BFS.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score as acc\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.datasets import fetch_california_housing\n\n\n\nfrom mlxtend.feature_selection import Sequential\n\n\n\n# Read data\ndata =  fetch_california_housing()\nX = data['data']\ny = data['target']\n\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.25,\n    random_state=42)\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\nprint('Training dataset shape:', X_train.shape, y_train.shape)\nprint('Testing dataset shape:', X_test.shape, y_test.shape)\n\nTraining dataset shape: (15480, 8) (15480,)\nTesting dataset shape: (5160, 8) (5160,)\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsfs1 = sfs(clf,\n           k_features=6,\n           forward=True,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sfs1.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.1s finished\n\n[2020-01-18 12:14:41] Features: 1/6 -- score: -0.9799855743872914[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.4s finished\n\n[2020-01-18 12:14:42] Features: 2/6 -- score: -0.6329184295861372[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.6s finished\n\n[2020-01-18 12:14:44] Features: 3/6 -- score: -0.6522227062026185[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:45] Features: 4/6 -- score: -0.6627208539646012[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:47] Features: 5/6 -- score: -0.6800772168838566[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.7s finished\n\n[2020-01-18 12:14:49] Features: 6/6 -- score: -0.6988981779449276\n\n\n\nfeat_cols = list(sfs1.k_feature_idx_)\n\n# data['features']\n\n\nnp.array(data['feature_names'])[feat_cols]\n\narray(['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'Latitude',\n       'Longitude'], dtype='&lt;U10')\n\n\n\ndata['feature_names']\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsbs = sfs(clf,\n           k_features=1,\n           forward=False,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sbs.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    5.2s finished\n\n[2020-01-18 12:26:50] Features: 7/1 -- score: -0.7097202737310716[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    3.9s finished\n\n[2020-01-18 12:26:54] Features: 6/1 -- score: -0.6985180206966245[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.8s finished\n\n[2020-01-18 12:26:57] Features: 5/1 -- score: -0.6766309576585353[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n\n[2020-01-18 12:26:58] Features: 4/1 -- score: -0.6683220592138266[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.2s finished\n\n[2020-01-18 12:27:00] Features: 3/1 -- score: -0.6613854217987167[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n\n[2020-01-18 12:27:00] Features: 2/1 -- score: -0.6320510743499647[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n\n[2020-01-18 12:27:00] Features: 1/1 -- score: -0.9799855743872914"
  },
  {
    "objectID": "notebooks/boosting-explanation.html",
    "href": "notebooks/boosting-explanation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\n\nx = np.linspace(0.04, 0.51, 1000)\n\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height &gt; MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': True,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=3)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\n\nText(0, 0.5, '$\\\\alpha_m$')\n\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=2)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/ensemble/alpha-boosting.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, np.exp(0.5*np.log((1-x)/x)), color='r', linewidth=2, label=r'$e^{\\alpha_m}$ ')\nplt.plot(x, np.exp(-0.5*np.log((1-x)/x)), color='g', linewidth=2, label=r'$e^{-\\alpha_m}$')\n\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(\"Weight Multiplier\")\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/ensemble/alpha-boosting-weight.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/ensemble-representation.html",
    "href": "notebooks/ensemble-representation.html",
    "title": "Representation of Ensemble Models",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nh = .02  # step size in the mesh\n\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            ]\n\n\n\nfrom latexify import latexify\ntry:\n    latexify()\nexcept:\n    pass\n\n\ndef plot_comparison(maximum_depth):\n    names = [\"Decision Tree (Depth %d)\" %maximum_depth, \"Random Forest\"]\n    classifiers = [\n    DecisionTreeClassifier(max_depth=maximum_depth),\n    RandomForestClassifier(max_depth=maximum_depth, n_estimators=200, max_features=1),\n   ]\n    figure = plt.figure(figsize=(8, 4))\n    i = 1\n    # iterate over datasets\n    for ds_cnt, ds in enumerate(datasets):\n        # preprocess dataset, split into training and test part\n        X, y = ds\n        X = StandardScaler().fit_transform(X)\n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=.4, random_state=42)\n\n        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                            np.arange(y_min, y_max, h))\n\n        # just plot the dataset first\n        cm = plt.cm.RdBu\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        if ds_cnt == 0:\n            ax.set_title(\"Input data\")\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n                edgecolors='k')\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        i += 1\n\n        # iterate over classifiers\n        for name, clf in zip(names, classifiers):\n            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n            clf.fit(X_train, y_train)\n            score = clf.score(X_test, y_test)\n\n            # Plot the decision boundary. For that, we will assign a color to each\n            # point in the mesh [x_min, x_max]x[y_min, y_max].\n            if hasattr(clf, \"decision_function\"):\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n            # Put the result into a color plot\n            Z = Z.reshape(xx.shape)\n            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n            # Plot the training points\n            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                    edgecolors='k')\n            # Plot the testing points\n            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                    edgecolors='k', alpha=0.6)\n\n            ax.set_xlim(xx.min(), xx.max())\n            ax.set_ylim(yy.min(), yy.max())\n            ax.set_xticks(())\n            ax.set_yticks(())\n            if ds_cnt == 0:\n                ax.set_title(name)\n            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                    size=15, horizontalalignment='right')\n            i += 1\n\n\n    plt.savefig(f\"../figures/ensemble/{str(maximum_depth)}-representation.pdf\" , transparent=True, bbox_inches=\"tight\")\n\n\nplot_comparison(2)\n\n\n\n\n\n\n\n\n\nplot_comparison(1)"
  },
  {
    "objectID": "notebooks/contour.html",
    "href": "notebooks/contour.html",
    "title": "Contour and Surface Plots",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport torch\n\ndef plot_surface_and_contour(f, function_name, uv = None, stride=4, alpha=1, scatter_pts=None, filled=True):\n    # Generate data\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    \n    # Convert to PyTorch tensors\n    X_torch = torch.from_numpy(X)\n    Y_torch = torch.from_numpy(Y)\n    X_torch.requires_grad_(True)\n    Y_torch.requires_grad_(True)\n\n    # Evaluate the function\n    Z = f(X_torch, Y_torch)\n\n    # Create the single figure with two subplots\n    fig = plt.figure()\n\n    # Plot the 3D surface on the first subplot\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot_surface(X, Y, Z.detach().numpy(), cmap='magma', edgecolor='none', alpha=alpha)  # Remove grid lines\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('z')\n    ax1.grid(False)\n    ax1.xaxis.pane.fill = False\n    ax1.yaxis.pane.fill = False\n    ax1.zaxis.pane.fill = False\n    ax1.view_init(elev=30, azim=30)\n    ax1.set_title('Surface Plot')\n    if scatter_pts is not None:\n        ax1.scatter(scatter_pts[0], scatter_pts[1], f(scatter_pts[0], scatter_pts[1]), s=100, c='black')\n    \n\n    # Plot the contour plot on the second subplot\n    ax2 = fig.add_subplot(122, aspect='equal')  # Set 1:1 aspect ratio\n    if filled:\n        scatter_color='white'\n        contour = ax2.contourf(X, Y, Z.detach().numpy(), levels=10, cmap='magma', alpha=alpha)\n    else:\n        scatter_color='black'\n        contour = ax2.contour(X, Y, Z.detach().numpy(), levels=10, cmap='magma', alpha=alpha)\n    if scatter_pts is not None:\n        ax2.scatter(scatter_pts[0], scatter_pts[1], s=10, c=scatter_color)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Contour Plot')\n\n    # Add a colorbar in between the subplots\n    divider = make_axes_locatable(ax2)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    cbar = plt.colorbar(contour, cax=cax)\n    \n    file_name = f\"../figures/mml/contour-{function_name}.pdf\"\n    if uv is not None:\n        u = uv[0](X_torch, Y_torch)\n        v = uv[1](X_torch, Y_torch)\n        # Quiver plot for gradient\n        ax2.quiver(X[::stride, ::stride], Y[::stride, ::stride], u[::stride, ::stride].detach().numpy(),\n                   v[::stride, ::stride].detach().numpy(), scale=140)\n        # Increase alpha of contourf\n        for c in contour.collections:\n            c.set_alpha(0.5)\n        \n        file_name = f\"../figures/mml/contour-{function_name}-with-gradient.pdf\"\n        \n\n    \n    # Save the figure\n    plt.tight_layout(pad=1.0, w_pad=1.0)\n    fig.savefig(file_name, bbox_inches=\"tight\")\n\n# Example usage:\n# Define your function f(x, y) and its gradient g(x, y)\n#f = lambda x, y: x**2 + y**2\n#g = lambda x, y: (2*x, 2*y)\n#plot_surface_and_contour(f, \"x_squared_plus_y_squared\", uv=(lambda x, y: 2*x, lambda x, y: 2*y))\n\n\nplot_surface_and_contour(lambda x, y: x**2 + y**2, \"x_squared_plus_y_squared_quiver\", \n                         uv=(lambda x, y: 2*x, lambda x, y: 2*y)\n                         ,stride=5)\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x**2 + y**2, \"x_squared_plus_y_squared\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x**2, \"x_squared\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: torch.abs(x) + torch.abs(y), \"mod_x_plus_mod_y\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: (x**2) * y, \"x_square_times_y\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x * y, \"x_times_y\")\n\n\n\n\n\n\n\n\n\ndef f(x, y):\n    return (14 + 3*x**2 +14*y**2 -12*x - 28*y + 12*x*y)/3\n\nx0, y0 = torch.tensor(4.0), torch.tensor(0.0)\n\n\n\ndel_x, del_y = torch.func.grad(f, argnums=(0, 1))(x0, y0)\nprint(del_x, del_y)\n\ntensor(4.) tensor(6.6667)\n\n\n\n\nxi = x0\nyi = y0\nalpha = 0.1\nfor i in range(30):\n    plot_surface_and_contour(f, f\"linreg-{i}\", alpha=0.8, scatter_pts=(xi, yi), filled=False)\n    del_x, del_y = torch.func.grad(f, argnums=(0, 1))(xi, yi)\n    xi = xi - alpha * del_x\n    yi = yi - alpha * del_y\n    print(xi, yi)\n\ntensor(3.6000) tensor(-0.6667)\ntensor(3.5467) tensor(-0.5511)\ntensor(3.4578) tensor(-0.5221)\ntensor(3.3751) tensor(-0.4846)\ntensor(3.2939) tensor(-0.4490)\ntensor(3.2147) tensor(-0.4141)\ntensor(3.1374) tensor(-0.3802)\ntensor(3.0620) tensor(-0.3470)\ntensor(2.9884) tensor(-0.3146)\ntensor(2.9165) tensor(-0.2830)\ntensor(2.8464) tensor(-0.2522)\ntensor(2.7780) tensor(-0.2221)\ntensor(2.7112) tensor(-0.1927)\ntensor(2.6461) tensor(-0.1640)\ntensor(2.5824) tensor(-0.1360)\ntensor(2.5204) tensor(-0.1087)\ntensor(2.4598) tensor(-0.0821)\ntensor(2.4006) tensor(-0.0560)\ntensor(2.3429) tensor(-0.0307)\ntensor(2.2866) tensor(-0.0059)\ntensor(2.2316) tensor(0.0183)\ntensor(2.1780) tensor(0.0419)\ntensor(2.1256) tensor(0.0649)\ntensor(2.0745) tensor(0.0874)\ntensor(2.0247) tensor(0.1093)\ntensor(1.9760) tensor(0.1308)\ntensor(1.9285) tensor(0.1517)\ntensor(1.8821) tensor(0.1720)\ntensor(1.8369) tensor(0.1919)\ntensor(1.7927) tensor(0.2114)\n\n\n/tmp/ipykernel_4080399/2389426738.py:23: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n  fig = plt.figure()"
  },
  {
    "objectID": "notebooks/basis2.html",
    "href": "notebooks/basis2.html",
    "title": "Basis Expansion in Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nx = np.linspace(-1, 1, 100)\n\n\nfrom sklearn.kernel_approximation import RBFSampler\n\n\nRBFSampler?\n\nInit signature: RBFSampler(*, gamma=1.0, n_components=100, random_state=None)\nDocstring:     \nApproximate a RBF kernel feature map using random Fourier features.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`.\n\nParameters\n----------\ngamma : 'scale' or float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2).\n    If ``gamma='scale'`` is passed then it uses\n    1 / (n_features * X.var()) as value of gamma.\n\n    .. versionadded:: 1.2\n       The option `\"scale\"` was added in 1.2.\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\nNystroem : Approximate a kernel map using a subset of the training data.\nPolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\nSkewedChi2Sampler : Approximate feature map for\n    \"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1, 1]\n&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)\n&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)\n&gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3)\n&gt;&gt;&gt; clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n&gt;&gt;&gt; clf.score(X_features, y)\n1.0\nFile:           ~/miniforge3/lib/python3.9/site-packages/sklearn/kernel_approximation.py\nType:           type\nSubclasses:     \n\n\n\nr= RBFSampler(n_components=5)\n\n\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=0.1)\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=20)\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))"
  },
  {
    "objectID": "notebooks/transcript.html",
    "href": "notebooks/transcript.html",
    "title": "YouTube video to transcript using openAI whisper and summary using OLLama",
    "section": "",
    "text": "try:\n    from pydub import AudioSegment\nexcept ImportError:\n    %pip install pydub\n    %pip install pydub[extras]\n    from pydub import AudioSegment\n    from pydub.playback import play\n\n\nfrom IPython.display import Audio\naudio_path = '../datasets/audio/Prime-minister.m4a'\naudio = AudioSegment.from_file(audio_path, format=\"m4a\")\naudio\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\ntry:    \n    import whisper\nexcept ImportError:\n    %pip install openai-whisper\n    import whisper\n\n\nwhisper_model = whisper.load_model(\"base.en\")\n\n\ntranscription = whisper_model.transcribe(audio_path, fp16=True, verbose=False)\n\n100%|██████████| 347/347 [00:00&lt;00:00, 367.83frames/s]\n\n\n\ntranscription\n\n{'text': ' Who is the Prime Minister of India?',\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 3.0,\n   'text': ' Who is the Prime Minister of India?',\n   'tokens': [50363, 5338, 318, 262, 5537, 4139, 286, 3794, 30, 50513],\n   'temperature': 0.0,\n   'avg_logprob': -0.34697675704956055,\n   'compression_ratio': 0.813953488372093,\n   'no_speech_prob': 0.005249415524303913}],\n 'language': 'en'}\n\n\n\nfrom IPython.display import Audio\n\n\ntry:\n    from gtts import gTTS\nexcept ImportError:\n    %pip install gtts\n    from gtts import gTTS\n\n\ndef speak(text, file):\n    tts = gTTS(text, lang='en')\n    with open(file, 'wb') as f:\n        tts.write_to_fp(f)\n    return Audio(file)\n\n\nspeak(transcription['text'], '../datasets/audio/pm-2.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n\n\nllm = Ollama(model=\"llama2\", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n\n\ndef answers(llm, prompt_qs, prompts, text):\n    outputs = []\n    for prompt, prompt_qs in zip(prompts, prompt_qs):\n        print(prompt_qs, end=\"\\n\")\n        output = llm(prompt, temperature=0.5)\n        #print(output, end=\"\\n\\n\")\n        print(\"\\n\" + \"==\"*50, end=\"\\n\\n\")\n    outputs.append(output) \n    return outputs\n\n\nprompt_qs = [\"Please be concise.\"] \nprompts = [q + \":\"+ transcription[\"text\"] for q in prompt_qs]\n\noutputs = answers(llm, prompt_qs, prompts, transcription[\"text\"])\n\nPlease be concise.\n\nThe Prime Minister of India is Narendra Modi.\n====================================================================================================\n\n\n\n\nspeak(outputs[0].replace(\"\\n\", \"\"), '../datasets/audio/pm-answer.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nReferences\n\nWhisper\nLangchain and LLama\nEnglish to Hindi using Transformers\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('https://www.youtube.com/watch?v=CuBzyh4Xmvk', width=500, height=300)\n\n\n        \n        \n\n\n\ntry:\n    import yt_dlp\nexcept ImportError:\n    %pip install yt_dlp\n    import yt_dlp\n\n\ndef download(video_id: str, save_path: str) -&gt; str:\n    video_url = f'https://www.youtube.com/watch?v={video_id}'\n    ydl_opts = {\n        'format': 'm4a/bestaudio/best',\n        'paths': {'home': save_path},\n        'outtmpl': {'default': \"lecture.m4a\"},\n        'postprocessors': [{\n            'key': 'FFmpegExtractAudio',\n            'preferredcodec': 'm4a',\n        }]\n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        error_code = ydl.download([video_url])\n        if error_code != 0:\n            raise Exception('Failed to download video')\n\n    return save_path\n\n\ndownload('CuBzyh4Xmvk', '../datasets/audio/')\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=CuBzyh4Xmvk\n[youtube] CuBzyh4Xmvk: Downloading webpage\n[youtube] CuBzyh4Xmvk: Downloading ios player API JSON\n[youtube] CuBzyh4Xmvk: Downloading android player API JSON\n[youtube] CuBzyh4Xmvk: Downloading m3u8 information\n[info] CuBzyh4Xmvk: Downloading 1 format(s): 140\n[download] ../datasets/audio/lecture.m4a has already been downloaded\n[download] 100% of   72.26MiB\n[ExtractAudio] Not converting audio ../datasets/audio/lecture.m4a; file is already in target format m4a\n\n\n'../datasets/audio/'\n\n\n\naudio_path = '../datasets/audio/lecture.m4a'\naudio = AudioSegment.from_file(audio_path, format=\"m4a\")\n\n\naudio[:13000]\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\ntranscription = whisper_model.transcribe(\"../datasets/audio/lecture.m4a\", fp16=True, verbose=False)\n\n 99%|█████████▉| 465481/468481 [02:07&lt;00:00, 3643.86frames/s]\n\n\n\nprint(transcription[\"text\"][:500].replace(\". \", \"\\n\"))\n\n Please look at the code mentioned above and please sign up on the Google Cloud\nWe've already started making some announcements\nYou will likely end up missing the announcements and you'll have no one else to play with\nThe second quick logistical announcement is that we'll have an extra lecture on Saturday, 11th Jan at 11am in 1.101\nSo a lot of ones over there\nAnd I think one or two people still have conflict, but in the larger, in the larger phone we'll have almost everyone available, so we\n\n\n\ntranscription.keys()\n\ndict_keys(['text', 'segments', 'language'])\n\n\n\ndef create_srt_from_transcription(transcription_objects, srt_file_path):\n    with open(srt_file_path, 'w') as srt_file:\n        index = 1  # SRT format starts with index 1\n\n        for entry in transcription_objects['segments']:\n            start_time = entry['start']\n            end_time = entry['end']\n            text = entry['text']\n\n            # Convert time to SRT format\n            start_time_str = format_time(start_time)\n            end_time_str = format_time(end_time)\n\n            # Write entry to SRT file\n            srt_file.write(f\"{index}\\n\")\n            srt_file.write(f\"{start_time_str} --&gt; {end_time_str}\\n\")\n            srt_file.write(f\"{text}\\n\\n\")\n\n            index += 1\n\ndef format_time(time_seconds):\n    minutes, seconds = divmod(time_seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},000\"\n\n\ncreate_srt_from_transcription(transcription, \"../datasets/audio/lecture.srt\")\n\n\n!head ../datasets/audio/lecture.srt\n\n1\n00:00:00,000 --&gt; 00:00:05,000\n Please look at the code mentioned above and please sign up on the Google Cloud.\n\n2\n00:00:05,000 --&gt; 00:00:08,000\n We've already started making some announcements.\n\n3\n00:00:08,000 --&gt; 00:00:14,000\n\n\n\nspeak(transcription['text'][:1300], '../datasets/audio/hello.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ntry:\n    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nexcept:\n    %pip install transformers -U -q\n    %pip install sentencepiece\n    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n    \n\n\n\n# download and save model\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n\n# import tokenizer\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n\n\ntext_to_translate = transcription[\"text\"][:500].split(\". \")\ntext_to_translate\n\n[' Please look at the code mentioned above and please sign up on the Google Cloud',\n \"We've already started making some announcements\",\n \"You will likely end up missing the announcements and you'll have no one else to play with\",\n \"The second quick logistical announcement is that we'll have an extra lecture on Saturday, 11th Jan at 11am in 1.101\",\n 'So a lot of ones over there',\n \"And I think one or two people still have conflict, but in the larger, in the larger phone we'll have almost everyone available, so we\"]\n\n\n\nmodel_inputs = tokenizer(text_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n\n\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n)\n\ntranslation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\n\n\ntranslation\n\n['कृपया उपर्युक्त कोड को देखें और कृपया Google क्लाउड पर साइन अप करें',\n 'हम पहले से ही कुछ घोषणाएं करने शुरू कर दी हैं',\n 'आप शायद अंत में घोषणाओं को खो देंगे और आप के साथ खेलने के लिए कोई अन्य नहीं होगा',\n 'दूसरा त्वरित लॉजिस्टिक घोषणा यह है कि हम एक अतिरिक्त व्याख्यान Saturday, 11th Jan 11am में 1.101 में होगा',\n 'तो वहाँ के बहुत से',\n 'और मुझे लगता है कि एक या दो लोग अभी भी संघर्ष है, लेकिन बड़ी, बड़ी फोन में हम लगभग सभी उपलब्ध हो जाएगा, तो हम']\n\n\n\nllm = Ollama(model=\"mistral\", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\nprompt_qs = [\"Please provide a bullet-point summary for the given text:\",\n             \"Highlight the important topics and subtopics in the given lecture:\",\n             \"Give us some question for a quiz based on the following text:\",\n             \"Summarize the following text in Hindi in 10 lines or less:\",\n            ]\n\nprompts = [q + \"\\n\\n\" + transcription[\"text\"] for q in prompt_qs]\n\noutputs = answers(llm, prompt_qs, prompts, transcription[\"text\"])\n\nPlease provide a bullet-point summary for the given text:\n * The text discusses a machine learning course and announces several logistical matters, including signing up for Google Cloud, an extra lecture on Saturday, and providing access to Google Docs for FAQ and project questions.\n* The definition of machine learning is discussed, with the ability to learn without explicit programming being highlighted.\n* A task to recognize digits from a dataset is introduced as an example, and rules are suggested for recognizing the digit \"4\".\n* It is explained that traditional programming involves explicitly programming rules, while machine learning involves using data and experience to learn patterns and make predictions.\n* An example of predicting tomato quality based on visual features is given, with the goal being to scale up this process in a business setting.\n* The concept of precision and recall in machine learning evaluation metrics is touched upon, as well as the idea of a decision tree algorithm for classification tasks.\n* The text encourages students to come up with simple rules for recognizing patterns and using decision trees to make predictions based on those rules.\n* The greedy algorithm for finding the best attribute for splitting data in a decision tree is mentioned, along with the concept of entropy as a measure of disorder or uncertainty in a dataset.\n====================================================================================================\n\nHighlight the important topics and subtopics in the given lecture:\n The given lecture covers several important topics related to machine learning, including:\n\n1. Machine Learning Definition and Concepts\n* Explicit programming vs. machine learning\n* Linear programming vs. machine learning\n* Learning into a computer program\n2. Recognizing Digits using Machine Learning\n* Writing rules to recognize digits from dataset\n3. Machine Learning Algorithms and Techniques\n* Decision Trees for Classification Problems\n4. Performance Measures in Machine Learning\n* Accuracy, Precision, Recall, F-score, and Matthew's Correlation Coefficient\n5. Optimal Decision Tree and Greedy Algorithm\n6. Data Preprocessing and Feature Selection\n7. Entropy, Information Gain, and Attribute Selection\n8. Decision Tree Implementation and Details\n9. Limitations and Future Work in Machine Learning\n\nThe lecture also includes discussions on the importance of data preprocessing, feature selection, and understanding performance measures for evaluating machine learning models effectively. It is important to note that this list might not be exhaustive, but it covers the main topics mentioned in the given lecture.\n====================================================================================================\n\nGive us some question for a quiz based on the following text:\n 1. What is machine learning and when was it first introduced?\n2. What is the difference between explicit programming and machine learning?\n3. In the context of machine learning, what is a training set and a test set?\n4. What are some rules for recognizing the digit \"4\" in an image dataset?\n5. What is precision and recall in machine learning?\n6. What is the difference between precision and Matthew's correlation coefficient?\n7. In the given example, what is the precision, recall, F score, and Matthew's correlation coefficient for predicting cancerous or not based on a dataset with 91 entries, of which 90 are not cancerous and 1 is cancerous?\n8. What is the main difference between decision trees and other machine learning algorithms?\n9. How does a decision tree algorithm work to classify data based on attributes?\n10. What is entropy in information theory and how is it related to decision trees?\n====================================================================================================\n\nSummarize the following text in Hindi in 10 lines or less:\n हेज़रूदीन भाषा में 10 शोधनावलिका:\n\n1. यहाँ देखें लोगों को Google Cloud पर स्IGN UP करें। अगर आप मिटावे पहुँचती हैं, तो बेहतरीन शायद अपने साथ लगे जाएँगे। पहले तक हमें कुछ अख्बरें दिये गये हैं, वहाँ आप नहीं पहूँचेंगे और कोई भी साथी नहीं होगा।\n2. सब्बत, 11-01-2023 रात्रिकाल 11:00 वज़न में एक और पदार्थ होगा। यहाँ कुछ लोगों की संख्या काफी बढ़ जाती है, और अधिक लोगों में सभी पहुँचेंगे।\n3. FAQ और प्रोजेक्ट्स जिसे Google Docs में पहले शार्तीय थे, आप सभी टिप्साहित रखा जाएंगे। अगर आपको कुछ सवाल है, तो इसमें टिप्साहित शून्य भागान बनाएं और हमें दिजिएं।\n4. अगर आपको कुछ प्रश्न है, तो Google Docs पर प्रोजेक्ट्स के लिए टिप्साहित दूं।\n5. पहले ध्यान रखें कि वही चीज़ पर और जो वही समझाई गयी थी, वही समझाएं।\n6. Arthur Sandler द्वारा 1959 से पहले के साथ उन्होंने \"मशीन लर्निंग\" (Machine Learning) का शब्द पहली बातचीत की।\n7. एक सामग्री के लिए वेज़न से अपनी शिकाई करने के लिए इंजामत है।\n8. प्रोग्राम स्वयं प्रोग्राम है, नहीं दोनों तक प्रोग्राम होते हैं।\n9. आज के लिए काम करने वाले प्रश्रेण की बताव क्या है? यह सामग्री के वेज़न से अपनी शिकाई करने के लिए इंजामत है।\n10. दिए गये डिगिट्स (0-9) के लिए, पहली अध्ययन के लिए एक प्रोग्राम बनाएं जिससे वे दिगिट्स recognize करें।\n===================================================================================================="
  },
  {
    "objectID": "notebooks/dummy-variables-multi-colinearity.html",
    "href": "notebooks/dummy-variables-multi-colinearity.html",
    "title": "Dummy Variables and Multi-collinearity",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\n\n\nx1 = np.array([1, 2, 3])\nx2 = 2*x1\n\ny = np.array([4, 6, 8])\n\n\nall_ones = np.ones(x1.shape[0])\nX = np.array([all_ones, x1, x2]).T\n\n\nX.shape\n\n(3, 3)\n\n\n\nX\n\narray([[1., 1., 2.],\n       [1., 2., 4.],\n       [1., 3., 6.]])\n\n\n\ndef solve_normal_equation(X, y):\n    try:\n        theta = np.linalg.inv(X.T @ X) @ X.T @ y\n        return theta\n    except np.linalg.LinAlgError:\n        print('The matrix is singular')\n        print(\"X.T @ X = \\n\", X.T @ X)\n        return None\n    \n### Assignment question: Use np.linalg.solve instead of inv. Why is this better?\n\n\nsolve_normal_equation(X, y)\n\nThe matrix is singular\nX.T @ X = \n [[ 3.  6. 12.]\n [ 6. 14. 28.]\n [12. 28. 56.]]\n\n\n\nnp.linalg.matrix_rank(X), np.linalg.matrix_rank(X.T @ X)\n\n(2, 2)\n\n\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\ndata = np.array([x1, x2]).T\n\nlr.fit(data, y)\nlr.coef_, lr.intercept_\n\n\n# Assignment question: figure why sklearn is able to solve the problem\n\n(array([0.4, 0.8]), 2.0)\n\n\n\n# Regularization\n\neps = 1e-5\nX = np.array([all_ones, x1, x2]).T\nX = np.eye(3)*eps + X\nX\n\narray([[1.00001, 1.     , 2.     ],\n       [1.     , 2.00001, 4.     ],\n       [1.     , 3.     , 6.00001]])\n\n\n\nnp.linalg.matrix_rank(X)\n\n3\n\n\n\nsolve_normal_equation(X, y)\n\narray([2.00023248, 1.19987743, 0.40001887])\n\n\n\n# Drop variables\nX = np.array([all_ones, x1]).T\nprint(X)\n\n[[1. 1.]\n [1. 2.]\n [1. 3.]]\n\n\n\nsolve_normal_equation(X, y)\n\narray([2., 2.])\n\n\n\n# Dummy variables\n\n## dataset\nnum_records = 12\nwindspeed = np.random.randint(0, 10, num_records)\nvehicles = np.random.randint(100, 500, num_records)\ndirection = np.random.choice(['N', 'S', 'E', 'W'], num_records)\npollution = np.random.randint(0, 100, num_records)\n\ndf = pd.DataFrame({'windspeed': windspeed, 'vehicles': vehicles, 'direction': direction, 'pollution': pollution})\ndf\n\n\n\n\n\n\n\n\nwindspeed\nvehicles\ndirection\npollution\n\n\n\n\n0\n0\n355\nW\n30\n\n\n1\n2\n367\nS\n30\n\n\n2\n2\n447\nS\n78\n\n\n3\n1\n223\nE\n32\n\n\n4\n1\n272\nS\n16\n\n\n5\n9\n394\nS\n36\n\n\n6\n0\n333\nN\n45\n\n\n7\n3\n308\nW\n52\n\n\n8\n7\n480\nN\n24\n\n\n9\n9\n360\nN\n74\n\n\n10\n0\n125\nS\n36\n\n\n11\n9\n401\nS\n62\n\n\n\n\n\n\n\n\ndef fit_data(df, X, y):\n    try:\n        lr = LinearRegression()\n        lr.fit(X, y)\n        rep = f\"y = {lr.intercept_:0.2f}\"\n        for i, coef in enumerate(lr.coef_):\n            rep += f\" + {coef:0.2f}*{df.columns[i]}\"\n        return rep\n    except Exception as e:\n        print(e)\n        return None\n        \n\n\nfit_data(df, df[df.columns[:-1]], df['pollution'])\n\ncould not convert string to float: 'W'\n\n\n\n# Ordinal encoding\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nenc = OrdinalEncoder()\n\n\ndf2 = df.copy()\ndf2['direction'] = enc.fit_transform(df[['direction']]).flatten()\ndf2\n\n\n\n\n\n\n\n\nwindspeed\nvehicles\ndirection\npollution\n\n\n\n\n0\n0\n355\n3.0\n30\n\n\n1\n2\n367\n2.0\n30\n\n\n2\n2\n447\n2.0\n78\n\n\n3\n1\n223\n0.0\n32\n\n\n4\n1\n272\n2.0\n16\n\n\n5\n9\n394\n2.0\n36\n\n\n6\n0\n333\n1.0\n45\n\n\n7\n3\n308\n3.0\n52\n\n\n8\n7\n480\n1.0\n24\n\n\n9\n9\n360\n1.0\n74\n\n\n10\n0\n125\n2.0\n36\n\n\n11\n9\n401\n2.0\n62\n\n\n\n\n\n\n\n\nfit_data(df2, df2[df2.columns[:-1]], df2['pollution'])\n\n'y = 26.49 + 1.49*windspeed + 0.03*vehicles + 1.02*direction'\n\n\n\npd.Series({x: i for i, x in enumerate(enc.categories_[0])})\n\nE    0\nN    1\nS    2\nW    3\ndtype: int64\n\n\n\n# One-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\n\n\ndirection_ohe = ohe.fit_transform(df[['direction']])\ndirection_ohe\n\narray([[0., 0., 0., 1.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.]])\n\n\n\ncol_names_ohe = [f\"Is it {x}?\" for x in enc.categories_[0]]\n\n\ndirection_ohe_df = pd.DataFrame(direction_ohe, columns=col_names_ohe)\ndirection_ohe_df\n\n\n\n\n\n\n\n\nIs it E?\nIs it N?\nIs it S?\nIs it W?\n\n\n\n\n0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n1.0\n0.0\n\n\n5\n0.0\n0.0\n1.0\n0.0\n\n\n6\n0.0\n1.0\n0.0\n0.0\n\n\n7\n0.0\n0.0\n0.0\n1.0\n\n\n8\n0.0\n1.0\n0.0\n0.0\n\n\n9\n0.0\n1.0\n0.0\n0.0\n\n\n10\n0.0\n0.0\n1.0\n0.0\n\n\n11\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n# Confirm that we can write Is it W? as a linear combination of the other columns\n1-direction_ohe_df[[\"Is it N?\", \"Is it S?\", \"Is it E?\"]].sum(axis=1) - direction_ohe_df[\"Is it W?\"]\n\n0     0.0\n1     0.0\n2     0.0\n3     0.0\n4     0.0\n5     0.0\n6     0.0\n7     0.0\n8     0.0\n9     0.0\n10    0.0\n11    0.0\ndtype: float64\n\n\n\nX = np.hstack([df[['windspeed', 'vehicles']].values, direction_ohe])\n\n\nX\n\narray([[  0., 355.,   0.,   0.,   0.,   1.],\n       [  2., 367.,   0.,   0.,   1.,   0.],\n       [  2., 447.,   0.,   0.,   1.,   0.],\n       [  1., 223.,   1.,   0.,   0.,   0.],\n       [  1., 272.,   0.,   0.,   1.,   0.],\n       [  9., 394.,   0.,   0.,   1.,   0.],\n       [  0., 333.,   0.,   1.,   0.,   0.],\n       [  3., 308.,   0.,   0.,   0.,   1.],\n       [  7., 480.,   0.,   1.,   0.,   0.],\n       [  9., 360.,   0.,   1.,   0.,   0.],\n       [  0., 125.,   0.,   0.,   1.,   0.],\n       [  9., 401.,   0.,   0.,   1.,   0.]])\n\n\n\nX_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n\n\nX_aug\n\narray([[  1.,   0., 355.,   0.,   0.,   0.,   1.],\n       [  1.,   2., 367.,   0.,   0.,   1.,   0.],\n       [  1.,   2., 447.,   0.,   0.,   1.,   0.],\n       [  1.,   1., 223.,   1.,   0.,   0.,   0.],\n       [  1.,   1., 272.,   0.,   0.,   1.,   0.],\n       [  1.,   9., 394.,   0.,   0.,   1.,   0.],\n       [  1.,   0., 333.,   0.,   1.,   0.,   0.],\n       [  1.,   3., 308.,   0.,   0.,   0.,   1.],\n       [  1.,   7., 480.,   0.,   1.,   0.,   0.],\n       [  1.,   9., 360.,   0.,   1.,   0.,   0.],\n       [  1.,   0., 125.,   0.,   0.,   1.,   0.],\n       [  1.,   9., 401.,   0.,   0.,   1.,   0.]])\n\n\n\nX_aug.shape\n\n(12, 7)\n\n\n\nnp.linalg.matrix_rank(X_aug), np.linalg.matrix_rank(X_aug.T @ X_aug), (X_aug.T @ X_aug).shape\n\n(6, 6, (7, 7))\n\n\n\npd.DataFrame(X_aug.T @ X_aug)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n12.0\n43.0\n4065.0\n1.0\n3.0\n6.0\n2.0\n\n\n1\n43.0\n311.0\n16802.0\n1.0\n16.0\n23.0\n3.0\n\n\n2\n4065.0\n16802.0\n1481651.0\n223.0\n1173.0\n2006.0\n663.0\n\n\n3\n1.0\n1.0\n223.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n3.0\n16.0\n1173.0\n0.0\n3.0\n0.0\n0.0\n\n\n5\n6.0\n23.0\n2006.0\n0.0\n0.0\n6.0\n0.0\n\n\n6\n2.0\n3.0\n663.0\n0.0\n0.0\n0.0\n2.0\n\n\n\n\n\n\n\n\nohe = OneHotEncoder(sparse_output=False, drop='first')\nohe.fit_transform(df[['direction']])\n\narray([[0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\n\ndirection_ohe_n_1 = ohe.fit_transform(df[['direction']])\ncol_names_ohe_n_1 = [f\"Is it {x}?\" for x in enc.categories_[0][1:]]\ndf_ohe_n_1 = pd.DataFrame(direction_ohe_n_1, columns=col_names_ohe_n_1)\ndf_ohe_n_1\n\n\n\n\n\n\n\n\nIs it N?\nIs it S?\nIs it W?\n\n\n\n\n0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n1.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n5\n0.0\n1.0\n0.0\n\n\n6\n1.0\n0.0\n0.0\n\n\n7\n0.0\n0.0\n1.0\n\n\n8\n1.0\n0.0\n0.0\n\n\n9\n1.0\n0.0\n0.0\n\n\n10\n0.0\n1.0\n0.0\n\n\n11\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\nX = np.hstack([df[['windspeed', 'vehicles']].values, df_ohe_n_1.values])\nX_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n\nX_aug\n\narray([[  1.,   0., 355.,   0.,   0.,   1.],\n       [  1.,   2., 367.,   0.,   1.,   0.],\n       [  1.,   2., 447.,   0.,   1.,   0.],\n       [  1.,   1., 223.,   0.,   0.,   0.],\n       [  1.,   1., 272.,   0.,   1.,   0.],\n       [  1.,   9., 394.,   0.,   1.,   0.],\n       [  1.,   0., 333.,   1.,   0.,   0.],\n       [  1.,   3., 308.,   0.,   0.,   1.],\n       [  1.,   7., 480.,   1.,   0.,   0.],\n       [  1.,   9., 360.,   1.,   0.,   0.],\n       [  1.,   0., 125.,   0.,   1.,   0.],\n       [  1.,   9., 401.,   0.,   1.,   0.]])\n\n\n\nnp.linalg.matrix_rank(X_aug), np.linalg.matrix_rank(X_aug.T @ X_aug), (X_aug.T @ X_aug).shape\n\n(6, 6, (6, 6))\n\n\n\n# Interepeting dummy variables\n\n## dataset\n\nX = np.array(['F', 'F', 'F', 'M', 'M'])\ny = np.array([5, 5.2, 5.4, 5.8, 6])\n\n\nfrom sklearn.preprocessing import LabelBinarizer\nl = LabelBinarizer()\nl.fit_transform(X)\n\narray([[0],\n       [0],\n       [0],\n       [1],\n       [1]])\n\n\n\nX_binary = 1 - l.fit_transform(X)\n\n\nX_binary    \n\narray([[1],\n       [1],\n       [1],\n       [0],\n       [0]])\n\n\n\nlr = LinearRegression()\nlr.fit(X_binary, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlr.coef_, lr.intercept_\n\n(array([-0.7]), 5.8999999999999995)\n\n\n\ny[(X_binary==0).flatten()].mean()\n\n5.9\n\n\n\ny[(X_binary==1).flatten()].mean()\n\n5.2"
  },
  {
    "objectID": "notebooks/Linear Regression Notebook.html",
    "href": "notebooks/Linear Regression Notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "notebooks/Gradient Descent.html",
    "href": "notebooks/Gradient Descent.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000) y = x**2 plt.plot(x,y) plt.title(“Cost Function”)\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/hyperparameter-optimisation.html",
    "href": "notebooks/hyperparameter-optimisation.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\n\nMakeMoons Dataset\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\nlatexify(fig_width=5, fig_height=4)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label='Train') \nformat_axes(plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\n\n#Define the hyperparameters' possible values\nmax_depth_values = [1,2,3,4,5,6,7,8,9,10]\nmin_samples_split_values = [2,3,4,5,6,7,8]\ncriteria_values = ['gini', 'entropy']\n\nNested For Loops\n\nbest_accuracy = 0\nbest_params = {}\n\nfor max_depth in max_depth_values:\n    for min_samples_split in min_samples_split_values:\n        for criterion in criteria_values:\n            # Define the Decision Tree Classifier\n            dt_classifier = DecisionTreeClassifier(\n                max_depth=max_depth,\n                min_samples_split=min_samples_split,\n                criterion=criterion,\n                random_state=42\n            )\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            \n            # Check if this combination gives a better accuracy\n            if val_accuracy &gt; best_accuracy:\n                best_accuracy = val_accuracy\n                best_params = {\n                    'max_depth': max_depth,\n                    'min_samples_split': min_samples_split,\n                    'criterion': criterion\n                }\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_accuracy)\n\n# Train the model with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n\n# Evaluate on the test set\ntest_accuracy = best_dt_classifier.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'entropy'}\nBest Validation Accuracy: 0.925\nTest Accuracy: 0.8950\n\n\nUsing Itertools\n\nfrom itertools import product\n\nbest_accuracy = 0\nbest_params = {}\n\n# Use itertools.product for a more succinct code\nfor max_depth, min_samples_split, criterion in product(max_depth_values, min_samples_split_values, criteria_values):\n    # Define the Decision Tree Classifier\n    dt_classifier = DecisionTreeClassifier(\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        criterion=criterion,\n        random_state=42\n    )\n    dt_classifier.fit(X_train, y_train)\n    \n    # Evaluate on the validation set\n    val_accuracy = dt_classifier.score(X_val, y_val)\n    \n    # Check if this combination gives a better accuracy\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_params = {\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'criterion': criterion\n        }\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_accuracy)\n\n# Train the model with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n\n# Evaluate on the test set\ntest_accuracy = best_dt_classifier.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'entropy'}\nBest Validation Accuracy: 0.925\nTest Accuracy: 0.8950\n\n\nUsing Sklearn Grid Search (5 fold Cross-Validation)\n\nfrom sklearn.model_selection import GridSearchCV\n# Define the Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'max_depth': max_depth_values,\n    'min_samples_split': min_samples_split_values,\n    'criterion': criteria_values\n}\n\nX_train_val = np.concatenate([X_train, X_val], axis=0)\ny_train_val = np.concatenate([y_train, y_val], axis=0)\n\n# Use GridSearchCV for hyperparameter tuning\nnum_inner_folds = 5\ngrid_search = GridSearchCV(dt_classifier, param_grid, scoring='accuracy', cv=num_inner_folds)\ngrid_search.fit(X_train_val, y_train_val)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Evaluate on the test set\ntest_accuracy = grid_search.best_estimator_.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 8}\nTest Accuracy: 0.9000"
  },
  {
    "objectID": "notebooks/bias-variance.html",
    "href": "notebooks/bias-variance.html",
    "title": "Bias Variance Tradeoff",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nfrom latexify import latexify, format_axes\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nimport pandas as pd\nimport ipywidgets as widgets\n\n\nlatexify(columns=2)\n\n\nx_overall = np.linspace(0, 10, 50)\nf_x = 0.2*np.sin(x_overall) + 0.2*np.cos(2*x_overall)+ 0.6*x_overall - 0.05*x_overall**2 - 0.003*x_overall**3\n\neps = np.random.normal(0, 1, 50)\ny_overall = f_x + eps\nplt.plot(x_overall, f_x, label = 'True function')\nplt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\nformat_axes(plt.gca())\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef fit_plot_tree(x, y, depth=1, extra=None):\n    dt = DecisionTreeRegressor(max_depth=depth)\n    dt.fit(x.reshape(-1, 1), y)\n    y_pred = dt.predict(x.reshape(-1, 1))\n    plt.figure()\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    label = r\"$\\hat{f}$\" if not extra else fr\"$\\hat{{f}}_{{{extra}}}$\"\n\n    plt.plot(x, y_pred, label = label, lw=2)\n\n    format_axes(plt.gca())\n    plt.legend()\n    plt.title(f\"Depth = {depth}\")\n    return dt\n\n\nfor i in range(1, 10):\n    fit_plot_tree(x_overall, y_overall, i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef fit_plot_polynomial(x, y, degree=1, extra=None, ax=None):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x.reshape(-1, 1), y)\n    y_pred = model.predict(x.reshape(-1, 1))\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    ax.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    ax.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    label = r\"$\\hat{f}$\" if not extra else fr\"$\\hat{{f}}_{{{extra}}}$\"\n\n    ax.plot(x, y_pred, label = label, lw=2)\n\n    format_axes(ax)\n    ax.legend()\n    ax.set_title(f\"Degree = {degree}\")\n    return model\n\n\nfit_plot_polynomial(x_overall, y_overall, 5)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])PolynomialFeaturesPolynomialFeatures(degree=5)LinearRegressionLinearRegression()\n\n\n\n\n\n\n\n\n\n\ndef plot_degree(degree=1):\n    regs = []\n    fig, axes = plt.subplots(5, 2, figsize=(8, 12), sharex=True, sharey=True)\n\n    for i, ax in enumerate(axes.flatten()):\n        idx = np.random.choice(np.arange(1, 49), 15, replace=False)\n        idx = np.concatenate([[0], idx, [49]])\n        idx.sort()\n        x = x_overall[idx]\n        y = y_overall[idx]\n        regs.append(fit_plot_polynomial(x, y, degree=degree, extra=i, ax=ax))\n        # remove legend\n        #ax.legend().remove()\n        ax.scatter(x_overall[idx], y_overall[idx], s=50, c='b', label='Sample', alpha=0.1)\n        ax.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return regs\n\n\n_ = plot_degree(5)\n\n\n\n\n\n\n\n\n\nregs = {}\nfor i in range(0, 10):\n    regs[i] = plot_degree(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef plot_predictions(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    plt.plot(x_test, y_pred.mean(axis=0), label = r'$\\hat{f}$', lw=2)\n    plt.plot(x_test, f_x, label = r'$f_{true}$', lw=2)\n    plt.plot(x_test, y_pred.T, lw=1, c='k', alpha=0.5)  \n    format_axes(plt.gca())\n    plt.legend()\n\nplot_predictions(regs[1])\n\n\n\n\n\n\n\n\n\ndef plot_bias(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    #plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    plt.plot(x_test, y_pred_mean, label = r'$\\bar{f}$', lw=2)\n    plt.fill_between(x_test, y_pred_mean, f_x, alpha=0.2, color='green', label = 'Bias')\n    plt.legend()\n\nplot_bias(regs[7])\n\n\n\n\n\n\n\n\n\ndef plot_variance(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    #plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    plt.plot(x_test, y_pred_mean, label = r'$\\bar{f}$', lw=2)\n    plt.fill_between(x_test, y_pred_mean - y_pred_var, y_pred_mean + y_pred_var, alpha=0.2, color='red', label = 'Variance')\n    plt.legend()\n\n\nplot_variance(regs[8])\n\n\n\n\n\n\n\n\n\n# Plot bias^2 and variance for different depths as bar plot\n\ndef plot_bias_variance(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    bias = (y_pred_mean - f_x)**2\n    var = y_pred_var\n    return bias.sum(), var.sum()\n\n\nbs = {}\nvs = {}\nfor i in range(1, 8):\n    bs[i], vs[i] = plot_bias_variance(regs[i])\n\n\ndf = pd.DataFrame({'Bias': bs, 'Variance': vs})\n\n\ndf.plot.bar(rot=0)"
  },
  {
    "objectID": "notebooks/Maths for ML-2.html",
    "href": "notebooks/Maths for ML-2.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2 + Y**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Filled Contours Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-1.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-2.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(np.abs(X) + np.abs(Y))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-3.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-2.0, 8.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(Y*(X**2))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-4.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X*Y)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-5.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\n# Contour Plot\nX, Y = np.mgrid[-1:1:100j, -1:1:100j]\nZ = X**2 + Y**2 \ncp = plt.contour(X, Y, Z)\n# cb = plt.colorbar(cp)\n\n# Vector Field\nY, X = np.mgrid[-1:1:30j, -1:1:30j]\nU = 2*X\nV = 2*Y\nspeed = np.sqrt(U**2 + V**2)\nUN = U/speed\nVN = V/speed\nquiv = plt.quiver(X, Y, UN, VN,  # assign to var\n           color='Teal', \n           headlength=7)\nplt.savefig(\"gradient-field.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncp = ax.contour(X, Y, Z,levels=[1,2,3,4,5,6,7,8,9])\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(0.11,4.5,600)\nplt.plot(d,0.5/d)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-6.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i in np.linspace(0,2,100):\n    print (i,0.5 - i**2 + 1/i**2)\n\n0.0 inf\n0.020202020202020204 2450.7495918783793\n0.04040404040404041 613.0608675135189\n0.06060606060606061 272.74632690541773\n0.08080808080808081 153.63409505407608\n0.10101010101010102 98.4997969594939\n0.12121212121212122 68.54780762167124\n0.14141414141414144 50.4851040814244\n0.16161616161616163 38.75903646630445\n0.18181818181818182 30.71694214876033\n0.20202020202020204 24.96168783797571\n0.22222222222222224 20.700617283950617\n0.24242424242424243 17.45685548668503\n0.26262626262626265 14.929548156238129\n0.2828282828282829 12.92128367263648\n0.30303030303030304 11.298172635445361\n0.32323232323232326 9.966809927717833\n0.3434343434343435 8.860426554171964\n0.36363636363636365 7.930268595041323\n0.38383838383838387 7.1400642169759925\n0.4040404040404041 6.462376351902866\n0.42424242424242425 5.876140814452502\n0.4444444444444445 5.364969135802469\n0.4646464646464647 4.9159562148764175\n0.48484848484848486 4.518828196740127\n0.5050505050505051 4.165323987348229\n0.5252525252525253 3.848739962230637\n0.5454545454545455 3.5635904499540856\n0.5656565656565657 3.305351527280638\n0.5858585858585859 3.0702655556635308\n0.6060606060606061 2.8551905417814507\n0.6262626262626263 2.65748294812874\n0.6464646464646465 2.474905726496339\n0.6666666666666667 2.305555555555555\n0.686868686868687 2.1478048326048214\n0.7070707070707072 2.0002550968351827\n0.7272727272727273 1.8616993801652892\n0.7474747474747475 1.7310915822381832\n0.7676767676767677 1.6075214108402638\n0.787878787878788 1.4901937611727818\n0.8080808080808082 1.3784116576114678\n0.8282828282828284 1.27156207154134\n0.8484848484848485 1.1691040741365415\n0.8686868686868687 1.0705588948578604\n0.888888888888889 0.9755015432098765\n0.9090909090909092 0.8835537190082641\n0.9292929292929294 0.7943777895623855\n0.9494949494949496 0.7076716541475031\n0.9696969696969697 0.6231643494605139\n0.98989898989899 0.5406122763442311\n1.0101010101010102 0.4597959493929188\n1.0303030303030305 0.3805171882397417\n1.0505050505050506 0.302596683242079\n1.0707070707070707 0.22587187959584054\n1.090909090909091 0.15019513314967814\n1.1111111111111112 0.07543209876543189\n1.1313131313131315 0.0014603183062319447\n1.1515151515151516 -0.07183201951522311\n1.1717171717171717 -0.14454717092494984\n1.191919191919192 -0.2167788004559923\n1.2121212121212122 -0.2886128328741967\n1.2323232323232325 -0.36012820815496915\n1.2525252525252526 -0.4313975519179223\n1.272727272727273 -0.5024877719682923\n1.292929292929293 -0.5734605901083917\n1.3131313131313131 -0.6443730171236\n1.3333333333333335 -0.7152777777777782\n1.3535353535353536 -0.7862236917418948\n1.373737373737374 -0.8572560156014735\n1.393939393939394 -0.9284167504222499\n1.4141414141414144 -0.9997449187817161\n1.4343434343434345 -1.0712768146824043\n1.4545454545454546 -1.143046229338843\n1.474747474747475 -1.2150846554637709\n1.494949494949495 -1.2874214723620951\n1.5151515151515154 -1.3600841138659328\n1.5353535353535355 -1.4330982209048717\n1.5555555555555556 -1.5064877802973042\n1.575757575757576 -1.58027525116686\n1.595959595959596 -1.6544816802290594\n1.6161616161616164 -1.729126807054128\n1.6363636363636365 -1.8042291602897669\n1.6565656565656568 -1.8798061457204203\n1.676767676767677 -1.9558741269450486\n1.696969696969697 -2.032448499372201\n1.7171717171717173 -2.1095437581575784\n1.7373737373737375 -2.187173560644274\n1.7575757575757578 -2.2653507838082487\n1.777777777777778 -2.344087577160494\n1.7979797979797982 -2.423395411511965\n1.8181818181818183 -2.503285123966943\n1.8383838383838385 -2.583766959474586\n1.8585858585858588 -2.664850609236279\n1.878787878787879 -2.7465452462378015\n1.8989898989898992 -2.828859558149688\n1.9191919191919193 -2.9118017778162164\n1.9393939393939394 -2.9953797115329435\n1.9595959595959598 -3.079600765294187\n1.97979797979798 -3.1644719691753447\n2.0 -3.25\n\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n  \n\n\n\nxlist = np.linspace(-2.0, 2.0, 100)\nylist = np.linspace(-2.0, 2.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncontours = ax.contour(X, Y, Z,levels=[0.1,0.2,.3,.4,.5,.6,.7,.8,.9,1])#,levels=[.5**(.5),2,3,4,5,6,7,8,9])\nplt.clabel(contours, inline=True, fontsize=8)\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(-4,5,600)\nplt.plot(d,1-d)\nplt.xlim(-1,1)\nplt.ylim(-1,1)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-7.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n(2*0.5**2)**(.5)\n\n0.7071067811865476"
  },
  {
    "objectID": "notebooks/logistic-apple-oranges.html",
    "href": "notebooks/logistic-apple-oranges.html",
    "title": "Logistic Regression - I",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.patches as mpatches\n\n\nx = np.array([0, 0.1, 0.2, 0.3, 0.6, 0.7, 0.9])\n\n\ny = (x&gt;0.4).astype('int')\n\n\ny\n\narray([0, 0, 0, 0, 1, 1, 1])\n\n\n\n\nlatexify()\nplt.scatter(x, np.zeros_like(x), c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.xlabel('Radius')\nplt.gca().yaxis.set_visible(False) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-orange-tomatoes-original.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nFitting linear model\n\nfrom sklearn.linear_model import LinearRegression\n\n\nlinr_reg = LinearRegression()\n\n\nlinr_reg.fit(x.reshape(-1, 1), y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.axhline(y=1, color='grey', label='P(y=1)')\nplt.axhline(y=0, color='grey')\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\n (0.5-linr_reg.intercept_)/linr_reg.coef_\n\narray([0.44857143])\n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dash = np.append(x, 2.5)\ny_dash = np.append(y, 1)\nlinr_reg.fit(x_dash.reshape(-1, 1), y_dash)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nplt.plot(np.linspace(0, 2.5, 50), linr_reg.predict(np.linspace(0, 2.5, 50).reshape(-1, 1)))\nplt.scatter(x_dash, y_dash, c=y_dash)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch])\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision-modified.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(penalty='none', solver='lbfgs')\n\n\nclf.fit(x.reshape(-1,1), y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none')\n\n\n\nclf.coef_\n\narray([[55.99493009]])\n\n\n\n-clf.intercept_[0]/clf.coef_[0]\n\narray([0.4484548])\n\n\n\nclf.intercept_\n\narray([-25.11119514])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\nlatexify()\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nplt.axvline(x = -clf.intercept_[0]/clf.coef_[0],label='Decision Boundary',linestyle='--',color='k',lw=1)\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.legend(handles=[black_patch, yellow_patch, blue_patch])\nplt.axhspan(0,1, xmin=0, xmax=0.49, linestyle='--',color='darkblue',lw=1, alpha=0.2)\nplt.axhspan(0,0.001, xmin=0, xmax=0.49, linestyle='--',color='k',lw=1, )\n\nplt.axhspan(0,1, xmax=1, xmin=0.49, linestyle='--',color='yellow',lw=1, alpha=0.2)\nplt.axhspan(1,1.001,  xmax=1, xmin=0.49, linestyle='--',color='k',lw=1, )\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision-ideal.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\nplt.scatter(x, y, c=y)\nlatexify()\nplt.axvline(-clf.intercept_[0]/clf.coef_[0], lw=2, color='black')\nplt.axhline(0.5, linestyle='--',color='k',lw=3, label='P(y=1) = P(y=0)')\nplt.ylabel(\"P(y=1)\")\nplt.xlabel('Radius')\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nsigmoid_patch = mpatches.Patch(color='steelblue', label='Sigmoid')\nplt.legend(handles=[black_patch, yellow_patch, blue_patch, sigmoid_patch])\nformat_axes(plt.gca())\nplt.title(\"Logistic Regression\")\nplt.savefig(\"../figures/logistic-regression/logistic.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\n\n\nformat_axes(plt.gca())\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\sigma(z)$\")\nplt.savefig(\"../figures/logistic-regression/logistic-function.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-real-output.html",
    "href": "notebooks/decision-tree-real-input-real-output.html",
    "title": "Decision Trees [Real I/P Real O/P, Bias vs Variance]",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\nFIG_WIDTH = 5\nFIG_HEIGHT = 4\n\n\n# Create dataset\nx = np.array([1, 2, 3, 4, 5, 6])\ny = np.array([0, 0, 1, 1, 2, 2])\n\n# plot data\nlatexify(columns=2)\nplt.scatter(x, y, color='k')\nformat_axes(plt.gca()) \nplt.savefig(\"../figures/decision-trees/ri-ro-dataset.pdf\")\n\n\n\n\n\n\n\n\n\n# Depth 0 tree\n# Average of all y values\ny_pred = np.mean(y)\n# Plot data\nlatexify(columns=2)\nplt.scatter(x, y, color='C1', label='data')\n# Plot prediction\nplt.plot([0, 7], [y_pred, y_pred], color='k', linestyle='-', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/ri-ro-depth-0.pdf\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef create_DT_Regressor(x, y, depth, filename):\n    dt = DecisionTreeRegressor(max_depth=depth)\n    dt.fit(x.reshape(-1, 1), y)\n\n    # Plot data\n    latexify(columns=2)\n    plt.scatter(x, y, color='C1', label='Data')\n\n    x_test = np.linspace(0, 7, 500)\n    y_test = dt.predict(x_test.reshape(-1, 1))\n    plt.plot(x_test, y_test, color='k', label='Prediction')\n    format_axes(plt.gca())\n    plt.legend()\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n    return dt\n    \n\n\ndt_one = create_DT_Regressor(x, y, 1, \"ri-ro-depth-1\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\ndef create_graph(dt, filename, feature_names=['x']):\n    dot_data = export_graphviz(dt, out_file=None, feature_names=feature_names, filled=True)\n    graph = graphviz.Source(dot_data)\n    graph.format = 'pdf'\n    graph.render(f\"../figures/decision-trees/{filename}\")\n    return graph\n\n\ncreate_graph(dt_one, \"ri-ro-depth-1-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_two = create_DT_Regressor(x, y, 2, \"ri-ro-depth-2\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_two, \"ri-ro-depth-2-sklearn\")\n\n\n\n\n\n\n\n\nSine Dataset\n\n### Sine daatset\nx = np.linspace(0, 2*np.pi, 200)\ny = np.sin(x)\n\nlatexify(columns=2)\nplt.scatter(x, y, color='k', s=1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/sine-dataset.pdf\")\n\n\n\n\n\n\n\n\n\ndt_sine_one = create_DT_Regressor(x, y, 1, \"sine-depth-1\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_sine_one, \"sine-depth-1-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_sine_four = create_DT_Regressor(x, y, 4, \"sine-depth-4\")\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff - Dataset I\n\n### Dataset for showing bias-variance tradeoff\nX = np.array([[1, 1],[2, 1],[3, 1],[5, 1],\n              [6, 1],[7, 1],[1, 2],[2, 2],\n              [6, 2],[7, 2],[1, 4],[7, 4]])\ny = np.array([0, 0, 0, 1, 1, 1, 0, 1, 0, 1 ,0, 1])\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset.pdf\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\ndef create_DT_Classifier(X,y,depth,filename):\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n\n    # Predict in entire 2d space and contour plot\n    x1 = np.linspace(0, 8, 100)\n    x2 = np.linspace(0, 5, 100)\n\n    X1, X2 = np.meshgrid(x1, x2)\n    X_test = np.stack([X1.flatten(), X2.flatten()], axis=1)\n    y_test = dt.predict(X_test)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.contourf(X1, X2, y_test.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n    format_axes(plt.gca())\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n    return dt\n\n\ndt_bias_variance_one = create_DT_Classifier(X, y, 1, \"bias-variance-depth-1\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_bias_variance_one, \"bias-variance-depth-1-sklearn\", feature_names=['x1', 'x2'])\n\n\n\n\n\n\n\n\n\ndt_bias_variance_full_depth = create_DT_Classifier(X, y, None, \"bias-variance-full-depth\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_bias_variance_full_depth, \"bias-variance-full-depth-sklearn\", feature_names=['x1', 'x2'])\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff - Dataset II\n\n# Bias variance dataset 2\n# X is all integers from (1, 1) to (6, 6)\nX = np.array([[i, j] for i in range(1, 7) for j in range(1, 7)])\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\n\n\n\n\n\n\n\n\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 4)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2.pdf\")\n\n\n\n\n\n\n\n\n\n# X_test random uniform frmo (1, 1) to (6, 6) of size 1000\nX_test = np.random.uniform(1, 6, size=(1000, 2))\ny_test = np.zeros(len(X_test), dtype=int)\ny_test[(2 &lt;= X_test[:, 0]) & (X_test[:, 0] &lt;= 5) & (2 &lt;= X_test[:, 1]) & (X_test[:, 1] &lt;= 5)] = 1\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-test.pdf\")\n\n\n\n\n\n\n\n\n\ndef create_DT_Classifier_with_graph(X,y,depth,filename):\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n\n    # Predict in entire 2d space and contour plot\n    x1 = np.linspace(0.5, 6.5, 100)\n    x2 = np.linspace(0.5, 6.5, 100)\n\n    X1, X2 = np.meshgrid(x1, x2)\n    X_contour = np.stack([X1.flatten(), X2.flatten()], axis=1)\n    y_contour = dt.predict(X_contour)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n    format_axes(plt.gca())\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n\n    # Export tree\n    dot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\n    graph = graphviz.Source(dot_data)\n    graph.format = 'pdf'\n    graph.render(f\"../figures/decision-trees/{filename}-sklearn\")\n\n\n#Underfitting\ncreate_DT_Classifier_with_graph(X, y, 2, \"bias-variance-depth-2\")\n\n\n\n\n\n\n\n\n\n#Overfitting\ncreate_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth\")\n\n\n\n\n\n\n\n\n\n#Good Fit\ncreate_DT_Classifier_with_graph(X, y, 4, \"bias-variance-good-fit\")\n\n\n\n\n\n\n\n\nTest Accuracies\n\nfrom sklearn.metrics import accuracy_score\n### Train and test accuracy vs depth\ndepths = np.arange(2, 10)\ntrain_accs = {}\ntest_accs = {}\nfor depth in depths:\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n    train_accs[depth] = accuracy_score(y, dt.predict(X))\n    test_accs[depth] = accuracy_score(y_test, dt.predict(X_test))\n\n\ntrain_accs = pd.Series(train_accs)\ntest_accs = pd.Series(test_accs)\n\n\ntrain_accs\n\n2    0.722222\n3    0.833333\n4    0.944444\n5    0.944444\n6    0.944444\n7    0.944444\n8    0.944444\n9    0.944444\ndtype: float64\n\n\n\n\n\nax = train_accs.plot(label='Train')\ntest_accs.plot(label='Test', ax=ax)\nplt.xlabel(\"Depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.ylim(0, 1.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth.pdf\")\n\n# Highlight area of underfitting (depth &lt; 4) fill with green \nplt.fill_between(depths, 0, 1, where=depths &lt;= 4, color='g', alpha=0.1, label='Underfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-underfitting.pdf\")\n\n\n# Highlight area of overfitting (depth &gt;7 4) fill with red\nplt.fill_between(depths, 0, 1, where=depths &gt;= 7, color='r', alpha=0.1, label='Overfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-overfitting.pdf\")\n\n\n# Highlight good fit area (4 &lt; depth &lt; 7) fill with blue\nplt.fill_between(depths, 0, 1, where=(depths &gt;= 4) & (depths &lt;= 7), color='b', alpha=0.1, label='Good fit')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-good-fit.pdf\")\n\n\n\n\n\n\n\n\n\n# Slight variation of the dataset leads to a completely different tree\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 3)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-2.pdf\")\n\n\n\n\n\n\n\n\n\ncreate_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth-2\")"
  },
  {
    "objectID": "cnn/tensor-factorisation.html",
    "href": "cnn/tensor-factorisation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import tensorly as tl\n\nUsing numpy backend.\n\n\n\nimport cvxpy as cp\nimport numpy as np\n\n# Ensure repeatably random problem data.\nnp.random.seed(0)\n\n# Generate random data matrix A.\nm = 10\nn = 10\no = 5\nk = 2\nD = 10*np.ones((m, n, o)) + np.random.randn(m, n, o)\n\n# Initialize Y randomly.\nA_init = 10*np.ones((m, k))\nB_init = 10*np.ones((n, k))\nC_init = 10*np.ones((o, k))\n\n\n\n\nPred_A = np.einsum('ir, jr, kr -&gt;ijk', A_init, B_init, C_init)\n\n\n# Ensure same initial random Y, rather than generate new one\n# when executing this cell.\nB = B_init\nC = C_init\n\n# Perform alternating minimization.\nMAX_ITERS = 100\nresidual = np.zeros(MAX_ITERS)\nfor iter_num in range(0, 1+MAX_ITERS):\n\n    if iter_num % 3 == 0:\n        A = cp.Variable(shape=(n, k))\n        constraint = [A &gt;= 0]\n        prediction = A@tl.tenalg.khatri_rao([C, B]).T\n    elif iter_num % 3 == 1:\n        B = cp.Variable(shape=(m, k))\n        constraint = [B &gt;= 0]\n        prediction = B@tl.tenalg.khatri_rao([A, C]).T\n    elif iter_num % 3 == 2:\n        C = cp.Variable(shape=(o, k))\n        constraint = [C &gt;= 0]\n        prediction = C@tl.tenalg.khatri_rao([B, A]).T\n\n    obj = cp.Minimize(cp.norm(D.reshape(prediction.shape) - prediction, 'fro')/D.size)\n    prob = cp.Problem(obj, constraint)\n    prob.solve(solver=cp.SCS, max_iters=10000)\n\n    if prob.status != cp.OPTIMAL:\n        raise Exception(\"Solver did not converge!\")\n\n    print('Iteration {}, residual norm {}'.format(iter_num, prob.value))\n    residual[iter_num-1] = prob.value\n\n    # Convert variable to NumPy array constant for next iteration.\n    if iter_num % 3 == 0:\n        A = A.value\n    elif iter_num%3 == 1:\n        B = B.value\n    else:\n        C = C.value\n\nIteration 0, residual norm 0.044229038768601577\nIteration 1, residual norm 0.04438975125966638\nIteration 2, residual norm 0.04485089174072711\nIteration 3, residual norm 0.0446730384004453\nIteration 4, residual norm 0.044526862069177754\nIteration 5, residual norm 0.04484264445045543\nIteration 6, residual norm 0.044478708695822676\nIteration 7, residual norm 0.04447482085818169\nIteration 8, residual norm 0.045090403949033964\nIteration 9, residual norm 0.04454108258260972\nIteration 10, residual norm 0.04409564845828368\nIteration 11, residual norm 0.04513476609369982\nIteration 12, residual norm 0.04452491993393663\nIteration 13, residual norm 0.04378836637021587\nIteration 14, residual norm 0.045190029913906464\nIteration 15, residual norm 0.04437455947694065\nIteration 16, residual norm 0.04376389953499031\nIteration 17, residual norm 0.04474118548369649\nIteration 18, residual norm 0.04485265629997688\nIteration 19, residual norm 0.04423721109971314\nIteration 20, residual norm 0.04463836313028598\nIteration 21, residual norm 0.04509480034427943\nIteration 22, residual norm 0.04449824189344165\nIteration 23, residual norm 0.0446297404908044\nIteration 24, residual norm 0.044916791718091605\nIteration 25, residual norm 0.04475629136811872\nIteration 26, residual norm 0.044530205009593614\nIteration 27, residual norm 0.04481551774462266\nIteration 28, residual norm 0.04450448414700286\nIteration 29, residual norm 0.04473161074066948\nIteration 30, residual norm 0.044760915844488124\nIteration 31, residual norm 0.04447831568798033\nIteration 32, residual norm 0.04458463344810684\nIteration 33, residual norm 0.04486246216584251\nIteration 34, residual norm 0.04441553594150665\nIteration 35, residual norm 0.04465894246345131\nIteration 36, residual norm 0.04504337082596328\nIteration 37, residual norm 0.044526299849176346\nIteration 38, residual norm 0.04482460772221398\nIteration 39, residual norm 0.04500800584243032\nIteration 40, residual norm 0.04450100433007423\nIteration 41, residual norm 0.04484822280377619\nIteration 42, residual norm 0.04482311584413178\nIteration 43, residual norm 0.044522803791676675\nIteration 44, residual norm 0.044772484551616504\nIteration 45, residual norm 0.04458696318356589\nIteration 46, residual norm 0.044520256516783666\nIteration 47, residual norm 0.04482402095966313\nIteration 48, residual norm 0.04426750617419428\nIteration 49, residual norm 0.044306760469399145\nIteration 50, residual norm 0.04488415609564971\nIteration 51, residual norm 0.04462703244119978\nIteration 52, residual norm 0.04393438972821902\nIteration 53, residual norm 0.04496061133340096\nIteration 54, residual norm 0.04474813896252984\nIteration 55, residual norm 0.04412129577027855\nIteration 56, residual norm 0.04476074674202858\nIteration 57, residual norm 0.04493986609489073\nIteration 58, residual norm 0.04433894515459506\nIteration 59, residual norm 0.04479359793862809\nIteration 60, residual norm 0.04507730499950036\nIteration 61, residual norm 0.04443726027910054\nIteration 62, residual norm 0.04482400811580273\nIteration 63, residual norm 0.044904850352243696\nIteration 64, residual norm 0.04455396547559336\nIteration 65, residual norm 0.04468608722592516\nIteration 66, residual norm 0.04458327810705387\nIteration 67, residual norm 0.0445511237555968\nIteration 68, residual norm 0.044703743776719096\nIteration 69, residual norm 0.04439204969221895\nIteration 70, residual norm 0.04419479288919463\nIteration 71, residual norm 0.04496926843613955\nIteration 72, residual norm 0.04456014579787714\nIteration 73, residual norm 0.04390674892508923\nIteration 74, residual norm 0.04471211816232015\nIteration 75, residual norm 0.04477670162347586\nIteration 76, residual norm 0.044259574198538376\nIteration 77, residual norm 0.044866509022554346\nIteration 78, residual norm 0.04499103987882674\nIteration 79, residual norm 0.04443576055416042\nIteration 80, residual norm 0.04494346910320409\nIteration 81, residual norm 0.04480069447165729\nIteration 82, residual norm 0.044256568121726014\nIteration 83, residual norm 0.044744496252969\nIteration 84, residual norm 0.04448314930970543\nIteration 85, residual norm 0.044155459981671225\nIteration 86, residual norm 0.04521780706935539\nIteration 87, residual norm 0.04446147094598418\nIteration 88, residual norm 0.04404940602099275\nIteration 89, residual norm 0.04516034088743775\nIteration 90, residual norm 0.04457572511910495\nIteration 91, residual norm 0.044197230653923565\nIteration 92, residual norm 0.045118201306481545\nIteration 93, residual norm 0.04421183315228502\nIteration 94, residual norm 0.04375429174860549\nIteration 95, residual norm 0.04441022241202105\nIteration 96, residual norm 0.04401508759046129\nIteration 97, residual norm 0.04393321896924147\nIteration 98, residual norm 0.04435341885307329\nIteration 99, residual norm 0.04411809866902199\nIteration 100, residual norm 0.044273830016112556\n\n\n\nA\n\narray([[ 1.23599056e-01,  4.95799879e-02],\n       [-6.88417340e-12,  4.89807637e-02],\n       [ 1.97771012e-01,  5.00354889e-02],\n       [ 2.97660174e-11,  4.86588823e-02],\n       [ 1.16488475e-01,  4.82973254e-02],\n       [-1.07464945e-11,  4.91216434e-02],\n       [ 2.41321912e-11,  4.81319502e-02],\n       [ 1.23285226e-01,  4.79485790e-02],\n       [ 8.32457971e-12,  4.89433882e-02],\n       [ 1.25287913e-11,  4.91104662e-02]])\n\n\n\nB\n\narray([[ 9.47630862e+00,  1.31432989e+01],\n       [ 2.21280935e+00,  1.29931743e+01],\n       [ 5.84494148e-10,  1.33764365e+01],\n       [ 1.08369019e+01,  1.28286679e+01],\n       [ 5.85242755e+00,  1.28260099e+01],\n       [-2.53650233e-09,  1.30457870e+01],\n       [ 2.49310302e+00,  1.27637902e+01],\n       [-9.13914054e-10,  1.27890647e+01],\n       [ 1.19440017e+00,  1.29894214e+01],\n       [-5.49265493e-10,  1.30485092e+01]])\n\n\n\nC\n\narray([[ 1.52713520e-01,  1.58149369e+01],\n       [ 4.52202894e-01,  1.58153740e+01],\n       [ 5.73957401e-10,  1.56441552e+01],\n       [-1.73226998e-12,  1.54265628e+01],\n       [-1.68743313e-10,  1.57148110e+01]])\n\n\n\nnp.einsum('ir, jr, kr -&gt;ijk', A, B, C)\n\narray([[[10.48458589, 10.83565147, 10.19442928, 10.05263637,\n         10.24047173],\n        [10.22977218, 10.31196443, 10.07798707,  9.93781374,\n         10.12350361],\n        [10.48852243, 10.4888123 , 10.37525943, 10.23095138,\n         10.42211858],\n        [10.26356388, 10.66498719,  9.95038983,  9.81199123,\n          9.99533009],\n        [10.16739655, 10.38431148,  9.94832818,  9.80995826,\n          9.99325913],\n        [10.22925871, 10.22954142, 10.11879543,  9.97805451,\n         10.16449628],\n        [10.05520169, 10.1477645 ,  9.90006825,  9.76236957,\n          9.94478124],\n        [10.02796162, 10.02823877,  9.9196721 ,  9.78170076,\n          9.96447363],\n        [10.20760682, 10.25210095, 10.07507622,  9.93494338,\n         10.12057961],\n        [10.2313932 , 10.23167597, 10.12090687,  9.98013658,\n         10.16661726]],\n\n       [[10.18116321, 10.18144459, 10.07121931,  9.93114011,\n         10.11670529],\n        [10.06487253, 10.0651507 ,  9.95618442,  9.81770523,\n         10.00115085],\n        [10.36175805, 10.36204442, 10.24986394, 10.1073    ,\n         10.29615676],\n        [ 9.93744133,  9.93771598,  9.83012932,  9.69340341,\n          9.87452643],\n        [ 9.93538237,  9.93565696,  9.82809259,  9.69139501,\n          9.87248051],\n        [10.10562779, 10.10590708,  9.99649957,  9.85745964,\n         10.04164808],\n        [ 9.88718524,  9.88745849,  9.78041593,  9.64438148,\n          9.82458851],\n        [ 9.90676358,  9.90703738,  9.79978285,  9.66347903,\n          9.84404291],\n        [10.06196547, 10.06224355,  9.95330875,  9.81486955,\n          9.99826219],\n        [10.10773648, 10.10801583,  9.99858549,  9.85951655,\n         10.04374343]],\n\n       [[10.68660524, 11.24817743, 10.28808747, 10.14499189,\n         10.33455292],\n        [10.34843597, 10.47978553, 10.17057548, 10.02911436,\n         10.2165102 ],\n        [10.58488251, 10.58517505, 10.47057894, 10.32494511,\n         10.5178686 ],\n        [10.47872824, 11.12088193, 10.04180598,  9.90213589,\n         10.08715912],\n        [10.32608231, 10.67300395, 10.0397254 ,  9.90008425,\n         10.08506914],\n        [10.32323689, 10.32352219, 10.21175876, 10.06972482,\n         10.25787948],\n        [10.17538798, 10.3233344 ,  9.99102209,  9.85205835,\n         10.03614586],\n        [10.12009045, 10.12037014, 10.01080605,  9.87156714,\n         10.05601918],\n        [10.314708  , 10.38573678, 10.16763789, 10.02621762,\n         10.21355934],\n        [10.32539099, 10.32567635, 10.2138896 , 10.07182603,\n         10.26001994]],\n\n       [[10.11425681, 10.11453634, 10.00503541,  9.86587676,\n         10.05022247],\n        [ 9.99873034,  9.99900668,  9.89075648,  9.75318732,\n          9.93542741],\n        [10.29366485, 10.29394934, 10.18250606, 10.040879  ,\n         10.22849466],\n        [ 9.87213657,  9.87240941,  9.76552976,  9.62970236,\n          9.80963512],\n        [ 9.87009114,  9.87036392,  9.76350642,  9.62770716,\n          9.80760264],\n        [10.03921777, 10.03949523,  9.9308067 ,  9.79268048,\n          9.97565852],\n        [ 9.82221073,  9.82248219,  9.71614307,  9.58100258,\n          9.76002537],\n        [ 9.84166042,  9.84193242,  9.73538272,  9.59997463,\n          9.77935192],\n        [ 9.99584238,  9.99611864,  9.88789971,  9.75037028,\n          9.93255774],\n        [10.04131261, 10.04159012,  9.93287891,  9.79472388,\n          9.97774009]],\n\n       [[10.20768094, 10.53855895,  9.93069359,  9.79256895,\n          9.97554489],\n        [ 9.96379984, 10.04127254,  9.8172638 ,  9.68071684,\n          9.86160281],\n        [10.21717838, 10.21746076, 10.10684556,  9.96627084,\n         10.15249244],\n        [ 9.99156385, 10.36990232,  9.69296758,  9.55814943,\n          9.73674521],\n        [ 9.90086298, 10.10530773,  9.69095927,  9.55616906,\n          9.73472783],\n        [ 9.96462196,  9.96489736,  9.85701643,  9.71991656,\n          9.90153498],\n        [ 9.7935781 ,  9.88082458,  9.64394785,  9.50981151,\n          9.68750408],\n        [ 9.76853255,  9.76880252,  9.66304454,  9.52864259,\n          9.70668703],\n        [ 9.94281649,  9.98475981,  9.81442826,  9.67792073,\n          9.85875446],\n        [ 9.96670123,  9.96697669,  9.85907325,  9.72194476,\n          9.90360109]],\n\n       [[10.21044653, 10.21072872, 10.1001864 ,  9.95970431,\n         10.14580321],\n        [10.09382137, 10.09410034,  9.98482065,  9.84594316,\n         10.02991642],\n        [10.3915608 , 10.39184799, 10.27934486, 10.13637088,\n         10.32577082],\n        [ 9.96602365,  9.96629909,  9.85840299,  9.72128382,\n          9.9029278 ],\n        [ 9.96395877,  9.96423415,  9.8563604 ,  9.71926965,\n          9.90087599],\n        [10.13469385, 10.13497395, 10.02525176,  9.88581192,\n         10.07053013],\n        [ 9.91562301,  9.91589705,  9.80854661,  9.67212089,\n          9.85284625],\n        [ 9.93525767,  9.93553225,  9.82796924,  9.69127337,\n          9.87235659],\n        [10.09090595, 10.09118483,  9.98193671,  9.84309933,\n         10.02701945],\n        [10.13680861, 10.13708877, 10.02734368,  9.88787475,\n         10.0726315 ]],\n\n       [[10.00472845, 10.00500495,  9.89668982,  9.75903813,\n          9.94138755],\n        [ 9.89045303,  9.89072638,  9.78364843,  9.64756902,\n          9.82783562],\n        [10.18219366, 10.18247507, 10.07223863,  9.93214525,\n         10.11772921],\n        [ 9.76523015,  9.76550004,  9.65977781,  9.5254213 ,\n          9.70340554],\n        [ 9.76320687,  9.7634767 ,  9.65777638,  9.5234477 ,\n          9.70139507],\n        [ 9.93050202,  9.93077647,  9.82326494,  9.68663451,\n          9.86763105],\n        [ 9.71584497,  9.71611349,  9.61092593,  9.47724889,\n          9.65433302],\n        [ 9.73508404,  9.73535309,  9.62995723,  9.49601549,\n          9.67345028],\n        [ 9.88759634,  9.88786961,  9.78082259,  9.64478249,\n          9.82499702],\n        [ 9.93257417,  9.93284868,  9.82531472,  9.68865578,\n          9.86969008]],\n\n       [[10.14502635, 10.4951919 ,  9.85898582,  9.72185855,\n          9.90351326],\n        [ 9.89443406,  9.97640907,  9.74637509,  9.61081411,\n          9.79039394],\n        [10.14340196, 10.1436823 , 10.03386583,  9.89430618,\n         10.0791831 ],\n        [ 9.93205681, 10.33245243,  9.62297639,  9.48913174,\n          9.66643791],\n        [ 9.83619694, 10.05255267,  9.62098258,  9.48716567,\n          9.6644351 ],\n        [ 9.8926692 ,  9.89294261,  9.78584067,  9.64973077,\n          9.83003776],\n        [ 9.7257684 ,  9.81808778,  9.57431062,  9.44114286,\n          9.61755234],\n        [ 9.69799572,  9.69826374,  9.59326942,  9.45983796,\n          9.63659677],\n        [ 9.87241434,  9.91678695,  9.74356002,  9.60803819,\n          9.78756615],\n        [ 9.89473346,  9.89500692,  9.78788264,  9.65174433,\n          9.83208895]],\n\n       [[10.17339432, 10.17367549, 10.06353431,  9.92356201,\n         10.10898558],\n        [10.05719238, 10.05747033,  9.9485872 ,  9.81021368,\n          9.99351932],\n        [10.35385135, 10.3541375 , 10.24204263, 10.09958747,\n         10.28830012],\n        [ 9.92985842,  9.93013285,  9.82262829,  9.68600671,\n          9.86699153],\n        [ 9.92780103,  9.92807541,  9.82059312,  9.68399985,\n          9.86494716],\n        [10.09791654, 10.09819562,  9.98887159,  9.84993776,\n         10.03398565],\n        [ 9.87964067,  9.87991372,  9.77295283,  9.63702219,\n          9.81709171],\n        [ 9.89920408,  9.89947766,  9.79230498,  9.65610517,\n          9.83653126],\n        [10.05428753, 10.0545654 ,  9.94571372,  9.80738017,\n          9.99063287],\n        [10.10002362, 10.10030276,  9.99095592,  9.8519931 ,\n         10.0360794 ]],\n\n       [[10.20812322, 10.20840535, 10.09788818,  9.95743805,\n         10.14349461],\n        [10.0915246 , 10.0918035 ,  9.98254868,  9.84370279,\n         10.02763418],\n        [10.38919627, 10.3894834 , 10.27700587, 10.13406442,\n         10.32342127],\n        [ 9.96375596,  9.96403133,  9.85615978,  9.71907182,\n          9.90067446],\n        [ 9.96169154,  9.96196686,  9.85411766,  9.7170581 ,\n          9.89862311],\n        [10.13238778, 10.13266781, 10.02297059,  9.88356248,\n         10.06823865],\n        [ 9.91336678,  9.91364076,  9.80631475,  9.66992007,\n          9.8506043 ],\n        [ 9.93299697,  9.93327149,  9.82573295,  9.68906819,\n          9.87011021],\n        [10.08860984, 10.08888866,  9.97966539,  9.84085961,\n         10.02473787],\n        [10.13450206, 10.13478215, 10.02506203,  9.88562483,\n         10.07033954]]])"
  },
  {
    "objectID": "cnn/lenet.html",
    "href": "cnn/lenet.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel = keras.Sequential()\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(units=120, activation='relu'))\nmodel.add(layers.Dense(units=84, activation='relu'))\nmodel.add(layers.Dense(units=10, activation = 'softmax'))\n\n\nmodel\n\n&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x7f96fa3cd4e0&gt;\n\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n\n\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n\n\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\nFigure out which ones we are getting wrong\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "cnn/convolution-operation-stride.html",
    "href": "cnn/convolution-operation-stride.html",
    "title": "CIFAR",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_animation(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n    ani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                             frames=zip(i.flat, j.flat), interval=frate)\n    ani.save(fname, writer=\"imagemagick\")\n\n\ncreate_animation(a, kernel, p, s, 'demo.gif', 400)\n\n\n\n\n\n\n\n\n\nfrom keras.datasets import mnist\n\nUsing TensorFlow backend.\n\n\n\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\ncreate_animation(x_train[0], kernel, 0, 1, 'mnist.gif', 2, (20, 4))\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_static(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n     \n    frames=zip(i.flat, j.flat)\n    animate(frames)\n    fig.savefig(fname)\n\n\nfrom keras import backend as K\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel_vertical_edge = keras.Sequential()\nmodel_vertical_edge.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='linear', input_shape=(28, 28, 1)))\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_vertical_edge.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-linear.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[2:3]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-4.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_conv.T.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[0:1]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-5.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[5:6]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-2.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[6:7]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-1.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nCIFAR\n\nfrom keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 91s 1us/step\n\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nmodel_horizontal_edge_relu.layers[0].get_weights()[0].shape\n\n(3, 3, 3, 1)\n\n\n\nfilter_3d_horizontal = np.empty((3, 3, 3))\nfilter_3d_horizontal[:] = filter_conv.T\n\n\nfilter_3d_horizontal\n\narray([[[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]]])\n\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_horizontal.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow(x_train[4])\nplt.title(y_train[4])\nplt.savefig(\"cifar-10-car.pdf\", transparent=True)\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 0], cmap='Reds')\nplt.savefig(\"cifar-10-car-red.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 1], cmap='Greens')\nplt.savefig(\"cifar-10-car-green.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 2], cmap='Blues')\nplt.savefig(\"cifar-10-car-blue.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n\n\n\n\n\n\n\n\nx_train.shape[1:]\n\n(32, 32, 3)\n\n\n\nmodel_horizontal_edge_relu.predict(x_train[4:5]).shape\n\n(1, 30, 30, 1)\n\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nfilter_3d_vertical = np.empty((3, 3, 3))\nfilter_3d_vertical[:] = filter_conv\nfilter_3d_vertical = filter_3d_vertical\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_vertical.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow((filter_3d_horizontal+1)/2)\n\n\n\n\n\n\n\n\n\nplt.imshow(((filter_3d_vertical+1)/2).T)\n\n\n\n\n\n\n\n\n\nfilter_3d_vertical\n\narray([[[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]]])\n\n\n\n(filter_3d_vertical+1)/2\n\narray([[[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]]])\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n\n\n\n\n\n\n\n\nmodel_vertical_edge_relu.layers[0].get_weights()[0][0].shape\n\n(3, 3, 1)\n\n\n\nimport scipy\nimg = x_train[6:7].reshape(32, 32, 3)\nfrom skimage import color\nimg = color.rgb2gray(img)\nsharpen_kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\nimage_sharpen = scipy.signal.convolve2d(img, sharpen_kernel, 'valid')\n\n\nfrom skimage import io\n\n\nbeach = io.imread(\"beach.jpg\")\n\n\nbeach.shape\n\n(1704, 2272, 3)\n\n\n\nbuildings = io.imread(\"buildings.jpg\")\n\n\nbuildings.shape\n\n(1704, 2272, 3)\n\n\n\nplt.imshow(beach)\nplt.axis('OFF')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 0], cmap='Reds')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 1], cmap='Greens')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 2], cmap='Blues')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_red = scipy.signal.convolve2d(beach[:, :, 0], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_red, cmap='Greys')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_green = scipy.signal.convolve2d(beach[:, :, 1], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_green, cmap='Greens')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_blue = scipy.signal.convolve2d(beach[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_blue, cmap='Blues')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nimage_out_buildings_blue = scipy.signal.convolve2d(buildings[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_buildings_blue, cmap='Greys')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nhorizontal_kernel = vertical_kernel.T\nhorizontal_kernel\n\narray([[ 1,  1,  1],\n       [ 0,  0,  0],\n       [-1, -1, -1]])\n\n\n\nimage_out_buildings_blue_horizontal = scipy.signal.convolve2d(buildings[:, :, 2], horizontal_kernel, 'valid')\nplt.imshow(image_out_buildings_blue_horizontal, cmap='Greys')\nplt.axis('off')"
  },
  {
    "objectID": "Lasso/Lasso_Regression.html",
    "href": "Lasso/Lasso_Regression.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom mpl_toolkits import mplot3d\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n%matplotlib inline\n# Based on: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\n\ny_true = 4*x + 7\nmax_deg = 20\n\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.savefig('true_function.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\ndef cost_lasso(theta_0, theta_1, x, y, lamb):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2 + lamb*(abs(theta_0) + abs(theta_1))\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\n\n\n#lambda  = 1000 cost curve tends to lasso objective. \nfig = plt.figure(figsize=(7,7))\nlamb_list = [10,100,1000]\nfor lamb in lamb_list:\n    lasso_cost_matrix = np.zeros_like(x_grid)\n    for i in range(x_grid.shape[0]):\n        for j in range(x_grid.shape[1]):\n            lasso_cost_matrix[i, j] = cost_lasso(x_grid[i, j], y_grid[i, j], data['x'], data['y'],lamb)\n\n\n    ax = plt.axes(projection='3d')\n    ax.plot_surface(x_grid, y_grid, lasso_cost_matrix,cmap='viridis', edgecolor='none')\n\n    ax.set_title('Least squares objective function');\n    ax.set_xlabel(r\"$\\theta_0$\")\n    ax.set_ylabel(r\"$\\theta_1$\")\n    ax.set_xlim([-4,15])\n    ax.set_ylim([-4,15])\n\n    u = np.linspace(0, np.pi, 30)\n    v = np.linspace(0, 2 * np.pi, 30)\n\n    # x = np.outer(500*np.sin(u), np.sin(v))\n    # y = np.outer(500*np.sin(u), np.cos(v))\n    # z = np.outer(500*np.cos(u), np.ones_like(v))\n    # ax.plot_wireframe(x, y, z)\n\n    ax.view_init(45, 120)\n    plt.savefig('lasso_lamb_{}_surface.pdf'.format(lamb), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef yy(p,soln):\n    xx = np.linspace(-soln,soln,100)\n    xx_final = []\n    yy_final = []\n    for x in xx:\n        if(x&gt;0):\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - x**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n            \n        else:\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - (-x)**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n    return xx_final, yy_final\n\n\nfig,ax = plt.subplots()\nax.fill(xx_final,yy_final)\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\nsoln = 3\np  = 0.5\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nxx = np.linspace(-soln,soln,100)\ny1  = yy1(xx,soln,p)\ny2  = yy2(xx,soln,p)\n\nx_final = np.hstack((xx,xx))\ny_final = np.hstack((y1,y2))\n\nxx_final, yy_final = yy(p,soln)\n\nplt.fill(xx_final,yy_final)\n\n\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n  import sys\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n  \"\"\"\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nplt.scatter(xx, y1, s=0.1,color='k',)\nplt.scatter(xx, y2, s=0.1,color='k',)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nregressor.coef_[0] + regressor.coef_[1]\n\n5.74196012460497\n\n\n\n## Function generator. \niterations = 60\np = 4\nq = 4\nalpha  = 0.1\n\nx = np.linspace(-5,5,1000)\ny1 = x**2\ny2 = abs(x)\n\n\nfor i in range(iterations):\n    fig,ax = plt.subplots(1,2)\n    ax[0].plot(x,y1)\n    ax[1].plot(x,y2)\n    prev = p\n    qrev = q\n    p = p - 2*alpha*p\n    q = q - alpha\n    val = p\n    \n    ax[0].arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    ax[1].arrow(qrev,abs(qrev),q - qrev ,abs(q) - abs(qrev),head_width = 0.5)\n    ax[0].scatter([prev],[prev**2],s=100)\n    ax[1].scatter([qrev],abs(qrev),s=100)\n    ax[0].set_xlabel(\"x\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_ylabel(\"Cost\")\n    ax[0].set_ylabel(\"Cost\")\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(0,5)\n    ax[1].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    ax[0].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    if(i==0):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    if(i%10==9):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1,y1= 5**0.5,5\ny2,x2 = 5,5\nx_shift = 0\ny_shift = -0.5\niterations = 11\nfor i in range(iterations):\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n   \n    ax[0].set_ylim(-1,10)\n    ax[0].set_xlim(-5,5)\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(-1,10)\n    ax[0].plot(x,x**2, color = 'blue')\n    ax[1].plot(x,abs(x),color = 'red')\n    ax[0].scatter(x1,y1,color = 'black')\n    \n    ax[0].annotate(str(round(y1,3)), (x1 + x_shift, y1+y_shift))\n    ax[1].annotate(str(y2), (x2 + x_shift, y2 + y_shift))\n    ax[1].scatter(x2,y2,color = 'black')\n    fig.suptitle('Iteration {}'.format(i))\n    if(iteratio)\n    plt.savefig('GD_Iteration_{}.pdf'.format(i))\n    \n    y1 = y1 - alpha*y1\n    y2 = y2 - 0.5\n    x2 = y2\n    x1 = y1**0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nfrom matplotlib.patches import Rectangle\n\n\nfor alpha in np.linspace(1,2,5):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Lasso(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    total = abs(regressor.coef_[0]) + abs(regressor.coef_[1])\n    p1 = Rectangle((-total, 0), np.sqrt(2)*total, np.sqrt(2)*total, angle = -45, alpha=0.6, color='g', label=r'$|\\theta_0|+|\\theta_1|={:.2f}$'.format(total))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Lasso Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n\n    plt.savefig('lasso_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nfrom sklearn.linear_model import Lasso\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e10]):\n        regressor = Lasso(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('lasso_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 659.2329891662157, tolerance: 2.566256097809531\n  positive)\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5017.444529811921, tolerance: 2.566256097809531\n  positive)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"dataset.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-5,1,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Lasso(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nplt.savefig('lasso_reg.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "linear-reg/Linear Regression Notebook.html",
    "href": "linear-reg/Linear Regression Notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This webpage contains the course materials for the course “Machine Learning” that I (Nipun Batra) teaches at Indian Institute of Technology, Gandhinagar. These materials have been developed over several years by me and excellent teaching assistants who have helped me in teaching this course."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "1-introduction-ml.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\naccuracy_convention.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\nbias-variance.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\ncross-validation.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\ndecision-trees.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\nensemble.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\nfind-widths.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\ngradient-descent.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\nlinear-regression.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\nlogistic-1.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\n\n\n\n\n\nml-maths-2-contour.pdf\n\n\n2/3/24, 4:58:03 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bias-variance/Charts.html",
    "href": "bias-variance/Charts.html",
    "title": "Bias New",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.legend()\nplt.savefig('images/true.pdf', transparent=True)\n\n\n\n\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\nplt.savefig('images/data.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n# plt.fill_between(data['x'], y_true-dy, y_true+dy, color='green',alpha=0.2, label='Variance')\nplt.errorbar(data['x'][15], y_true[15], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\n\nplt.legend()\n\n\nplt.savefig('images/data_var.pdf', transparent=True)"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-new",
    "href": "bias-variance/Charts.html#bias-new",
    "title": "Bias New",
    "section": "Bias New",
    "text": "Bias New\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\n\nplt.savefig('images/biasn_1.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.legend()\n\nplt.savefig('images/biasn_2.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.fill_between(x, y_true, [data['y'].mean() for _ in data['x']], color='green',alpha=0.2, label='Bias')\nplt.legend()\n\nplt.savefig('images/biasn_3.pdf', transparent=True)"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-old",
    "href": "bias-variance/Charts.html#bias-old",
    "title": "Bias New",
    "section": "Bias Old",
    "text": "Bias Old\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\n\n\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nplt.savefig('images/bias1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\nax[1].plot(x, [y2.mean() for _ in x], 'r:', label='Prediction')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train4)}$')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias4.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\n\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias5.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(x, y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nfit = np.array([(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x])\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nplt.fill_between(x, y_true, fit, color='green',alpha=0.2, label='Bias')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias6.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nVarying Degree on Bias\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nax[0].plot(x, [y.mean() for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nax[0].plot(data['x'], data['y'], '.b', label='Actual Prices')\nax[0].plot(data['x'], y_true,'g', label='True Function')\nax[0].fill_between(x, y_true, [y.mean() for _ in x], color='green',alpha=0.2, label='Bias')\nax[0].set_title(f\"Degree = 0\")\nfor i,deg in enumerate([1]):\n    i=i+1\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias7.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nfor i,deg in enumerate([2,3]):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias8.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#variance",
    "href": "bias-variance/Charts.html#variance",
    "title": "Bias New",
    "section": "Variance",
    "text": "Variance\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 25\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\n# plt.errorbar(x[::3], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::3], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\nplt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nVaraince Variation\n\nfrom sklearn.linear_model import LinearRegression\n\nfig, ax = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(10, 8))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles = []\n\nfor i,seed in enumerate([2,4,8,16]):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n    \n    ax[int(i/2)][i%2].plot(data_s['x'],data_s['y'], '.b', label='Data Point')\n#     ax[i].plot(data_n['x'],data_n['y'], 'ok', label='UnSelected Points')\n    ax[int(i/2)][i%2].plot(data_s['x'], y_pred,'r-.', label='Prediction')\n    ax[int(i/2)][i%2].plot(data['x'], y_true,'g-', label='True Function')\n#     ax[i].set_title(f\"{deg} : {max(regressor.coef_, key=abs):.2f}\")\n    \n\nhandles, labels = ax[0][0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nmodles = []\n\nfor i,seed in enumerate(range(1,50)):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(8, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles=np.array(modles)\n\n# ax[0].plot(x, modles.mean(axis=0), 'r-.', label='Average Fit')\n# ax[0].plot(data['x'], y_true,'g-', label='True Function')\n# ax[0].set_xlabel('Size (sq.ft)')\n# ax[0].set_ylabel('Price (\\$)')\n\n# ax[1].errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\n# ax[1].plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\n# ax[1].plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\n# ax[1].plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\n# ax[1].plot(data['x'], y_true,'g-', label='True Function')\n# ax[1].set_xlabel('Size (sq.ft)')\n\nax.errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\nax.plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\nax.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\nax.plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\nax.plot(data['x'], y_true,'g-', label='True Function')\nax.set_xlabel('Size (sq.ft)')\n\n# plt.plot(x, modles.mean(axis=0), 'k.-', label=r'Average Fit')\n# plt.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train3)}$')\n# plt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\n# handles, labels = [(a + b) for a, b in zip(ax[0].get_legend_handles_labels(), ax[1].get_legend_handles_labels())]\nhandles, labels = ax.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var4.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-variance-tradeoff",
    "href": "bias-variance/Charts.html#bias-variance-tradeoff",
    "title": "Bias New",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nx = x = np.linspace(0, 4*np.pi, 201)\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 1\np = np.poly1d([1, 2, 3])\ny = np.sin(x) + 0.5*x -  0.05*x**2 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5*x - 0.05*x**2\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\n# plt.xlabel('Size (sq.ft)')\n# plt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\n# plt.ylim(-2,2)\n# plt.xlim(0,4*np.pi)\nplt.plot(data['x'], data['y'], '.', label='Data Points')\nplt.legend()\nplt.savefig('images/bv-1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(10, 5))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,4*np.pi))\n\ndegs = [1,3,7]\nfor i,deg in enumerate(degs):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n    \n#     print(predictors)\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n#     ax[0][i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[0][i].plot(data['x'], y_pred,'-.r', label='Prediction')\n    ax[0][i].plot(data['x'], y_true,'g', label='True Function')\n    ax[0][i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[0][i].set_title(f\"Degree = {deg}\")\n\nfor i,deg in enumerate(degs):    \n    predictors = ['x']\n    models=[]\n    \n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for t,seed in enumerate(range(1,50)):\n        np.random.seed(seed)\n        y_random = np.sin(x) + 0.5*x - 0.05*x**2 + np.random.normal(0,var,len(x))\n        data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n        data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n        data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n\n        predictors = ['x']\n        if deg &gt;= 2:\n            predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n        regressor = LinearRegression(normalize=True)  \n        regressor.fit(data_s[predictors],data_s['y'])\n        y_pred = regressor.predict(data_s[predictors])\n\n        models.append(y_pred)\n    \n    models=np.array(models)\n    ax[1][i].errorbar(x[::7], models.mean(axis=0)[::7], yerr=2*models.std(axis=0)[::7], fmt=':k', capsize=3, label='Variance')\n    ax[1][i].plot(data['x'], y_true,'g-', label='True Function')\n\nhandles, labels = [(a + b) for a, b in zip(ax[0][0].get_legend_handles_labels(), ax[1][0].get_legend_handles_labels())]\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bv-2.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "cnn/convolution-operation.html",
    "href": "cnn/convolution-operation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\n\n\n\n\nfrom scipy.signal import convolve2d\n\n\nconvolve2d(inp, filter_conv, mode='valid')\n\narray([[ 2, -3, -4],\n       [ 4,  8, -9],\n       [ 0, 14, -1]])\n\n\n\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt; from scipy.misc import lena as lena\n\n&gt;&gt;&gt; scharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n...                    [-10+0j, 0+ 0j, +10 +0j],\n...                    [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy\n&gt;&gt;&gt; grad = signal.convolve2d(lena, scharr, boundary='symm', mode='same')\n\nImportError: cannot import name 'lena' from 'scipy.misc' (/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/scipy/misc/__init__.py)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 1\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.array([[ 1,0, -1], [1, 0,-1], [ 1,0, -1]])\nf = kernel.shape[0]\n\npadding = True\n\nif padding:\n    # visualization array (2 bigger in each direction)\n    va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n    va[p:-p,p:-p] = a\n    va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n    va_color[p:-p,p:-p] = 0.5\nelse:\n    va = a\n    va_color = np.zeros_like(a)\n\n#output array\nres = np.zeros((n-f+1+2*p, n-f+1+2*p))\n\n\n\n#####################\n# Create inital plot\n#####################\nfig = plt.figure(figsize=(8,4))\n\ndef add_axes_inches(fig, rect):\n    w,h = fig.get_size_inches()\n    return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\naxwidth = 3.\ncellsize = axwidth/va.shape[1]\naxheight = cellsize*va.shape[0]\n\nax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\nax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                   (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                   kernel.shape[1]*cellsize,  \n                                   kernel.shape[0]*cellsize])\nax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                               2*cellsize, \n                               res.shape[1]*cellsize,  \n                               res.shape[0]*cellsize])\nax_kernel.set_title(\"Kernel\", size=12)\n\nim_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\nax_va.set_title(\"Image size: {}X{}\\n Padding: {}\".format(n, n, p))\nfor i in range(va.shape[0]):\n    for j in range(va.shape[1]):\n        ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\nax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\nfor i in range(kernel.shape[0]):\n    for j in range(kernel.shape[1]):\n        ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\nim_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\nres_texts = []\nfor i in range(res.shape[0]):\n    row = []\n    for j in range(res.shape[1]):\n        row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n    res_texts.append(row)    \n\nax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\nfor ax  in [ax_va, ax_kernel, ax_res]:\n    ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.grid(color=\"k\")\n\n###############\n# Animation\n###############\ndef init():\n    for row in res_texts:\n        for text in row:\n            text.set_text(\"\")\n\ndef animate(ij):\n    i,j=ij\n    o = kernel.shape[1]//2\n    # calculate result\n    \n   \n    res_ij = (kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]).sum()\n    \n    res_texts[i][j].set_text(res_ij)\n    # make colors\n    c = va_color.copy()\n    c[1+i-o:1+i+o+1, 1+j-o:1+j+o+1] = 1.\n    im_va.set_array(c)\n\n    r = res.copy()\n    r[i,j] = 1\n    im_res.set_array(r)\n    \n\n\ni,j = np.indices(res.shape)\nani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                         frames=zip(i.flat, j.flat), interval=5)\nani.save(\"algo.gif\", writer=\"imagemagick\")\n\n\n\n\n\n\n\n\n\nva\n\narray([[0, 2, 2, 2, 0],\n       [4, 3, 0, 2, 2],\n       [1, 3, 3, 4, 2],\n       [3, 0, 0, 0, 2],\n       [0, 3, 4, 2, 3]])\n\n\n\ni\n\narray([[0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4]])\n\n\n\ni = 0\nj = 3\no =kernel.shape[1]//2\n(kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1])\n\nValueError: operands could not be broadcast together with shapes (3,3) (3,2) \n\n\n\n(kernel)\n\narray([[ 1,  0, -1],\n       [ 1,  0, -1],\n       [ 1,  0, -1]])\n\n\n\nva[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]\n\narray([[3, 2],\n       [2, 1],\n       [3, 0]])"
  },
  {
    "objectID": "cnn/visualisation.html",
    "href": "cnn/visualisation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n%matplotlib inline\n\n\nsomeX, someY = 0.5, 0.5\nfig,ax = plt.subplots()\nax.set_aspect(\"equal\")\nax.add_patch(patches.Rectangle((0.5, 0.5), 0.1, 0.1,\n                      alpha=1, facecolor='gray'))\n\n\n\n\n\n\n\n\n\nimport sys\nsys.path.append(\"convnet-drawer/\")\n\n\nfrom convnet_drawer import Model, Conv2D, MaxPooling2D, Flatten, Dense\nfrom matplotlib_util import save_model_to_file\nchannel_scale = 1/5\n\nmodel = Model(input_shape=(32, 32, 1))\nmodel.add(Conv2D(6, (3, 3), (1, 1)))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(256, (5, 5), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(256, (3, 3), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(4096))\nmodel.add(Dense(4096))\nmodel.add(Dense(1000))\n\n# save as svg file\nmodel.save_fig(\"example.svg\")\n\n\n\n# save via matplotlib\nsave_model_to_file(model, \"example.pdf\")\n\n\n\n\n\n\n\n\nmodel\n\n##### from mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef cuboid_data(o, size=(1,1,1)):\n    # code taken from\n    # https://stackoverflow.com/a/35978146/4124317\n    # suppose axis direction: x: to left; y: to inside; z: to upper\n    # get the length, width, and height\n    l, w, h = size\n    x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  \n    y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1], o[1], o[1]],          \n         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]   \n    z = [[o[2], o[2], o[2], o[2], o[2]],                       \n         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],   \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]],               \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]]]               \n    return np.array(x), np.array(y), np.array(z)\n\ndef plotCubeAt(pos=(0,0,0), size=(1,1,1), ax=None,**kwargs):\n    # Plotting a cube element at position pos\n    if ax !=None:\n        X, Y, Z = cuboid_data( pos, size )\n        ax.plot_surface(X, Y, Z, rstride=1, cstride=1, **kwargs)\n\nsizes = [(32,32,1), (28, 28, 6), (14, 14, 6), (10, 10, 16), (5, 5, 16), (1, 120, 1)]\npositions = [(0, 0, 0)]*len(sizes)\nfor i in range(1, len(sizes)):\n    positions[i] = (positions[i-1][0] + sizes[i-1][0]+10, 0, 0)\ncolors = [\"grey\"]*len(sizes)\n\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.view_init(84, -90)\nax.set_aspect('equal')\nax.set_axis_off()\nax.set_xlabel('X')\nax.set_xlim(-5, positions[-1][0]+10)\nax.set_ylabel('Y')\nax.set_ylim(-1, 130)\nax.set_zlabel('Z')\nax.set_zlim(-1, 5)\n#ax.set_visible(False)\nfor p,s,c in zip(positions,sizes,colors):\n    plotCubeAt(pos=p, size=s, ax=ax, color=c)\nax.w_zaxis.line.set_lw(0.)\nax.set_zticks([])\n\nfor i in range(len(positions)):\n    ax.text(positions[i][0], -5, 0, \"X\".join(str(x) for x in sizes[i]), color='black', fontsize=4)\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1)\nfig.tight_layout()\nplt.tight_layout()\nplt.savefig(\"lenet.pdf\", bbox_inches=\"tight\", transparent=True, dpi=600)"
  },
  {
    "objectID": "cnn/vgg-minst.html",
    "href": "cnn/vgg-minst.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nmodel = VGG16()\n\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 362s 1us/step\n\n\n\nmodel.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n\n\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n\n\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\nFigure out which ones we are getting wrong\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "notebooks/geometric-linear-regression.html",
    "href": "notebooks/geometric-linear-regression.html",
    "title": "Geometric interpretation of Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\n\n\nfrom latexify import latexify, format_axes\nlatexify(columns=2)\n\n\n\n\n# Define points\nA = (1, 3)\nB = (2, 1)\nOrigin = (0, 0)\n\n# Plot vectors\nplt.quiver(*Origin, *A, angles='xy', scale_units='xy', scale=1, color='b', label='$v_1$')\nplt.quiver(*Origin, *B, angles='xy', scale_units='xy', scale=1, color='r', label='$v_2$')\n\n# Set axis limits\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\n\n# Add legend\nplt.legend()\n\n# Show plot\nplt.grid(alpha=0.1)\n\nax = plt.gca()\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-1.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Now, create v3 = v1 + v2 and v4 = v1 - v2 and plot\nC = (A[0] + B[0], A[1] + B[1])\nD = (A[0] - B[0], A[1] - B[1])\n\n# Set axis limits\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\n\nplt.quiver(*Origin, *A, angles='xy', scale_units='xy', scale=1, color='b', label='$v_1$')\nplt.quiver(*Origin, *B, angles='xy', scale_units='xy', scale=1, color='r', label='$v_2$')\nplt.quiver(*Origin, *C, angles='xy', scale_units='xy', scale=1, color='g', label='$v_3$')\nplt.quiver(*Origin, *D, angles='xy', scale_units='xy', scale=1, color='y', label='$v_4$')\n\nax = plt.gca()\nformat_axes(ax)\nplt.legend()\n\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-2.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nA_arr = np.array(A)\nB_arr = np.array(B)\n\nAB_matrix = np.zeros((2, 2))\n# First column is A, second column is B\nAB_matrix[:, 0] = A_arr\nAB_matrix[:, 1] = B_arr\n\nprint(AB_matrix)\n\n[[1. 2.]\n [3. 1.]]\n\n\n\ndef new_vector(AB_matrix, alpha):\n    return AB_matrix @ alpha\n\nprint(new_vector(AB_matrix, np.array([1, -1])))\n\n[-1.  2.]\n\n\n\n# Generate a bunch of alphas\nalphas = np.random.uniform(-3, 3, size=(30000, 2))\nnew_vecs = []\nfor i, alpha in enumerate(alphas):\n    new_vecs.append(new_vector(AB_matrix, alpha))\n\nnew_vecs = np.array(new_vecs)\n\n\nt = new_vecs\nplt.scatter(t[:, 0], t[:, 1], alpha=0.2)\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nax = plt.gca()\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-3.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nA = np.array([1, 2])\nB = np.array([2, 4])\n\nAB_matrix = np.zeros((2, 2))\n# First column is A, second column is B\nAB_matrix[:, 0] = A\nAB_matrix[:, 1] = B\n\nprint(AB_matrix)\n\n[[1. 2.]\n [2. 4.]]\n\n\n\n# Generate a bunch of alphas\nalphas = np.random.uniform(-3, 3, size=(10000, 2))\nnew_vecs = []\nfor i, alpha in enumerate(alphas):\n    new_vecs.append(new_vector(AB_matrix, alpha))\n\nnew_vecs = np.array(new_vecs)\n\nplt.scatter(new_vecs[:, 0], new_vecs[:, 1], alpha=0.2, s=2)\nax = plt.gca()\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-4.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a figure and 3D axis\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define the surface plot\nX = np.linspace(0, 2.2, 100)\nY = np.linspace(-2.2, 1, 100)\nX, Y = np.meshgrid(X, Y)\nZ = X\n\n# Plot the surface\nsurf = ax.plot_surface(X, Y, Z,  alpha=0.3, rstride=100, cstride=100)\n\n# Define points\nA = np.array([1, 1, 1])\nD = np.array([0, 0, 0])\nB = np.array([2, -2, 2])\n\n# Mark the origin\nax.scatter(*D, color='black', label='Origin')\n\n# Plot vectors with labels including the vector\nax.quiver(D[0], D[1], D[2], A[0], A[1], A[2], color='b', label=f'$X_1 = {A.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], B[0], B[1], B[2], color='r', label=f'$X_2 = {B.tolist()}$', arrow_length_ratio=0.1)\n\n# Set axis labels\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_zlabel('$z$', fontsize=12)\n\n# Set legend\nax.legend()\n\n# Adjust view angle\nax.view_init(elev=15, azim=-35)\n\n# Customize grid lines\nax.grid(linestyle='dashed', color='white', alpha=0.2)  # Adjust color here\n\n\nplt.savefig(\"../figures/linear-regression/geometric-1.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\n\n# Create a figure and 3D axis\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define the surface plot\nX = np.linspace(0, 10.2, 300)\nY = np.linspace(-5, 5, 300)\nX, Y = np.meshgrid(X, Y)\nZ = X\n\n# Plot the surface\nsurf = ax.plot_surface(X, Y, Z, alpha=0.3, rstride=100, cstride=100,)\n\n# Define points\nA = np.array([1, 1, 1])\nD = np.array([0, 0, 0])\nB = np.array([2, -2, 2])\ny_vec = np.array([8.8957, 0.6130, 1.7761])\n\n# Mark the origin\nax.scatter(*D, color='black', label='Origin')\n\n# Plot vectors with labels including the vector\nax.quiver(D[0], D[1], D[2], A[0], A[1], A[2], color='b', label=f'$X_1 = {A.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], B[0], B[1], B[2], color='r', label=f'$X_2 = {B.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], y_vec[0], y_vec[1], y_vec[2], color='g', label=f'$y = {y_vec.tolist()}$', arrow_length_ratio=0.1)\n\n# Set axis labels\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_zlabel('$z$', fontsize=12)\n\n# Set legend\nax.legend()\n\n# Adjust view angle\nax.view_init(elev=15, azim=-35)\n\n# Customize grid lines\nax.grid(linestyle='dashed', color='white', alpha=0.2)  # Adjust color here\n\n\nplt.savefig(\"../figures/linear-regression/geometric-2.pdf\", bbox_inches=\"tight\")\n\n\nX_matrix = np.zeros((3, 2))\nX_matrix[:, 0] = A\nX_matrix[:, 1] = B\n\nprint(X_matrix)\n\ntheta_hat = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y_vec\n\nprint(theta_hat)\n\ny_hat = X_matrix @ theta_hat\nprint(y_hat)\n\n\n# Plot y_hat vector\nax.quiver(D[0], D[1], D[2], y_hat[0], y_hat[1], y_hat[2], color='y', label=f'$\\hat y = {list(map(lambda x: round(x, 4), y_hat))}$', arrow_length_ratio=0.1)\nplt.legend()\nplt.savefig(\"../figures/linear-regression/geometric-3.pdf\", bbox_inches=\"tight\")\n\n\n# perpendiculat vector\nperp_vec = y_vec - y_hat\n# Plot perp vector with y_hat as origin\nax.quiver(y_hat[0], y_hat[1], y_hat[2], perp_vec[0], perp_vec[1], perp_vec[2], color='m', label=f'$y - \\hat y = {list(map(lambda x: round(x, 4), perp_vec))}$', arrow_length_ratio=0.1)\nplt.legend()\nplt.savefig(\"../figures/linear-regression/geometric-4.pdf\", bbox_inches=\"tight\")\n\n[[ 1.  2.]\n [ 1. -2.]\n [ 1.  2.]]\n[2.97445  1.180725]\n[5.3359 0.613  5.3359]\n\n\n\n\n\n\n\n\n\n\nperp_vec\n\narray([ 3.5598,  0.    , -3.5598])\n\n\n\nX_matrix[:, 0]\n\narray([1., 1., 1.])\n\n\n\n\nperp_vec@X_matrix[:, 0]\n\n-1.3322676295501878e-15\n\n\n\nperp_vec@X_matrix[:, 1]\n\n-2.6645352591003757e-15\n\n\n\nX_matrix.T @ perp_vec\n\narray([-1.33226763e-15, -2.66453526e-15])"
  },
  {
    "objectID": "notebooks/rule-based-vs-ml.html",
    "href": "notebooks/rule-based-vs-ml.html",
    "title": "Traditional Programming vs Machine Learning",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom latexify import latexify\nimport seaborn as sns\n%matplotlib inline\n# config retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n# Load MNIST dataset\nmnist_train = torchvision.datasets.MNIST('../datasets', train=True, transform=torchvision.transforms.ToTensor(), download=True)\nmnist_test = torchvision.datasets.MNIST('../datasets', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n\n\n\n# Function to show a digit marking 28x28 grid with arrows pointing to random pixels\ndef show_digit_with_arrows(digit, label=None):\n    fig, ax = plt.subplots()\n    digit = digit.numpy().reshape(28, 28)\n\n    # Display the digit\n    ax.imshow(digit, cmap='gray')\n\n    # Add gridlines corresponding to 28 rows and columns\n    for i in range(1, 28):\n        ax.axhline(i, color='white', linewidth=0.5)\n        ax.axvline(i, color='white', linewidth=0.5)\n\n    # Display label if available\n    if label is not None:\n        ax.set_title(f'Label: {label}')\n    return fig, ax\n\n\nindex = 2\n# Show a random digit with arrows pointing to random 10 pixels\nfig, ax = show_digit_with_arrows(*mnist_train[index])\n# save figure\nfig.savefig(\"../figures/mnist.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Find indices of digit 4 in the training set\ndigit_4_indices_train = torch.where(torch.tensor(mnist_train.targets) == 4)[0]\ndigit_4_indices_test = torch.where(torch.tensor(mnist_test.targets) == 4)[0]\n\nprint(f\"Indices of digit 4 in Train dataset: {digit_4_indices_train}\")\nprint(f\"Number of digit 4 images in training set: {len(digit_4_indices_train)}\\n\")\n\nIndices of digit 4 in Train dataset: tensor([    2,     9,    20,  ..., 59943, 59951, 59975])\nNumber of digit 4 images in training set: 5842\n\n\n\n/tmp/ipykernel_1361527/214778730.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  digit_4_indices_train = torch.where(torch.tensor(mnist_train.targets) == 4)[0]\n/tmp/ipykernel_1361527/214778730.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  digit_4_indices_test = torch.where(torch.tensor(mnist_test.targets) == 4)[0]\n\n\n\nlatexify(fig_width=7, fig_height=5)\n\nfor i in range(15):\n    plt.subplot(3, 5, i+1)\n    plt.imshow(mnist_train.data[digit_4_indices_train[i]], cmap='gray')\n    plt.title(f\"idx: {digit_4_indices_train[i]}\")\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\n# Select a sample from the training set\nsample_idx_1 = 60\nimage, label = mnist_train[sample_idx_1]\nplt.imshow(image.squeeze().numpy(), cmap='gray')\nplt.title(f\"Label: {label}\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Function to extract edges based on intensity threshold\ndef extract_edges(image, threshold=0.1):\n    '''\n    Input:\n        image: torch.tensor of shape (28, 28)\n        threshold: (float) the minimum intensity value to be considered as white pixel\n    '''\n    edges = torch.zeros_like(image)\n\n    # converting all the pixels with intensity greater than threshold to white\n    edges[image &gt; threshold] = 1.0\n    return edges\n\n\n# Creating rules based upon one image\nedges = extract_edges(image)\n\nplt.imshow(edges[0, :, :], cmap='gray')\n\n# finding areas of edges\nleft_edge_train = edges[:, 4:15, 3:12]\nupper_right_edge_train = edges[:, 4:19, 17:24]\nmiddle_edge_train = edges[:, 14:20, 5:25]\nlower_right_edge_train = edges[:, 17:24, 18:24]\n\n\n# R1 (4-15, 3-12)\nr1 = plt.Rectangle((3, 4), 9, 11, linewidth=1, edgecolor='r', facecolor='none')\nr2 = plt.Rectangle((17, 4), 7, 15, linewidth=1, edgecolor='g', facecolor='none')\nr3 = plt.Rectangle((5, 14), 20, 6, linewidth=1, edgecolor='b', facecolor='none')\nr4 = plt.Rectangle((18, 17), 6, 7, linewidth=1, edgecolor='y', facecolor='none')\nfor rect in [r1, r2, r3, r4]:\n    plt.gca().add_patch(rect)\n\n\n\n\n\n\n\n\n\n\n# creat a subplot 2 rows by 2 columns\nfig, axs = plt.subplots(2, 2, figsize=(8, 10))\n\n# plotting the images\naxs[0, 0].imshow(left_edge_train.squeeze().numpy(), cmap='gray')\naxs[0, 1].imshow(upper_right_edge_train.squeeze().numpy(), cmap='gray')\naxs[1, 0].imshow(middle_edge_train.squeeze().numpy(), cmap='gray')\naxs[1, 1].imshow(lower_right_edge_train.squeeze().numpy(), cmap='gray')\n\naxs[0, 0].set_title(f\"Left Edge\\nWhite pixels: {int(left_edge_train.sum())}/{left_edge_train.numel()}\")\naxs[0, 1].set_title(f\"Upper Right Edge\\nWhite pixels: {int(upper_right_edge_train.sum())}/{upper_right_edge_train.numel()}\")\naxs[1, 0].set_title(f\"Middle Edge\\nWhite pixels: {int(middle_edge_train.sum())}/{middle_edge_train.numel()}\")\naxs[1, 1].set_title(f\"Lower Right Edge\\nWhite pixels: {int(lower_right_edge_train.sum())}/{lower_right_edge_train.numel()}\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rule-based digit classifier for digit 4\ndef rule_based_classifier(image):\n    # Extract edges\n    edges = extract_edges(image)\n\n    # Define rules for digit 4 based on the edges of the digit\n    left_edge = edges[:, 4:15, 3:12]\n    upper_right_edge = edges[:, 4:19, 17:24]\n    middle_edge = edges[:, 14:20, 5:25]\n    lower_right_edge = edges[:, 17:24, 18:24]\n\n    # Check if all required edges are present by checking the number of white pixels for each edge.\n    # The number of white pixels for each edge is 'sub' less than the number of pixels in the edge for the above take digit.\n    sub = 10\n    if torch.sum(left_edge) &gt; left_edge_train.sum() - sub and torch.sum(upper_right_edge) &gt; upper_right_edge_train.sum() - sub and torch.sum(middle_edge) &gt; middle_edge_train.sum() - sub and torch.sum(lower_right_edge) &gt; lower_right_edge_train.sum() - sub:\n        return 4\n    else:\n        return -1 # -1 indicates that the digit is not 4\n\n\n# Display some wrongly classified images\n\nindices = [6, 19, 25, 200]\n# define image size\nplt.figure(figsize=(14, 3))\n\nfor i in range(4):\n    plt.subplot(1, 4, i+1)\n    image, label = mnist_test[indices[i]]\n    pred = rule_based_classifier(image)\n    pred = pred if pred != -1 else \"Not 4\"\n    plt.title(f\"Label: {label}, Predicted: {pred}\")\n    plt.imshow(image.squeeze().numpy(), cmap='gray')\n\n\n\n\n\n\n\n\n\n# Evaluating the rule-based classifier\ncount = 0\ncount_4 = 0\nfor i, (image, label) in enumerate(mnist_test):\n    classification = rule_based_classifier(image)\n    if (classification == 4 and label == 4) or (classification == -1 and label != 4):\n        count += 1\n    if (classification == 4 and label == 4):\n        count_4 += 1\n\naccuracy_rule = count * 100/ len(mnist_test)\npercentage_TP_rule = count_4 * 100/ len(digit_4_indices_test)\nprint(f\"Accuracy of the rule-based classifier: {accuracy_rule} %\")\nprint(f\"Percentage of 4s actually classified as 4 (percentage of True Positives): {percentage_TP_rule:.3} %\")\n\nAccuracy of the rule-based classifier: 88.56 %\nPercentage of 4s actually classified as 4 (percentage of True Positives): 4.28 %\n\n\nNote: As per rules, it is predicting most of the digits as non-4 for most of the digits. And since the number of non-4 digits are much more compared to number of instances of the digit 4, the accuracy is high. But this is not a good model as it is not predicting the digit 4 correctly.\n\nML based approach\n\n# Flatten the images and convert the labels to 4 and -1 for binary classification problem\nX_train = mnist_train.data.numpy().reshape((len(mnist_train), -1))\ny_train = np.where(mnist_train.targets.numpy() == 4, 4, -1)\n\nX_test = mnist_test.data.numpy().reshape((len(mnist_test), -1))\ny_test = np.where(mnist_test.targets.numpy() == 4, 4, -1)\n\n\n# Create and train the MLP model\nmlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\nmlp_model.fit(X_train, y_train)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\nMLPClassifier(max_iter=20, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier(max_iter=20, random_state=42)\n\n\n\n# Evaluate the model\ny_pred = mlp_model.predict(X_test)\naccuracy_ML = accuracy_score(y_test,( y_pred))\naccuracy_ML = accuracy_ML * 100\npercentage_TP_ML = np.sum((y_test == 4) & (y_pred == 4)) * 100 / len(digit_4_indices_test)\nprint(f'Test Accuracy: {accuracy_ML:.2f}%')\nprint(f\"Percentage of 4s actually classified as 4 (percentage of True Positives): {percentage_TP_ML:.3} %\")\n\nTest Accuracy: 99.47%\nPercentage of 4s actually classified as 4 (percentage of True Positives): 97.1 %\n\n\n\n\nComparison of Rule-based system and ML based system\n\n# Categories for the bar plot\ncategories = ['Accuracy', 'True Positive Percentage']\n\n# Values for the rule-based classifier\nrule_based_values = [accuracy_rule, percentage_TP_rule]\n\n# Values for the MLP classifier\nmlp_values = [accuracy_ML, percentage_TP_ML]\n\n# Bar width\nbar_width = 0.35\n\n# X-axis positions for the bars\nindex = range(len(categories))\n\n# Plotting the bar plot\nfig, ax = plt.subplots(figsize=(9, 5))\nbar1 = ax.bar(index, rule_based_values, bar_width, label='Rule-Based Classifier')\nbar2 = ax.bar([i + bar_width for i in index], mlp_values, bar_width, label='MLP Classifier')\n\n# Adding labels, title, and legend\nax.set_xlabel('Metrics')\nax.set_ylabel('Percentage / Accuracy')\nax.set_title('Comparison of Classifiers')\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(categories)\nax.legend()\n\n# Display the values on top of the bars\nfor bar in bar1 + bar2:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "notebooks/cross-validation-diagrams.html",
    "href": "notebooks/cross-validation-diagrams.html",
    "title": "Cross Validation Diagrams",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nimport matplotlib.pyplot as plt\n\n# Define the train/test split percentages\ntrain_percentage = 0.7\ntest_percentage = 1 - train_percentage\n\n# Create a rectangular plot to represent the train/test split\ntrain_rectangle = plt.Rectangle((0, 0), train_percentage, 1, fill=True, color='lightgreen', label='Train Set')\ntest_rectangle = plt.Rectangle((train_percentage, 0), test_percentage, 1, fill=True, color='lightcoral', label='Test Set')\n\n# Add rectangles to the plot\nplt.gca().add_patch(train_rectangle)\nplt.gca().add_patch(test_rectangle)\n\n# Set labels and legend\nplt.xlabel('Data Split')\nplt.ylabel('Data Points')\nplt.title('Train/Test Split Illustration')\nplt.legend()\n\n# Remove x and y ticks\nplt.xticks([])\nplt.yticks([])\n\n\n\n\n\n\n\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/hyperparams-experiments.html",
    "href": "notebooks/hyperparams-experiments.html",
    "title": "Hyperparams Tuning Strategies Experimentation",
    "section": "",
    "text": "import numpy as np \nnp.random.seed(20)\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\n\nMakeMoons Dataset\n1.1 Fixed Train-Test (70:30) split ; No Tuning\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n# Split the data into training, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\nlatexify(fig_width=5, fig_height=4)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label='Train') \nformat_axes(plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\n\n#hyperparameters take their default values\ndt_classifier = DecisionTreeClassifier(random_state=42)\ndt_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ntest_accuracy = dt_classifier.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTest set accuracy: 0.8833\n\n\n1.2 Multiple Random Train-Test splits\n\n# Initialize an empty list to store the accuracy metrics\naccuracy_metrics = []\nall_test_sets = []\nall_predictions = []\n\n# Perform 20 random train-test splits and repeat the fit\nfor _ in range(20):\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=np.random.randint(100))\n    \n    # Create and fit the decision tree classifier\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(X_train, y_train)\n\n    current_predictions = dt_classifier.predict(X_test)\n    all_predictions.append(current_predictions)\n    current_accuracy = np.mean(current_predictions == y_test)\n    all_test_sets.append(y_test)\n    \n    # Calculate the accuracy on the test set\n    test_accuracy = dt_classifier.score(X_test, y_test)\n    \n    # Append the accuracy to the list\n    accuracy_metrics.append(test_accuracy)\n\n# Calculate the mean and standard deviation of the accuracy metrics\nmean_accuracy = np.mean(accuracy_metrics)\nstd_accuracy = np.std(accuracy_metrics)\n\n# Print the mean and standard deviation\nprint(\"Mean accuracy: {:.4f}\".format(mean_accuracy))\nprint(\"Standard deviation: {:.4f}\".format(std_accuracy))\n\n# Print minimum and maximum accuracies\nprint(\"Minimum accuracy: {:.4f}\".format(min(accuracy_metrics)))\nprint(\"Maximum accuracy: {:.4f}\".format(max(accuracy_metrics)))\n\nMean accuracy: 0.8932\nStandard deviation: 0.0165\nMinimum accuracy: 0.8633\nMaximum accuracy: 0.9133\n\n\n1.3 K-Fold Cross Validation\n\nimport numpy as np\n# Define the number of folds (k)\nk = 5\n\n# Initialize lists to store predictions and accuracies\npredictions = {}\naccuracies = []\n\n# Calculate the size of each fold\nfold_size = len(X) // k\n\n# Perform k-fold cross-validation\nfor i in range(k):\n    # Split the data into training and test sets\n    test_start = i * fold_size\n    test_end = (i + 1) * fold_size\n    test_set = X[test_start:test_end]\n    test_labels = y[test_start:test_end]\n    \n    training_set = np.concatenate((X[:test_start], X[test_end:]), axis=0)\n    training_labels = np.concatenate((y[:test_start], y[test_end:]), axis=0)\n    \n    # Train the model\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(training_set, training_labels)\n    \n    # Make predictions on the validation set\n    fold_predictions = dt_classifier.predict(test_set)\n    \n    # Calculate the accuracy of the fold\n    fold_accuracy = np.mean(fold_predictions == test_labels)\n    \n    # Store the predictions and accuracy of the fold\n    predictions[i] = fold_predictions\n    accuracies.append(fold_accuracy)\n\n# Print the predictions and accuracies of each fold\nfor i in range(k):\n    print(\"Fold {}: Accuracy: {:.4f}\".format(i+1, accuracies[i]))\n\nFold 1: Accuracy: 0.8700\nFold 2: Accuracy: 0.8850\nFold 3: Accuracy: 0.9300\nFold 4: Accuracy: 0.8650\nFold 5: Accuracy: 0.8850\n\n\n\nfrom sklearn.model_selection import KFold\n\n# Define the number of folds (k)\nk = 5\n\n# Initialize lists to store predictions and accuracies\npredictions = {}\naccuracies = []\n\n# Create a KFold instance\nkf = KFold(n_splits=k, shuffle=False)\n\n# Perform k-fold cross-validation\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    # Split the data into training and test sets\n    training_set, test_set = X[train_index], X[test_index]\n    training_labels, test_labels = y[train_index], y[test_index]\n    \n    # Train the model\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(training_set, training_labels)\n    \n    # Make predictions on the validation set\n    fold_predictions = dt_classifier.predict(test_set)\n    \n    # Calculate the accuracy of the fold\n    fold_accuracy = np.mean(fold_predictions == test_labels)\n    \n    # Store the predictions and accuracy of the fold\n    predictions[i] = fold_predictions\n    accuracies.append(fold_accuracy)\n\n    # Print the predictions and accuracy of each fold\n    print(\"Fold {}: Accuracy: {:.4f}\".format(i+1, fold_accuracy))\n\nFold 1: Accuracy: 0.8700\nFold 2: Accuracy: 0.8850\nFold 3: Accuracy: 0.9300\nFold 4: Accuracy: 0.8650\nFold 5: Accuracy: 0.8850\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Method 1 for computing accuracy\naccuracy_1 = accuracy_score(y, np.concatenate(list(predictions.values())))\n\n# Calculate macro-averaged accuracy\naccuracy_2 = np.mean(accuracies)\n\n# Print the micro and macro averaged accuracy\nprint(\"Method 1 accuracy: {:.4f}\".format(accuracy_1))\nprint(\"Method2 accuracy: {:.4f}\".format(accuracy_2))\n\nMethod 1 accuracy: 0.8870\nMethod2 accuracy: 0.8870\n\n\n2.1 Fixed Train-Test Split (hyperparameters tuned on Validation set)\n2.1.1 Validation Set as fixed Subset of Training Set\n\n# Step 1: Split the data into training, validation, and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=42)\n\n\nprint(\"Number of training examples: {}\".format(len(X_train)))\nprint(\"Number of validation examples: {}\".format(len(X_val)))\nprint(\"Number of testing examples: {}\".format(len(X_test)))\n\nNumber of training examples: 500\nNumber of validation examples: 200\nNumber of testing examples: 300\n\n\n\nhyperparameters = {}\nhyperparameters['max_depth'] = [1,2,3,4,5,6,7,8,9,10]\nhyperparameters['min_samples_split'] = [2,3,4,5,6,7,8]\nhyperparameters['criteria_values'] = ['gini', 'entropy']\n\nbest_accuracy = 0\nbest_hyperparameters = {}\n\nout = {}\ncount = 0\nfor max_depth in hyperparameters['max_depth']:\n    for min_samples_split in hyperparameters['min_samples_split']:\n        for criterion in hyperparameters['criteria_values']:\n            # Create and fit the decision tree classifier with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate the performance on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            out[count] = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'criterion': criterion, 'val_accuracy': val_accuracy}\n            count += 1\n\n\nhparam_df = pd.DataFrame(out).T\nhparam_df\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n1\n2\ngini\n0.785\n\n\n1\n1\n2\nentropy\n0.785\n\n\n2\n1\n3\ngini\n0.785\n\n\n3\n1\n3\nentropy\n0.785\n\n\n4\n1\n4\ngini\n0.785\n\n\n...\n...\n...\n...\n...\n\n\n135\n10\n6\nentropy\n0.895\n\n\n136\n10\n7\ngini\n0.89\n\n\n137\n10\n7\nentropy\n0.895\n\n\n138\n10\n8\ngini\n0.885\n\n\n139\n10\n8\nentropy\n0.895\n\n\n\n\n140 rows × 4 columns\n\n\n\n\nhparam_df.sort_values(by='val_accuracy', ascending=False).head(10)\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n76\n6\n5\ngini\n0.925\n\n\n77\n6\n5\nentropy\n0.925\n\n\n78\n6\n6\ngini\n0.925\n\n\n79\n6\n6\nentropy\n0.925\n\n\n80\n6\n7\ngini\n0.925\n\n\n81\n6\n7\nentropy\n0.925\n\n\n83\n6\n8\nentropy\n0.925\n\n\n70\n6\n2\ngini\n0.92\n\n\n82\n6\n8\ngini\n0.92\n\n\n90\n7\n5\ngini\n0.915\n\n\n\n\n\n\n\n\nbest_hyperparameters_row = hparam_df.iloc[hparam_df['val_accuracy'].idxmax()]\nbest_accuracy = best_hyperparameters_row['val_accuracy']\nbest_hyperparameters = best_hyperparameters_row[['max_depth', 'min_samples_split', 'criterion']].to_dict()\n\n\n# Evaluate the performance of the selected hyperparameter combination on the test set\ndt_classifier = DecisionTreeClassifier(max_depth=best_hyperparameters['max_depth'], \n                                       min_samples_split=best_hyperparameters['min_samples_split'], \n                                       criterion=best_hyperparameters['criterion'], \n                                       random_state=42)\ndt_classifier.fit(X_train_val, y_train_val)\ntest_accuracy = dt_classifier.score(X_test, y_test)\n\nprint(\"Best Hyperparameters:\", best_hyperparameters)\nprint(\"Validation Set accuracy: {:.4f}\".format(best_accuracy))\nprint(\"Test Set accuracy: {:.4f}\".format(test_accuracy))\n\nBest Hyperparameters: {'max_depth': 6, 'min_samples_split': 5, 'criterion': 'gini'}\nValidation Set accuracy: 0.9250\nTest Set accuracy: 0.9067\n\n\nAvoiding nested loops by using itertools.product\nfor max_depth in hyperparameters['max_depth']:\n    for min_samples_split in hyperparameters['min_samples_split']:\n        for criterion in hyperparameters['criteria_values']:\n            # Create and fit the decision tree classifier with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate the performance on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            out[count] = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'criterion': criterion, 'val_accuracy': val_accuracy}\n            count += 1\n\nfrom itertools import product\n\nfor max_depth, min_samples_split, criterion in product(hyperparameters['max_depth'], hyperparameters['min_samples_split'], hyperparameters['criteria_values']):\n    # Define the Decision Tree Classifier\n    dt_classifier = DecisionTreeClassifier(\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        criterion=criterion,\n        random_state=42\n    )\n    dt_classifier.fit(X_train, y_train)\n\n2.1.2 Multiple random subsets of Training Set used as Validation Set\n\n# Initialize a list to store the optimal hyperparameters for each validation set\noptimal_hyperparameters = {}\ntest_accuracies = []\n\n# Set the number of subsets and iterations\nnum_subsets = 5\n\n# Make a pandas dataframe with columns as the hyperparameters, subset number, and validation accuracy\nhyperparameters_df = pd.DataFrame(columns=['max_depth', 'min_samples_split', 'criterion', 'subset', 'validation accuracy'])\n\n# Iterate over the subsets\nfor i in range(num_subsets):\n    # Split the data into training and validation sets\n    X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=i)\n    \n    # Initialize variables to store the best hyperparameters and accuracy for the current subset\n    best_accuracy = 0\n    best_hyperparameters = {}\n    \n    # Iterate over the hyperparameter values\n\n    for max_depth in hyperparameters['max_depth']:\n        for min_samples_split in hyperparameters['min_samples_split']:\n            for criterion in hyperparameters['criteria_values']:\n                # Initialize and train the model with the current hyperparameters\n                dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n                dt_classifier.fit(X_train_subset, y_train_subset)\n                \n                # Evaluate the model on the validation set\n                val_accuracy = dt_classifier.score(X_val_subset, y_val_subset)\n                hyperparameters_df.loc[len(hyperparameters_df)] = [max_depth, min_samples_split, criterion, i+1, val_accuracy]\n                \n                # Update the best accuracy and hyperparameters\n                if val_accuracy &gt; best_accuracy:\n                    best_accuracy = val_accuracy\n                    best_hyperparameters = {\n                        'max_depth': max_depth,\n                        'min_samples_split': min_samples_split,\n                        'criterion': criterion\n                    }\n    \n    optimal_hyperparameters[i] = best_hyperparameters\n\n    # Evaluate the model with the best hyperparameters on the test set\n    dt_classifier = DecisionTreeClassifier(max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], criterion=best_hyperparameters['criterion'], random_state=42)\n    dt_classifier.fit(X_train_val, y_train_val)\n    test_accuracy = dt_classifier.score(X_test, y_test)\n    test_accuracies.append(test_accuracy)\n\n\n\nprint(\"Optimal hyperparameters for {} inner folds/validation sets\".format(num_subsets))\nprint()\n# Print the optimal hyperparameters for each validation set\nfor i in range(num_subsets):\n    print(\"Optimal hyperparameters for validation set {}: {}\".format(i+1, optimal_hyperparameters[i]))\n    print(\"Test Accuracy for validation set {}: {:.4f}\".format(i+1, test_accuracies[i]))\n\nOptimal hyperparameters for 5 inner folds/validation sets\n\nOptimal hyperparameters for validation set 1: {'max_depth': 7, 'min_samples_split': 6, 'criterion': 'entropy'}\nTest Accuracy for validation set 1: 0.9000\nOptimal hyperparameters for validation set 2: {'max_depth': 5, 'min_samples_split': 7, 'criterion': 'gini'}\nTest Accuracy for validation set 2: 0.9033\nOptimal hyperparameters for validation set 3: {'max_depth': 6, 'min_samples_split': 2, 'criterion': 'entropy'}\nTest Accuracy for validation set 3: 0.9233\nOptimal hyperparameters for validation set 4: {'max_depth': 7, 'min_samples_split': 4, 'criterion': 'entropy'}\nTest Accuracy for validation set 4: 0.9000\nOptimal hyperparameters for validation set 5: {'max_depth': 6, 'min_samples_split': 2, 'criterion': 'entropy'}\nTest Accuracy for validation set 5: 0.9233\n\n\n\nhyperparameters_df\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nsubset\nvalidation accuracy\n\n\n\n\n0\n1\n2\ngini\n1\n0.790\n\n\n1\n1\n2\nentropy\n1\n0.790\n\n\n2\n1\n3\ngini\n1\n0.790\n\n\n3\n1\n3\nentropy\n1\n0.790\n\n\n4\n1\n4\ngini\n1\n0.790\n\n\n...\n...\n...\n...\n...\n...\n\n\n695\n10\n6\nentropy\n5\n0.900\n\n\n696\n10\n7\ngini\n5\n0.905\n\n\n697\n10\n7\nentropy\n5\n0.900\n\n\n698\n10\n8\ngini\n5\n0.905\n\n\n699\n10\n8\nentropy\n5\n0.900\n\n\n\n\n700 rows × 5 columns\n\n\n\n\ngrouped_df = hyperparameters_df.groupby(['max_depth', 'min_samples_split', 'criterion']).mean()['validation accuracy']\ngrouped_df\n\nmax_depth  min_samples_split  criterion\n1          2                  entropy      0.769\n                              gini         0.771\n           3                  entropy      0.769\n                              gini         0.771\n           4                  entropy      0.769\n                                           ...  \n10         6                  gini         0.889\n           7                  entropy      0.902\n                              gini         0.894\n           8                  entropy      0.904\n                              gini         0.893\nName: validation accuracy, Length: 140, dtype: float64\n\n\n\ngrouped_df.sort_values(ascending=False).head(10)\n\nmax_depth  min_samples_split  criterion\n6          7                  entropy      0.914\n           8                  entropy      0.914\n7          7                  entropy      0.912\n6          6                  entropy      0.912\n7          8                  entropy      0.912\n           6                  entropy      0.910\n6          4                  entropy      0.910\n           5                  entropy      0.910\n7          4                  entropy      0.909\n           5                  entropy      0.909\nName: validation accuracy, dtype: float64\n\n\n\noptimal_hyperparams = grouped_df.idxmax()\noptimal_hyperparams\n\n(6, 7, 'entropy')\n\n\n\ndf_classifier = DecisionTreeClassifier(max_depth=optimal_hyperparams[0], min_samples_split=optimal_hyperparams[1], criterion=optimal_hyperparams[2], random_state=42)\ndf_classifier.fit(X_train_val, y_train_val)\ntest_accuracy = df_classifier.score(X_test, y_test)\nprint(\"Test accuracy: {:.4f}\".format(test_accuracy))\n\nTest accuracy: 0.9233\n\n\n2.2 Nested Cross-Validation\n\nhyperparameters['max_depth'] = [1,2,3,4,5,6,7,8,9,10]\nhyperparameters['min_samples_split'] = [2,3,4,5,6,7,8]\nhyperparameters['criteria_values'] = ['gini', 'entropy']\n\n\nnum_outer_folds = 5\nnum_inner_folds = 5\n\nkf_outer = KFold(n_splits=num_outer_folds, shuffle=False)\nkf_inner = KFold(n_splits=num_inner_folds, shuffle=False)\n\n# Initialize lists to store the accuracies for the outer and inner loops\nouter_loop_accuracies = []\ninner_loop_accuracies = []\n\nresults= {}\nouter_count = 0\noverall_count = 0\n# Iterate over the outer folds\nfor outer_train_index, outer_test_index in kf_outer.split(X):\n    # Split the data into outer training and test sets\n    X_outer_train, X_outer_test = X[outer_train_index], X[outer_test_index]\n    y_outer_train, y_outer_test = y[outer_train_index], y[outer_test_index]\n    \n    \n    inner_count = 0\n    \n    for innner_train_index, inner_test_index in kf_inner.split(X_outer_train):\n        print(\"Outer Fold {}, Inner Fold {}\".format(outer_count+1, inner_count+1))\n        # Split the data into inner training and test sets\n        X_inner_train, X_inner_test = X_outer_train[innner_train_index], X_outer_train[inner_test_index]\n        y_inner_train, y_inner_test = y_outer_train[innner_train_index], y_outer_train[inner_test_index]\n        \n        for max_depth, min_samples_split, criterion in product(hyperparameters['max_depth'],\n                                                               hyperparameters['min_samples_split'],\n                                                               hyperparameters['criteria_values']):\n            \n            #print(max_depth, min_samples_split, criterion)\n            # Initialize and train the model with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, \n                                                   min_samples_split=min_samples_split, \n                                                   criterion=criterion, random_state=42)\n            dt_classifier.fit(X_inner_train, y_inner_train)\n            \n            # Evaluate the model on the inner test set\n            val_accuracy = dt_classifier.score(X_inner_test, y_inner_test)\n            \n            results[overall_count] = {'outer_fold': outer_count, \n                                      'inner_fold': inner_count, \n                                      'max_depth': max_depth, \n                                      'min_samples_split': min_samples_split, \n                                      'criterion': criterion, \n                                      'val_accuracy': val_accuracy}\n            overall_count += 1\n\n        inner_count += 1\n    outer_count += 1\n    \n            \n            \n\nOuter Fold 1, Inner Fold 1\nOuter Fold 1, Inner Fold 2\nOuter Fold 1, Inner Fold 3\nOuter Fold 1, Inner Fold 4\nOuter Fold 1, Inner Fold 5\nOuter Fold 2, Inner Fold 1\nOuter Fold 2, Inner Fold 2\nOuter Fold 2, Inner Fold 3\nOuter Fold 2, Inner Fold 4\nOuter Fold 2, Inner Fold 5\nOuter Fold 3, Inner Fold 1\nOuter Fold 3, Inner Fold 2\nOuter Fold 3, Inner Fold 3\nOuter Fold 3, Inner Fold 4\nOuter Fold 3, Inner Fold 5\nOuter Fold 4, Inner Fold 1\nOuter Fold 4, Inner Fold 2\nOuter Fold 4, Inner Fold 3\nOuter Fold 4, Inner Fold 4\nOuter Fold 4, Inner Fold 5\nOuter Fold 5, Inner Fold 1\nOuter Fold 5, Inner Fold 2\nOuter Fold 5, Inner Fold 3\nOuter Fold 5, Inner Fold 4\nOuter Fold 5, Inner Fold 5\n\n\n\noverall_results = pd.DataFrame(results).T\n\n\noverall_results\n\n\n\n\n\n\n\n\nouter_fold\ninner_fold\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n0\n0\n1\n2\ngini\n0.7625\n\n\n1\n0\n0\n1\n2\nentropy\n0.7625\n\n\n2\n0\n0\n1\n3\ngini\n0.7625\n\n\n3\n0\n0\n1\n3\nentropy\n0.7625\n\n\n4\n0\n0\n1\n4\ngini\n0.7625\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3495\n4\n4\n10\n6\nentropy\n0.9\n\n\n3496\n4\n4\n10\n7\ngini\n0.91875\n\n\n3497\n4\n4\n10\n7\nentropy\n0.9\n\n\n3498\n4\n4\n10\n8\ngini\n0.925\n\n\n3499\n4\n4\n10\n8\nentropy\n0.9\n\n\n\n\n3500 rows × 6 columns\n\n\n\nFind the best hyperparameters for each outer fold\n\nouter_fold = 0\nouter_fold_df = overall_results.query('outer_fold == @outer_fold')\nouter_fold_df\n\n\n\n\n\n\n\n\nouter_fold\ninner_fold\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n0\n0\n1\n2\ngini\n0.7625\n\n\n1\n0\n0\n1\n2\nentropy\n0.7625\n\n\n2\n0\n0\n1\n3\ngini\n0.7625\n\n\n3\n0\n0\n1\n3\nentropy\n0.7625\n\n\n4\n0\n0\n1\n4\ngini\n0.7625\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n695\n0\n4\n10\n6\nentropy\n0.85\n\n\n696\n0\n4\n10\n7\ngini\n0.86875\n\n\n697\n0\n4\n10\n7\nentropy\n0.85625\n\n\n698\n0\n4\n10\n8\ngini\n0.86875\n\n\n699\n0\n4\n10\n8\nentropy\n0.85625\n\n\n\n\n700 rows × 6 columns\n\n\n\nAggregate the validation accuracies for each hyperparameter combination across all inner folds\n\nouter_fold_df.groupby(['max_depth', 'min_samples_split', 'criterion']).mean()['val_accuracy'].sort_values(ascending=False).head(10)\n\nmax_depth  min_samples_split  criterion\n6          7                  gini          0.9175\n           8                  gini          0.9175\n           6                  gini          0.9175\n           4                  gini         0.91625\n           3                  gini         0.91625\n           2                  gini         0.91625\n           5                  gini         0.91625\n7          6                  gini         0.91625\n           7                  gini         0.91625\n           8                  gini           0.915\nName: val_accuracy, dtype: object"
  },
  {
    "objectID": "notebooks/text_to_image.html",
    "href": "notebooks/text_to_image.html",
    "title": "Using Diffusion to Generate Images from Text",
    "section": "",
    "text": "References\n\nPromptHero guide\n\n\n%pip install --upgrade \\\n  diffusers \\\n  transformers \\\n  safetensors \\\n  sentencepiece \\\n  accelerate \\\n  bitsandbytes \\\n  torch \\\n  huggingface_hub --quiet\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\ntorchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom huggingface_hub import login\n\nlogin()\n\n\n\n\n\nBasic Imports\n\n# Display the images in a grid\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\nimport torch\npipe = DiffusionPipeline.from_pretrained(\n    \"prompthero/openjourney\", \n    torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.vae = vae\npipe = pipe.to(\"cuda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n\n\n\n\n\n\n\n\n\ndef generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions):\n    random_seeds = [random.randint(0, 65000) for _ in range(num_variations)]\n    images = pipe(prompt= num_variations * [prompt],\n              num_inference_steps=num_steps,\n              guidance_scale=prompt_guidance,\n              height = dimensions[0],\n              width = dimensions[1],\n              generator = [torch.Generator('cuda').manual_seed(i) for i in random_seeds]\n             ).images\n    return images\n\n\nimport random\n\n\n# Setting for image generation\nprompt = 'Small happy dog anf owner learning to walk on a rainy day. Colored photography. Leica lens. Hi-res. hd 8k --ar 2:3'\nnum_steps = 150\nnum_variations = 4\nprompt_guidance = 8\ndimensions = (400, 600) # (width, height) tuple\nrandom_seeds = [random.randint(0, 65000) for _ in range(num_variations)]\n\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\n\n\n\n\n\n\n\ndef display_images(images, num_variations, dimensions):\n    fig = plt.figure(figsize=(dimensions[0]/10, dimensions[1]/10))\n    columns = num_variations\n    rows = 1\n    for i in range(1, columns*rows +1):\n        img = images[i-1]\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img)\n        # hide axes\n        plt.axis('off')\n    plt.show()\n    \ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\nprompt = \"Batman, cinematic lighting, dark background, very high resolution 3D render.\"\n\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"A logo for a research group in India called Sustainability lab that works on AI for sustainability. Show AI as the central theme. Show applications in health, air quality\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"Portrait of a small kid with a big smile.\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"A photorealistic render of an academic campus in India, on the edges of a river, low-light, evening.\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-discrete-output.html",
    "href": "notebooks/decision-tree-real-input-discrete-output.html",
    "title": "Decision Trees Real Input and Discrete Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\nHeavily borrowed and inspired from https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n\nlatexify(columns=2)\n\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.2)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='rainbow')\nformat_axes(plt.gca())\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\n\nText(0, 0.5, '$x_2$')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef visualize_tree(depth, X, y, ax=None, cmap='rainbow'):\n    model = DecisionTreeClassifier(max_depth=depth)\n    ax = ax or plt.gca()\n    print(model, depth)\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    format_axes(plt.gca())\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.2,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    plt.tight_layout()\n    plt.savefig(f\"../figures/decision-trees/dt-{depth}.pdf\", bbox_inches=\"tight\")\n    plt.clf()\n\n\nfor depth in range(1, 11):\n    visualize_tree(depth, X, y)\n\nDecisionTreeClassifier(max_depth=1) 1\nDecisionTreeClassifier(max_depth=2) 2\nDecisionTreeClassifier(max_depth=3) 3\nDecisionTreeClassifier(max_depth=4) 4\nDecisionTreeClassifier(max_depth=5) 5\nDecisionTreeClassifier(max_depth=6) 6\nDecisionTreeClassifier(max_depth=7) 7\nDecisionTreeClassifier(max_depth=8) 8\nDecisionTreeClassifier(max_depth=9) 9\nDecisionTreeClassifier(max_depth=10) 10\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\n&lt;Figure size 400x246.914 with 0 Axes&gt;"
  },
  {
    "objectID": "notebooks/dt_weighted.html",
    "href": "notebooks/dt_weighted.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom latexify import latexify, format_axes\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nlatexify(columns=2)\n\n\n# Dummy Data\nx1 = np.array([1, 3, 2, 5, 7, 8])\nx2 = np.array([1.5, 3, 5, 2, 4, 4.5])\ncategory = np.array([0, 1, 1, 1, 0, 0]) # 0 -&gt; - class and 1 -&gt; + class\n\n# Separate data points for each class\nclass_0_x1 = x1[category == 0]\nclass_0_x2 = x2[category == 0]\nclass_1_x1 = x1[category == 1]\nclass_1_x2 = x2[category == 1]\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\n\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig1.pdf\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\n\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig2.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig3.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n\nplt.fill_betweenx(y=np.arange(0, 6, 0.01), x1=0, x2= 4, color='red', alpha=0.3)\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig4.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n\nplt.fill_betweenx(y=np.arange(0, 6, 0.01), x1=4, x2= 10, color='blue', alpha=0.3)\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig5.pdf\")\nplt.show()"
  },
  {
    "objectID": "notebooks/entropy.html",
    "href": "notebooks/entropy.html",
    "title": "Decision Trees Entropy",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\nfrom scipy.special import xlogy\n\n# Function to calculate entropy\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Generate data\nx_values = np.linspace(0.000, 1.0, 100)  # Avoid log(0) in the calculation\ny_values = entropy(x_values)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: divide by zero encountered in log2\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: invalid value encountered in multiply\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\n\ny_values\n\narray([       nan, 0.08146203, 0.14257333, 0.19590927, 0.24414164,\n       0.28853851, 0.32984607, 0.36855678, 0.40502013, 0.43949699,\n       0.47218938, 0.50325833, 0.53283506, 0.56102849, 0.58793037,\n       0.61361902, 0.63816195, 0.66161791, 0.68403844, 0.70546904,\n       0.72595015, 0.74551784, 0.76420451, 0.78203929, 0.79904852,\n       0.81525608, 0.83068364, 0.84535094, 0.85927598, 0.87247521,\n       0.88496364, 0.89675502, 0.90786192, 0.91829583, 0.92806728,\n       0.93718586, 0.9456603 , 0.95349858, 0.9607079 , 0.96729478,\n       0.97326507, 0.97862399, 0.98337619, 0.98752571, 0.99107606,\n       0.99403021, 0.99639062, 0.99815923, 0.9993375 , 0.9999264 ,\n       0.9999264 , 0.9993375 , 0.99815923, 0.99639062, 0.99403021,\n       0.99107606, 0.98752571, 0.98337619, 0.97862399, 0.97326507,\n       0.96729478, 0.9607079 , 0.95349858, 0.9456603 , 0.93718586,\n       0.92806728, 0.91829583, 0.90786192, 0.89675502, 0.88496364,\n       0.87247521, 0.85927598, 0.84535094, 0.83068364, 0.81525608,\n       0.79904852, 0.78203929, 0.76420451, 0.74551784, 0.72595015,\n       0.70546904, 0.68403844, 0.66161791, 0.63816195, 0.61361902,\n       0.58793037, 0.56102849, 0.53283506, 0.50325833, 0.47218938,\n       0.43949699, 0.40502013, 0.36855678, 0.32984607, 0.28853851,\n       0.24414164, 0.19590927, 0.14257333, 0.08146203,        nan])\n\n\n\n# Replace NaN values with 0\ny_values = np.nan_to_num(y_values, nan=0.0)\n\n\nlatexify(columns=2)\n\n\nplt.plot(x_values, y_values, color='black')\n\n# Set labels and title\nplt.xlabel('$P(+)$')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. $P(+)$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/entropy.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Function to calculate entropy with numerical stability\ndef entropy_numerically_stable(p):\n    return (-xlogy(p, p) - xlogy(1 - p, 1 - p))/np.log(2)\n\ny_values = entropy_numerically_stable(x_values)\n\n\nplt.plot(x_values, y_values)\n\n\n\n\n\n\n\n\nHow does xlogy handle the corner case?\n\nxlogy??\n\nCall signature:  xlogy(*args, **kwargs)\nType:            ufunc\nString form:     &lt;ufunc 'xlogy'&gt;\nFile:            ~/miniconda3/lib/python3.9/site-packages/numpy/__init__.py\nDocstring:      \nxlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nxlogy(x, y, out=None)\n\nCompute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n\nParameters\n----------\nx : array_like\n    Multiplier\ny : array_like\n    Argument\nout : ndarray, optional\n    Optional output array for the function results\n\nReturns\n-------\nz : scalar or ndarray\n    Computed x*log(y)\n\nNotes\n-----\nThe log function used in the computation is the natural log.\n\n.. versionadded:: 0.13.0\n\nExamples\n--------\nWe can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n.. math::\n    L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i))\n\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.special import xlogy\n&gt;&gt;&gt; y = np.array([0, 1, 0, 1, 1, 0])\n&gt;&gt;&gt; y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\n&gt;&gt;&gt; n = len(y)\n&gt;&gt;&gt; loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\n&gt;&gt;&gt; loss /= n\n&gt;&gt;&gt; loss\n0.29597052165495025\n\nA lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilties are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.\nClass docstring:\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\n**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays."
  },
  {
    "objectID": "notebooks/meshgrid.html",
    "href": "notebooks/meshgrid.html",
    "title": "Meshgrid",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nimport ipywidgets as widgets\nfrom ipywidgets import interactive\n\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\n\nrf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\n# Decision surface\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.viridis)\nax = plt.gca()\nxlim = X[:, 0].min()-0.5, X[:, 0].max() + 0.5\nylim = X[:, 1].min()-0.5, X[:, 1].max() + 0.5\n\n# Create grid to evaluate model\nx_lin = np.linspace(xlim[0], xlim[1], 30)\ny_lin = np.linspace(ylim[0], ylim[1], 30)\n\nXX, YY = np.meshgrid(x_lin, y_lin)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\nZ = rf.predict_proba(xy)[:, 1].reshape(XX.shape)\n\n# Plot decision boundary\nax.contourf(XX, YY, Z, cmap=plt.cm.viridis, alpha=0.2);\n\nplt.colorbar();\n\n\n\n\n\n\n\n\n\nX_arr = np.array([1, 2, 3, 4])\nY_arr = np.array([5, 6, 7])\n\nXX, YY = np.meshgrid(X_arr, Y_arr)\n\n\nXX\n\narray([[1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4]])\n\n\n\nYY\n\narray([[5, 5, 5, 5],\n       [6, 6, 6, 6],\n       [7, 7, 7, 7]])\n\n\n\nXX.shape, YY.shape\n\n((3, 4), (3, 4))\n\n\n\nout = {}\ncount = 0\nfor i in range(XX.shape[0]):\n    for j in range(XX.shape[1]):\n        count = count + 1\n        out[count] = {\"i\": i, \"j\": j, \"XX\": XX[i, j], \"YY\": YY[i, j]}\n\n\npd.DataFrame(out).T\n\n\n\n\n\n\n\n\ni\nj\nXX\nYY\n\n\n\n\n1\n0\n0\n1\n5\n\n\n2\n0\n1\n2\n5\n\n\n3\n0\n2\n3\n5\n\n\n4\n0\n3\n4\n5\n\n\n5\n1\n0\n1\n6\n\n\n6\n1\n1\n2\n6\n\n\n7\n1\n2\n3\n6\n\n\n8\n1\n3\n4\n6\n\n\n9\n2\n0\n1\n7\n\n\n10\n2\n1\n2\n7\n\n\n11\n2\n2\n3\n7\n\n\n12\n2\n3\n4\n7\n\n\n\n\n\n\n\n\nXX[0], YY[0]\n\n(array([1, 2, 3, 4]), array([5, 5, 5, 5]))\n\n\n\nfor i in range(XX.shape[0]):\n    plt.plot(XX[i], YY[i], 'o', label=i)\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(XX, YY, 'o');\n\n\n\n\n\n\n\n\n\nxlim = X[:, 0].min()-0.5, X[:, 0].max() + 0.5\nylim = X[:, 1].min()-0.5, X[:, 1].max() + 0.5\n\n# Create grid to evaluate model\nx_lin = np.linspace(xlim[0], xlim[1], 30)\ny_lin = np.linspace(ylim[0], ylim[1], 30)\n\n\nx_lin\n\narray([-1.67150293, -1.52070662, -1.36991031, -1.219114  , -1.06831769,\n       -0.91752137, -0.76672506, -0.61592875, -0.46513244, -0.31433613,\n       -0.16353982, -0.01274351,  0.1380528 ,  0.28884911,  0.43964542,\n        0.59044173,  0.74123804,  0.89203435,  1.04283066,  1.19362697,\n        1.34442328,  1.49521959,  1.6460159 ,  1.79681221,  1.94760852,\n        2.09840483,  2.24920114,  2.39999745,  2.55079376,  2.70159007])\n\n\n\nXX, YY = np.meshgrid(x_lin, y_lin)\n\n\ndef update_plot(i=0, j=2):\n    x_point = XX[i, j]\n    y_point = YY[i, j]\n\n\n    plt.plot(XX, YY, 'o', alpha=0.1, color='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.viridis)\n\n    pred = rf.predict_proba([[x_point, y_point]])[:, 1]\n\n    plt.scatter(x_point, y_point, s=100, c='r')\n    plt.title(f\"Prediction P(Class 1): {pred[0]:.2f}\")\n    plt.show()\n\nupdate_plot(0, 0)\n\n\n\n\n\n\n\n\n\nwidget = interactive(update_plot, i=(0, XX.shape[0]-1), j=(0, XX.shape[1]-1))\n\n# Display the widget\ndisplay(widget)\n\n\n\n\n\nXX[0], YY[:, 0]\n\n(array([-1.67150293, -1.52070662, -1.36991031, -1.219114  , -1.06831769,\n        -0.91752137, -0.76672506, -0.61592875, -0.46513244, -0.31433613,\n        -0.16353982, -0.01274351,  0.1380528 ,  0.28884911,  0.43964542,\n         0.59044173,  0.74123804,  0.89203435,  1.04283066,  1.19362697,\n         1.34442328,  1.49521959,  1.6460159 ,  1.79681221,  1.94760852,\n         2.09840483,  2.24920114,  2.39999745,  2.55079376,  2.70159007]),\n array([-1.23673767, -1.13313159, -1.02952551, -0.92591943, -0.82231335,\n        -0.71870727, -0.61510119, -0.51149511, -0.40788903, -0.30428295,\n        -0.20067687, -0.09707079,  0.00653529,  0.11014137,  0.21374745,\n         0.31735353,  0.42095961,  0.52456569,  0.62817177,  0.73177785,\n         0.83538393,  0.93899001,  1.04259609,  1.14620217,  1.24980825,\n         1.35341433,  1.45702041,  1.56062649,  1.66423257,  1.76783865]))\n\n\n\nXX.shape\n\n(30, 30)\n\n\n\nfrom einops import rearrange, repeat, reduce\n\n\nXX.shape\n\n(30, 30)\n\n\n\nXX.ravel().shape\n\n(900,)\n\n\n\nrearrange(XX, 'i j -&gt; (i j) 1').shape, rearrange(XX, 'i j -&gt; (i j)').shape\n\n((900, 1), (900,))\n\n\n\nrearrange(YY, 'i j -&gt; (i j) 1').shape\n\n(900, 1)\n\n\n\nXX_flat = rearrange(XX, 'i j -&gt; (i j) 1')\nYY_flat = rearrange(YY, 'i j -&gt; (i j) 1')\n\n\nnp.array([XX_flat, YY_flat]).shape\n\n(2, 900, 1)\n\n\n\nrearrange([XX_flat, YY_flat], 'f n 1 -&gt; n f').shape\n\n(900, 2)\n\n\n\nX_feature = rearrange([XX_flat, YY_flat], 'f n 1 -&gt; n f')\n\n\nX_feature[:32]\n\narray([[-1.67150293, -1.23673767],\n       [-1.52070662, -1.23673767],\n       [-1.36991031, -1.23673767],\n       [-1.219114  , -1.23673767],\n       [-1.06831769, -1.23673767],\n       [-0.91752137, -1.23673767],\n       [-0.76672506, -1.23673767],\n       [-0.61592875, -1.23673767],\n       [-0.46513244, -1.23673767],\n       [-0.31433613, -1.23673767],\n       [-0.16353982, -1.23673767],\n       [-0.01274351, -1.23673767],\n       [ 0.1380528 , -1.23673767],\n       [ 0.28884911, -1.23673767],\n       [ 0.43964542, -1.23673767],\n       [ 0.59044173, -1.23673767],\n       [ 0.74123804, -1.23673767],\n       [ 0.89203435, -1.23673767],\n       [ 1.04283066, -1.23673767],\n       [ 1.19362697, -1.23673767],\n       [ 1.34442328, -1.23673767],\n       [ 1.49521959, -1.23673767],\n       [ 1.6460159 , -1.23673767],\n       [ 1.79681221, -1.23673767],\n       [ 1.94760852, -1.23673767],\n       [ 2.09840483, -1.23673767],\n       [ 2.24920114, -1.23673767],\n       [ 2.39999745, -1.23673767],\n       [ 2.55079376, -1.23673767],\n       [ 2.70159007, -1.23673767],\n       [-1.67150293, -1.13313159],\n       [-1.52070662, -1.13313159]])\n\n\n\nZ = rf.predict_proba(X_feature)[:, 1]\n\n\nZ.shape\n\n(900,)\n\n\n\nplt.scatter(XX_flat, YY_flat, c=Z, cmap=plt.cm.viridis)\n\n\n\n\n\n\n\n\n\nZ[:10]\n\narray([0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.36, 0.73])\n\n\n\n# Divide Z into k levels\nk = 10\nmin_Z = Z.min()\nmax_Z = Z.max()\n\nlevels = np.linspace(min_Z, max_Z, k)\n\nlevels\n\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])\n\n\n\n# Create an image from Z \nimg = rearrange(Z, '(h w) -&gt; h w', h=XX.shape[0])\nplt.imshow(img, cmap=plt.cm.viridis, \n           extent=[XX.min(), XX.max(), YY.min(), YY.max()], \n           origin='lower',\n           interpolation='spline36')\n\n\n\n\n\n\n\n\n\nplt.contourf(XX, YY, Z.reshape(XX.shape), cmap=plt.cm.viridis, levels=10);\nplt.colorbar();\n\n\n\n\n\n\n\n\n\nplt.contourf?\n\nSignature: plt.contourf(*args, data=None, **kwargs)\nDocstring:\nPlot filled contours.\n\nCall signature::\n\n    contourf([X, Y,] Z, [levels], **kwargs)\n\n`.contour` and `.contourf` draw contour lines and filled contours,\nrespectively.  Except as noted, function signatures and return values\nare the same for both versions.\n\nParameters\n----------\nX, Y : array-like, optional\n    The coordinates of the values in *Z*.\n\n    *X* and *Y* must both be 2D with the same shape as *Z* (e.g.\n    created via `numpy.meshgrid`), or they must both be 1-D such\n    that ``len(X) == N`` is the number of columns in *Z* and\n    ``len(Y) == M`` is the number of rows in *Z*.\n\n    *X* and *Y* must both be ordered monotonically.\n\n    If not given, they are assumed to be integer indices, i.e.\n    ``X = range(N)``, ``Y = range(M)``.\n\nZ : (M, N) array-like\n    The height values over which the contour is drawn.\n\nlevels : int or array-like, optional\n    Determines the number and positions of the contour lines / regions.\n\n    If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries\n    to automatically choose no more than *n+1* \"nice\" contour levels\n    between *vmin* and *vmax*.\n\n    If array-like, draw contour lines at the specified levels.\n    The values must be in increasing order.\n\nReturns\n-------\n`~.contour.QuadContourSet`\n\nOther Parameters\n----------------\ncorner_mask : bool, default: :rc:`contour.corner_mask`\n    Enable/disable corner masking, which only has an effect if *Z* is\n    a masked array.  If ``False``, any quad touching a masked point is\n    masked out.  If ``True``, only the triangular corners of quads\n    nearest those points are always masked out, other triangular\n    corners comprising three unmasked points are contoured as usual.\n\ncolors : color string or sequence of colors, optional\n    The colors of the levels, i.e. the lines for `.contour` and the\n    areas for `.contourf`.\n\n    The sequence is cycled for the levels in ascending order. If the\n    sequence is shorter than the number of levels, it's repeated.\n\n    As a shortcut, single color strings may be used in place of\n    one-element lists, i.e. ``'red'`` instead of ``['red']`` to color\n    all levels with the same color. This shortcut does only work for\n    color strings, not for other ways of specifying colors.\n\n    By default (value *None*), the colormap specified by *cmap*\n    will be used.\n\nalpha : float, default: 1\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\ncmap : str or `.Colormap`, default: :rc:`image.cmap`\n    A `.Colormap` instance or registered colormap name. The colormap\n    maps the level values to colors.\n\n    If both *colors* and *cmap* are given, an error is raised.\n\nnorm : `~matplotlib.colors.Normalize`, optional\n    If a colormap is used, the `.Normalize` instance scales the level\n    values to the canonical colormap range [0, 1] for mapping to\n    colors. If not given, the default linear scaling is used.\n\nvmin, vmax : float, optional\n    If not *None*, either or both of these values will be supplied to\n    the `.Normalize` instance, overriding the default color scaling\n    based on *levels*.\n\norigin : {*None*, 'upper', 'lower', 'image'}, default: None\n    Determines the orientation and exact position of *Z* by specifying\n    the position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y*\n    are not given.\n\n    - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n    - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n    - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left\n      corner.\n    - 'image': Use the value from :rc:`image.origin`.\n\nextent : (x0, x1, y0, y1), optional\n    If *origin* is not *None*, then *extent* is interpreted as in\n    `.imshow`: it gives the outer pixel boundaries. In this case, the\n    position of Z[0, 0] is the center of the pixel, not a corner. If\n    *origin* is *None*, then (*x0*, *y0*) is the position of Z[0, 0],\n    and (*x1*, *y1*) is the position of Z[-1, -1].\n\n    This argument is ignored if *X* and *Y* are specified in the call\n    to contour.\n\nlocator : ticker.Locator subclass, optional\n    The locator is used to determine the contour levels if they\n    are not given explicitly via *levels*.\n    Defaults to `~.ticker.MaxNLocator`.\n\nextend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n    Determines the ``contourf``-coloring of values that are outside the\n    *levels* range.\n\n    If 'neither', values outside the *levels* range are not colored.\n    If 'min', 'max' or 'both', color the values below, above or below\n    and above the *levels* range.\n\n    Values below ``min(levels)`` and above ``max(levels)`` are mapped\n    to the under/over values of the `.Colormap`. Note that most\n    colormaps do not have dedicated colors for these by default, so\n    that the over and under values are the edge values of the colormap.\n    You may want to set these values explicitly using\n    `.Colormap.set_under` and `.Colormap.set_over`.\n\n    .. note::\n\n        An existing `.QuadContourSet` does not get notified if\n        properties of its colormap are changed. Therefore, an explicit\n        call `.QuadContourSet.changed()` is needed after modifying the\n        colormap. The explicit call can be left out, if a colorbar is\n        assigned to the `.QuadContourSet` because it internally calls\n        `.QuadContourSet.changed()`.\n\n    Example::\n\n        x = np.arange(1, 10)\n        y = x.reshape(-1, 1)\n        h = x * y\n\n        cs = plt.contourf(h, levels=[10, 30, 50],\n            colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n        cs.cmap.set_over('red')\n        cs.cmap.set_under('blue')\n        cs.changed()\n\nxunits, yunits : registered units, optional\n    Override axis units by specifying an instance of a\n    :class:`matplotlib.units.ConversionInterface`.\n\nantialiased : bool, optional\n    Enable antialiasing, overriding the defaults.  For\n    filled contours, the default is *True*.  For line contours,\n    it is taken from :rc:`lines.antialiased`.\n\nnchunk : int &gt;= 0, optional\n    If 0, no subdivision of the domain.  Specify a positive integer to\n    divide the domain into subdomains of *nchunk* by *nchunk* quads.\n    Chunking reduces the maximum length of polygons generated by the\n    contouring algorithm which reduces the rendering workload passed\n    on to the backend and also requires slightly less RAM.  It can\n    however introduce rendering artifacts at chunk boundaries depending\n    on the backend, the *antialiased* flag and value of *alpha*.\n\nlinewidths : float or array-like, default: :rc:`contour.linewidth`\n    *Only applies to* `.contour`.\n\n    The line width of the contour lines.\n\n    If a number, all levels will be plotted with this linewidth.\n\n    If a sequence, the levels in ascending order will be plotted with\n    the linewidths in the order specified.\n\n    If None, this falls back to :rc:`lines.linewidth`.\n\nlinestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n    *Only applies to* `.contour`.\n\n    If *linestyles* is *None*, the default is 'solid' unless the lines\n    are monochrome.  In that case, negative contours will take their\n    linestyle from :rc:`contour.negative_linestyle` setting.\n\n    *linestyles* can also be an iterable of the above strings\n    specifying a set of linestyles to be used. If this\n    iterable is shorter than the number of contour levels\n    it will be repeated as necessary.\n\nhatches : list[str], optional\n    *Only applies to* `.contourf`.\n\n    A list of cross hatch patterns to use on the filled areas.\n    If None, no hatching will be added to the contour.\n    Hatching is supported in the PostScript, PDF, SVG and Agg\n    backends only.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\nNotes\n-----\n1. `.contourf` differs from the MATLAB version in that it does not draw\n   the polygon edges. To draw edges, add line contours with calls to\n   `.contour`.\n\n2. `.contourf` fills intervals that are closed at the top; that is, for\n   boundaries *z1* and *z2*, the filled region is::\n\n      z1 &lt; Z &lt;= z2\n\n   except for the lowest interval, which is closed on both sides (i.e.\n   it includes the lowest value).\n\n3. `.contour` and `.contourf` use a `marching squares\n   &lt;https://en.wikipedia.org/wiki/Marching_squares&gt;`_ algorithm to\n   compute contour locations.  More information can be found in\n   the source ``src/_contour.h``.\nFile:      ~/miniforge3/lib/python3.9/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\n\nX, Y = np.meshgrid(x, y)\n\nZ = X**2 + Y**2\n\nplt.contourf(X, Y, Z, cmap=plt.cm.viridis);\nplt.gca().set_aspect('equal')\nplt.colorbar();\n\n\n\n\n\n\n\n\n\n# Surface plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.gca(projection='3d')\n\nax.plot_surface(X, Y, Z, cmap=plt.cm.viridis)\n\n/tmp/ipykernel_2896512/3580262868.py:5: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().\n  ax = fig.gca(projection='3d')"
  },
  {
    "objectID": "notebooks/logistic-iris.html",
    "href": "notebooks/logistic-iris.html",
    "title": "Logistic Regression - Iris dataset",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.datasets import load_iris\n\n\nd = load_iris()\nX = d['data'][:, :2]\ny = d['target']\n\n\nd['feature_names']\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\nlatexify()\ncolours = ['blue', 'red', 'green']\nspecies = ['I. setosa', 'I. versicolor', 'I. virginica']\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i] ,\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\n\n\nplt.savefig(\"../figures/logistic-regression/logisitic-iris.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(X, y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nclf.coef_\n\narray([[-55.82562338,  47.29592374],\n       [ 26.96162409, -23.85029157],\n       [ 28.86399931, -23.44563218]])\n\n\n\nX.shape\n\n(150, 2)\n\n\n\ny.shape\n\n(150,)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\n#plt.scatter(X[:, 0], X[:, 1], c=y)\nlatexify()\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i],\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\nplt.savefig(\"../figures/logistic-regression/logisitic-iris-prediction.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/confusion-mnist.html",
    "href": "notebooks/confusion-mnist.html",
    "title": "Notion of Confusion in ML",
    "section": "",
    "text": "import numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.utils.multiclass import unique_labels\nimport torchvision\nimport torchvision.transforms as transforms\nfrom latexify import latexify\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n# Define transformations\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\n# Download and load MNIST dataset using torchvision\ntrain_dataset = torchvision.datasets.MNIST(root='../datasets', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.MNIST(root='../datasets', train=False, download=True, transform=transform)\n\n\n# Flatten the images for sklearn MLP\nX_train = train_dataset.data.numpy().reshape((len(train_dataset), -1))\ny_train = train_dataset.targets.numpy()\nX_test = test_dataset.data.numpy().reshape((len(test_dataset), -1))\ny_test = test_dataset.targets.numpy()\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Plot few images\nlatexify()\nfig, axs = plt.subplots(1, 7, figsize=(8, 10))\nfor i in range(7):\n    axs[i].imshow(X_train[i].reshape((28, 28)), cmap='gray')\n    axs[i].set_title(y_train[i])\n    axs[i].axis('off')\n\n\n\n\n\n\n\n\n\n# Create and train the MLP model\nmlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\nmlp_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the test set\ny_probabilities = mlp_model.predict_proba(X_test_scaled)\n\n# Predict on the test set\ny_pred = mlp_model.predict(X_test_scaled)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9735\n\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\n\nlatexify(fig_width=6)\ncm = confusion_matrix(y_test, y_pred)\ncm_display = ConfusionMatrixDisplay(cm).plot(values_format='d', cmap='gray', ax=plt.gca())\n\n# Save the figure with a higher resolution and without lossy compression\nplt.savefig(\"../figures/mnist-cm.png\", bbox_inches=\"tight\", dpi=400, transparent=True)\n\n# Show the plot\n\n\n\n\n\n\n\n\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.98       980\n           1       0.99      0.99      0.99      1135\n           2       0.97      0.96      0.97      1032\n           3       0.96      0.98      0.97      1010\n           4       0.98      0.97      0.98       982\n           5       0.98      0.97      0.97       892\n           6       0.97      0.97      0.97       958\n           7       0.97      0.98      0.97      1028\n           8       0.96      0.96      0.96       974\n           9       0.98      0.96      0.97      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000\n\n\n\n\n# Display the first k wrong classified images with highest probabilities\n\n# Find indices of wrongly classified samples\nwrong_indices = np.where(y_pred != y_test)[0]\n\n# Sort wrong predictions by highest class probability\nsorted_indices = np.argsort(np.max(y_probabilities[wrong_indices], axis=1))[::-1]\n\nk = 9\nlatexify(fig_width=8)\nfor i, idx in enumerate(sorted_indices[:k]):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(test_dataset[wrong_indices[idx]][0].numpy().squeeze(), cmap='gray')\n    plt.title(f'True: {y_test[wrong_indices[idx]]}, Pred: {y_pred[wrong_indices[idx]]}\\nProb: {np.max(y_probabilities[wrong_indices[idx]]):.1f}')\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/ensemble-feature-importance.html",
    "href": "notebooks/ensemble-feature-importance.html",
    "title": "Random Forest Feature Importance",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import Image\n\n# To plot trees in forest via graphviz\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ntry:\n    from latexify import latexify, format_axes\n    latexify(columns=2)\nexcept:\n    pass\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load IRIS dataset from Seaborn\niris = sns.load_dataset('iris')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n# classes\niris.species.unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nplt.hist(iris.sepal_width, bins=20)\n\n(array([ 1.,  3.,  4.,  3.,  8., 14., 14., 10., 26., 11., 19., 12.,  6.,\n         4.,  9.,  2.,  1.,  1.,  1.,  1.]),\n array([2.  , 2.12, 2.24, 2.36, 2.48, 2.6 , 2.72, 2.84, 2.96, 3.08, 3.2 ,\n        3.32, 3.44, 3.56, 3.68, 3.8 , 3.92, 4.04, 4.16, 4.28, 4.4 ]),\n &lt;BarContainer object of 20 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=iris, x=\"sepal_length\")\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.\n\n\n\n\n\n\n\n\n\n\nsns.displot(iris.sepal_length.values, kind='kde')\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.\n\n\n\n\n\n\n\n\n\n\nsns.displot(data=iris, x=\"sepal_length\", kind='kde')\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.\n\n\n\n\n\n\n\n\n\n\niris.groupby(\"species\")[\"petal_length\"].mean()\n\nspecies\nsetosa        1.462\nversicolor    4.260\nvirginica     5.552\nName: petal_length, dtype: float64\n\n\n\n# Pairplot\nsns.pairplot(iris, hue=\"species\")\n\n\n\n\n\n\n\n\n\n# Divide dataset into X and y\nX, y = iris.iloc[:, :-1], iris.iloc[:, -1]\nrf = RandomForestClassifier(n_estimators=10,random_state=0, criterion='entropy', bootstrap=True)\nrf.fit(X, y)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)\n\n\n\n# Visualize each tree in the Random Forest\nfor i, tree in enumerate(rf.estimators_):\n    # Create DOT data for the i-th tree\n    dot_data = export_graphviz(tree, out_file=None, \n                               feature_names=iris.columns[:-1],  \n                               class_names=iris.species.unique(),\n                               filled=True, rounded=True,\n                               special_characters=True,\n                               impurity=True,\n                               node_ids=True)\n    \n    # Use Graphviz to render the DOT data into a graph\n    graph = graphviz.Source(dot_data)\n    \n    # Save or display the graph (change the format as needed)\n    graph.render(filename=f'../figures/ensemble/feature-imp-{i}', format='pdf', cleanup=True)\n    graph.render(filename=f'../figures/ensemble/feature-imp-{i}', format='png', cleanup=True)\n\n\n# Visualize the tree\nImage(filename='../figures/ensemble/feature-imp-0.png')\n\n\n\n\n\n\n\n\n\n\\(t\\) = node\n\\(N_t\\) = number of observations at node \\(t\\)\n\\(N_{t_L}\\) = number of observations in the left child node of node \\(t\\)\n\\(N_{t_R}\\) = number of observations in the right child node of node \\(t\\)\n\\(p(t)=N_t/N\\) = proportion of observations in node \\(t\\)\n\\(X_j\\) = feature \\(j\\)\n\\(j_t\\) = feature used at node \\(t\\) for splitting\n\\(i(t)\\) = impurity at node \\(t\\) (impurity = entropy in this case)\n\\(M\\) = number of trees in the forest\n\n\nFor a particular node:\n\nInformation gain at node \\(t\\) = Impurity reduction at node \\(t\\) = entropy(parent) - weighted entropy(children)\n\n\\(\\Delta i(t) = i(t) - \\frac{N_{t_L}}{N_t} i(t_L) - \\frac{N_{t_r}}{N_t} i(t_R)\\)\n\n\nFor a tree:\nImportance of feature \\(X_j\\) is given by:\n\\(\\text{Imp}(X_j) = \\sum_{t \\in \\varphi_{m}} 1(j_t = j) \\Big[ p(t) \\Delta i(t) \\Big]\\)\n\n\nFor a forest:\nImportance of feature \\(X_j\\) for an ensemble of \\(M\\) trees \\(\\varphi_{m}\\) is:\n\\[\\begin{equation*}\n  \\text{Imp}(X_j) = \\frac{1}{M} \\sum_{m=1}^M \\sum_{t \\in \\varphi_{m}} 1(j_t = j) \\Big[ p(t) \\Delta i(t) \\Big]\n\\end{equation*}\\]\n\n1-1/np.e\n\n0.6321205588285577\n\n\n\nN = 150\n(1-1/np.e)*N\n\n94.81808382428365\n\n\n\nrf.feature_importances_\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\ns = []\nfor tree in rf.estimators_:\n    s.append(tree.feature_importances_)\n\n\nnp.array(s).mean(axis=0)\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\ntree_0 = rf.estimators_[0]\ntree_0.feature_importances_\n\narray([0.00397339, 0.01375245, 0.35802357, 0.6242506 ])\n\n\n\n# take one tree \ntree = rf.estimators_[0].tree_\n\n\ntree.feature\n\narray([ 3, -2,  2,  3, -2,  1, -2, -2,  0, -2,  2, -2, -2], dtype=int64)\n\n\n\n\n\n# Creating a mapping of feature names to the feature indices\nmapping = {-2: 'Leaf', 0: 'sepal_length', 1: 'sepal_width', 2: 'petal_length', 3: 'petal_width'}\n\n# print the node number along with the corresponding feature name\nfor node in range(tree.node_count):\n    print(f'Node {node}: {mapping[tree.feature[node]]}')\n\nNode 0: petal_width\nNode 1: Leaf\nNode 2: petal_length\nNode 3: petal_width\nNode 4: Leaf\nNode 5: sepal_width\nNode 6: Leaf\nNode 7: Leaf\nNode 8: sepal_length\nNode 9: Leaf\nNode 10: petal_length\nNode 11: Leaf\nNode 12: Leaf\n\n\n\nid = 2\ntree.children_left[id], tree.children_right[id]\n\n(3, 8)\n\n\n\ndef print_child_id(tree, node):\n    '''\n    Prints the child node ids of a given node.\n    tree: tree object\n    node: int\n    '''\n\n    # check if leaf\n    l, r = tree.children_left[node], tree.children_right[node]\n    if l == -1 and r == -1:\n        return None, None\n    return tree.children_left[node], tree.children_right[node]\n\nprint_child_id(tree, 0)\n\n(1, 2)\n\n\n\ntree.impurity\n\narray([1.57310798, 0.        , 0.98464683, 0.34781691, 0.        ,\n       0.81127812, 0.        , 0.        , 0.12741851, 0.        ,\n       0.2108423 , 0.        , 0.        ])\n\n\n\ndef all_data(tree, node):\n    '''\n    Returns all the data required to calculate the information gain.\n    '''\n\n    # get the child nodes\n    left, right = print_child_id(tree, node)\n\n    # check if leaf, then return None\n    if left is None:\n        return None\n    \n    # get the data\n    entropy_node = tree.impurity[node]\n    entropy_left = tree.impurity[left]\n    entropy_right = tree.impurity[right]\n\n    # N = total number of samples considered during bagging, therefore, it is equal to the number of samples at the root node\n    N = tree.n_node_samples[0]\n\n    # n_l = number of samples at the left child node\n    n_l = tree.n_node_samples[left]\n\n    # n_r = number of samples at the right child node\n    n_r = tree.n_node_samples[right]\n\n    # n_t = total number of samples at the node\n    n_t = n_l + n_r\n\n    feature = mapping[tree.feature[node]]\n    \n    # calculate the information gain\n    info_gain_t = entropy_node - (n_l/n_t * entropy_left + n_r/n_t * entropy_right)\n    \n    return info_gain_t, N, n_l, n_r, n_t, feature\n\n\n# Calculate the importance of each features using the information gain for a tree\n\nscores = {}\nfor node in range(tree.node_count):\n    # Add the information gain of the node to the dictionary if it is not a leaf node\n    try:\n        ig, N, n_l, n_r, n_t, feature = all_data(tree, node)\n        p_t = n_t / N\n        scores[feature] = scores.get(feature, 0) + p_t * ig\n\n    # Skip if it is a leaf node\n    except:\n        continue\n\nser = pd.Series(scores) \ninfo_gain_tree = ser/ser.sum()\ninfo_gain_tree.sort_values(ascending=False)\n\npetal_width     0.639307\npetal_length    0.340335\nsepal_width     0.016459\nsepal_length    0.003899\ndtype: float64\n\n\n\n# Feature importance using sklearn for a tree\nsklearn_imp = tree.compute_feature_importances()\npd.Series(sklearn_imp, index=iris.columns[:-1]).sort_values(ascending=False)\n\npetal_width     0.624251\npetal_length    0.358024\nsepal_width     0.013752\nsepal_length    0.003973\ndtype: float64\n\n\n\n# Feature importance using sklearn for the forest\nsklearn_imp_forest = np.array([x.tree_.compute_feature_importances() for x in rf.estimators_]).mean(axis=0)\npd.Series(sklearn_imp_forest, index=iris.columns[:-1]).sort_values(ascending=False)\n\npetal_width     0.544270\npetal_length    0.323122\nsepal_length    0.098647\nsepal_width     0.033960\ndtype: float64\n\n\n\nser = pd.Series(sklearn_imp_forest, index=iris.columns[:-1])\nser.plot(kind='bar', rot=0)\nformat_axes(plt.gca())\nplt.savefig('../figures/ensemble/feature-imp-forest.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\nAside:\ntree.tree_.feature returns the feature used at each node to divide the node into two child nodes with the below given mapping. The sequence of the features is the same as the column sequence of the input data.\n\n-2: leaf node\n0: sepal_length\n1: sepal_width\n2: petal_length\n3: petal_width\n\ntree.tree_.children_left[node] returns the node number of the left child of the node\ntree.tree_.children_right[node] returns the node number of the right child of the node\nif there is no left or right child, it returns -1\n\n\nBootstrap code:\nin the random_forest.fit() function\n\n\n\nbootstrap_code"
  },
  {
    "objectID": "notebooks/logistic-circular.html",
    "href": "notebooks/logistic-circular.html",
    "title": "Logistic Regression - Basis",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\n\n# Choose some points between\n\n\nnp.random.seed(0)\nx1 = np.random.randn(1, 100)\nx2 = np.random.randn(1, 100)\n\n\ny = x1**2 + x2**2\n\n\nx1\n\narray([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n        -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n         0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n         0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n        -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n        -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n         0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n         0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n        -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n        -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n        -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n         0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n        -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n        -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n         0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n        -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n        -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n         1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n        -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n         0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936]])\n\n\n\ny[y&gt;1] = 1\ny[y&lt;1] = 0\n\nc = 0\nfor i in range(100):\n    if y[0, i] == 1:\n        y[0, i] = 0\n        c += 1\n    if c == 10:\n        break\n\n\nlatexify()\nplt.scatter(x1, x2, c=y,s=5)\n\n\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-data.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nnp.vstack((x1, x2)).shape\n\n(2, 100)\n\n\n\nclf_1 = LogisticRegression(penalty='none',solver='newton-cg')\nclf_1.fit(np.vstack((x1, x2)).T, y.T)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nX = np.vstack((x1, x2)).T\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf_1.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"../figures/logistic-regression/logisitic-linear-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nnew_x = np.zeros((4, 100))\n\n\nnew_x[0] = x1\nnew_x[1] = x2\nnew_x[2] = x1**2\nnew_x[3] = x2**2\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(new_x.T, y.T)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nclf.coef_\n\narray([[-0.50464855, -0.30337009,  1.08937351,  0.73697949]])\n\n\n\nnew_x.T[:, 0]\n\narray([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n        0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n        0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n       -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n       -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n        0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n        0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n       -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n       -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n       -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n        0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n       -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n       -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n        0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n       -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n       -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n        1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n       -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n        0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936])\n\n\n\nX = np.vstack((x1, x2)).T\nX.shape\n\n(100, 2)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nZ.shape\n\n(261, 272)\n\n\n\nnp.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())]\n\narray([[-2.85298982, -2.52340315,  8.13955089,  6.36756347],\n       [-2.83298982, -2.52340315,  8.0258313 ,  6.36756347],\n       [-2.81298982, -2.52340315,  7.9129117 ,  6.36756347],\n       ...,\n       [ 2.52701018,  2.67659685,  6.38578047,  7.16417069],\n       [ 2.54701018,  2.67659685,  6.48726088,  7.16417069],\n       [ 2.56701018,  2.67659685,  6.58954129,  7.16417069]])\n\n\n\nxx.ravel()\n\narray([-2.85298982, -2.83298982, -2.81298982, ...,  2.52701018,\n        2.54701018,  2.56701018])\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - h, X[:, 0].max() + h\ny_min, y_max = X[:, 1].min() - h, X[:, 1].max() + h\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n# Put the result into a color plot\nZ = Z[:, 0].reshape(xx.shape)\nlatexify()\nplt.contourf(xx, yy, Z,levels=np.linspace(0, 1.1, num=10),cmap='Blues')\nplt.gca().set_aspect('equal')\n#plt.scatter(x1, x2, c=y)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.colorbar(label='P(Tomatoes)')\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-probability.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nxx.shape\n\n(233, 244)\n\n\n\nZ.size\n\n56852\n\n\n\nnp.linspace(0, 1.1, num=50)\n\narray([0.        , 0.02244898, 0.04489796, 0.06734694, 0.08979592,\n       0.1122449 , 0.13469388, 0.15714286, 0.17959184, 0.20204082,\n       0.2244898 , 0.24693878, 0.26938776, 0.29183673, 0.31428571,\n       0.33673469, 0.35918367, 0.38163265, 0.40408163, 0.42653061,\n       0.44897959, 0.47142857, 0.49387755, 0.51632653, 0.53877551,\n       0.56122449, 0.58367347, 0.60612245, 0.62857143, 0.65102041,\n       0.67346939, 0.69591837, 0.71836735, 0.74081633, 0.76326531,\n       0.78571429, 0.80816327, 0.83061224, 0.85306122, 0.8755102 ,\n       0.89795918, 0.92040816, 0.94285714, 0.96530612, 0.9877551 ,\n       1.01020408, 1.03265306, 1.05510204, 1.07755102, 1.1       ])"
  },
  {
    "objectID": "notebooks/perceptron-learning.html",
    "href": "notebooks/perceptron-learning.html",
    "title": "Perceptron learning algorithm",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_and = np.array([0, 0, 0, 1])\ny_or = np.array([0, 1, 1, 1])\ny_xor = np.array([0, 1, 1, 0])\n\n\nclass Perceptron(object):\n    def __init__(self, lr=0.01, iterations=100):\n        self.lr = lr\n        self.iterations = iterations\n        \n    def activation(self, z):\n        ac = np.zeros_like(z)\n        ac[z&gt;0] = 1\n        return ac\n    \n    def fit(self, X, y):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        self.W = np.zeros((X_with_one.shape[1], 1))\n        for i in range(self.iterations):\n            for j in range(len(X)):\n                summation = (X_with_one@self.W).flatten()\n                y_hat = self.activation(summation)\n                err = y - y_hat.flatten() \n                self.W = self.W + (self.lr*err[j]*X_with_one[j]).reshape(*(self.W.shape))\n        \n    def predict(self, X):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        summation = (X_with_one@self.W).flatten()\n        y_hat = self.activation(summation)           \n        return y_hat\n\n    \n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X, y_or)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 1., 1., 1.])\n\n\n\nperceptron.fit(X, y_and)\n\n\nperceptron.W\n\narray([[-0.02],\n       [ 0.02],\n       [ 0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 0., 0., 1.])\n\n\n\nperceptron.fit(X, y_xor)\n\n\nperceptron.W\n\narray([[ 0.01],\n       [-0.01],\n       [ 0.  ]])\n\n\n\nperceptron.predict(X)\n\narray([1., 1., 0., 0.])\n\n\n\nXOR using feature transformation\n\n# Transformation: 1 \n# x1, x2, x1x2\nX_xor_1 = np.append(X, (X[:, 0]*X[:, 1]).reshape(-1, 1), axis=1)\n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X_xor_1, y_xor)\n\n\nperceptron.W\n\narray([[ 0.  ],\n       [ 0.01],\n       [ 0.01],\n       [-0.04]])\n\n\n\nnp.allclose(perceptron.predict(X_xor_1), y_xor)\n\nTrue\n\n\n\n(X[:, 0]*X[:, 1]).reshape(-1, 1)\n\narray([[0],\n       [0],\n       [0],\n       [1]])\n\n\n\n# Transformation: 1 \n# x1, x2, x1x2\n\n\nX_xor_2 = np.array([(1-X[:, 0])*X[:,1], (1-X[:, 1])*X[:,0]]).T\n\n\nperceptron = Perceptron()\nperceptron.fit(X_xor_2, y_xor)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])"
  },
  {
    "objectID": "notebooks/decision-tree-discrete-input-discrete-output.html",
    "href": "notebooks/decision-tree-discrete-input-discrete-output.html",
    "title": "Decision Trees Discrete Input and Discrete Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-discrete-output.csv\", index_col=0)\n\n\ndf\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\nNo\n\n\nD2\nSunny\nHot\nHigh\nStrong\nNo\n\n\nD3\nOvercast\nHot\nHigh\nWeak\nYes\n\n\nD4\nRain\nMild\nHigh\nWeak\nYes\n\n\nD5\nRain\nCool\nNormal\nWeak\nYes\n\n\nD6\nRain\nCool\nNormal\nStrong\nNo\n\n\nD7\nOvercast\nCool\nNormal\nStrong\nYes\n\n\nD8\nSunny\nMild\nHigh\nWeak\nNo\n\n\nD9\nSunny\nCool\nNormal\nWeak\nYes\n\n\nD10\nRain\nMild\nNormal\nWeak\nYes\n\n\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\nD12\nOvercast\nMild\nHigh\nStrong\nYes\n\n\nD13\nOvercast\nHot\nNormal\nWeak\nYes\n\n\nD14\nRain\nMild\nHigh\nStrong\nNo\n\n\n\n\n\n\n\n\ndef entropy(ser):\n    \"\"\"\n    Calculate entropy for a categorical variable.\n\n    Parameters:\n    - ser: pd.Series of categorical data\n\n    Returns:\n    - Entropy value\n    \"\"\"\n    # Count the occurrences of each unique value in the series\n    value_counts = ser.value_counts()\n\n    # Calculate the probabilities of each unique value\n    probabilities = value_counts / len(ser)\n\n    # Calculate entropy using the formula: H(S) = -p1*log2(p1) - p2*log2(p2) - ...\n    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n\n    return entropy_value\n    \n\n\nentropy(df[\"Play\"])\n\n0.9402859586706311"
  },
  {
    "objectID": "notebooks/classes-trees.html",
    "href": "notebooks/classes-trees.html",
    "title": "Basics of Classes and Plotting Trees",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport graphviz\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nclass TreeNode:\n    def __init__(self, name, value=None, shape='rectangle'):\n        self.name = name\n        self.value = value\n        self.children = []\n        self.shape = shape\n\n    def add_child(self, child_node):\n        self.children.append(child_node)\n\n    def display_tree_text(self, level=0):\n        indent = \"  \" * level\n        print(f\"{indent}|- {self.name}: {self.value}\")\n        for child in self.children:\n            child.display_tree_text(level + 1)\n\n    def display_tree_graphviz(self, dot=None, parent_name=None, graph=None):\n        if graph is None:\n            graph = graphviz.Digraph(format='png')\n        graph.node(str(id(self)), str(self.name), shape=self.shape)\n\n        if parent_name is not None:\n            graph.edge(str(id(parent_name)), str(id(self)))\n\n        for child in self.children:\n            child.display_tree_graphviz(dot, self, graph)\n\n        return graph\n\n    def display_tree_directly(self):\n        graph = self.display_tree_graphviz()\n        src = graph.source\n        display(graphviz.Source(src, format='png'))\n\n\n# Creating nodes\nroot = TreeNode(\"Root\")\nchild1 = TreeNode(\"Child 1\")\nchild2 = TreeNode(\"Child 2\")\nchild3 = TreeNode(\"Child 3\")\n\n# Building the tree structure\nroot.add_child(child1)\nroot.add_child(child2)\nchild2.add_child(child3)\n\n\n# Displaying the tree in text format\nroot.display_tree_text()\n\n|- Root: None\n  |- Child 1: None\n  |- Child 2: None\n    |- Child 3: None\n\n\n\ngraph = root.display_tree_graphviz()\ngraph\n\n\n\n\n\n\n\n\n\nclass DecisionTreeNode:\n    def __init__(self, feature, threshold, decision=None, left=None, right=None, shape='box'):\n        self.feature = feature\n        self.threshold = threshold\n        self.decision = decision\n        self.left = left\n        self.right = right\n        self.shape = shape\n\n    def display_tree_graphviz(self, dot=None, parent_name=None, graph=None, edge_label=None):\n        if graph is None:\n            graph = graphviz.Digraph(format='png')\n\n        node_label = self.feature\n        \n        if self.threshold is not None:\n            node_label += f\" &lt;= {self.threshold}\"\n        \n        if self.decision is not None:\n            node_label += f\"\\nDecision: {self.decision}\"\n        \n        graph.node(str(id(self)), node_label, shape=self.shape)\n\n        if parent_name is not None:\n            if edge_label is not None:\n                graph.edge(str(id(parent_name)), str(id(self)), label=edge_label)\n            else:\n                graph.edge(str(id(parent_name)), str(id(self)))\n\n        if self.left is not None:\n            self.left.display_tree_graphviz(dot, self, graph, edge_label=\"True\")\n        if self.right is not None:\n            self.right.display_tree_graphviz(dot, self, graph, edge_label=\"False\")\n\n        return graph\n\n\nroot = DecisionTreeNode(\"Feature A\", 5.0, decision=None)\nleft_child = DecisionTreeNode(\"Feature B\", 3.0, decision=None)\nright_child = DecisionTreeNode(\"Feature C\", 8.0, decision=None)\nroot.left = left_child\nroot.right = right_child\n\nleft_left = DecisionTreeNode(\"\", None, decision = 20.0)\nleft_right = DecisionTreeNode(\"\", None, decision = 10.0)\n\nleft_child.left = left_left\nleft_child.right = left_right\n\nright_left = DecisionTreeNode(\"\", None, decision = 30.0)\nright_right = DecisionTreeNode(\"\", None, decision = 40.0)\n\nright_child.left = right_left\nright_child.right = right_right\n\n\nroot.display_tree_graphviz()"
  },
  {
    "objectID": "notebooks/object-detection-segmentation.html",
    "href": "notebooks/object-detection-segmentation.html",
    "title": "Object detection using YOLO and segmentation using Segment Anything",
    "section": "",
    "text": "References\n\nhttps://blog.roboflow.com/how-to-use-segment-anything-model-sam/\n\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#config retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load pre-trained Faster R-CNN model\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n_ = model.eval()\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nimg = Image.open('../datasets/images/office.jpg')\n\n\n# Transform the image\nimg_tensor = F.to_tensor(img)\nimg_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n\n\nfig = plt.figure(figsize=(6, 6))\nplt.imshow(img)\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\n# Forward pass through the model\nwith torch.no_grad():\n    prediction = model(img_tensor)\n\n\nclass_names = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n\nthreshold = 0.5\n\n# Filter boxes based on confidence scores\nfiltered_boxes = prediction[0]['boxes'][prediction[0]['scores'] &gt; threshold]\nfiltered_scores = prediction[0]['scores'][prediction[0]['scores'] &gt; threshold]\nfiltered_labels = prediction[0]['labels'][prediction[0]['scores'] &gt; threshold]\n\n# Plot the image with bounding boxes and class names\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(img)\n\n# Add bounding boxes and labels to the plot\nfor i in range(filtered_boxes.size(0)):\n    box = filtered_boxes[i].cpu().numpy()\n    score = filtered_scores[i].item()\n    label = filtered_labels[i].item()\n\n    # Create a Rectangle patch\n    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n\n    # Add the patch to the Axes\n    ax.add_patch(rect)\n\n    # Add class name and score as text\n    class_name = class_names[label]\n    ax.text(box[0], box[1], f'{class_name} ({score:.2f})', color='r', verticalalignment='top')\n\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n%pip install -q roboflow supervision\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\ntry:\n    from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\nexcept ImportError:\n    %pip install git+https://github.com/facebookresearch/segment-anything.git\n    pip install -q roboflow supervision\n    from segment_anything import SamPredictor, sam_model_registry\n\n\n# Place the model weights\nimport os\npth_path = os.path.expanduser('~/.cache/torch/sam.pth')\npth_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n\nif not os.path.exists(pth_path):\n    import urllib.request\n    print(f'Downloading SAM weights to {pth_path}')\n    urllib.request.urlretrieve(pth_url, pth_path)\n\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nMODEL_TYPE = \"vit_h\"\n\nsam = sam_model_registry[MODEL_TYPE](checkpoint=pth_path)\n_ = sam.to(device=DEVICE)\n\n\nimport cv2\nfrom segment_anything import SamAutomaticMaskGenerator\n\nmask_generator = SamAutomaticMaskGenerator(sam)\n\nimage_bgr = cv2.imread('../datasets/images/office.jpg')\nimage_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\nresult = mask_generator.generate(image_rgb)\n\n\nimport supervision as sv\n\nmask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\ndetections = sv.Detections.from_sam(result)\nannotated_image = mask_annotator.annotate(image_bgr, detections)\n\n\n# Blending the annotated image with the original image\nalpha = 0.5  # Adjust the alpha value as needed\nblended_image = cv2.addWeighted(image_bgr, 1 - alpha, annotated_image, alpha, 0)\n\n# Display the original image, annotated image, and the blended result\nfig, ax = plt.subplots(1, 3, figsize=(18, 6))\nax[0].imshow(img_tensor.squeeze(0).permute(1, 2, 0))\nax[0].set_title('Original Image')\nax[1].imshow(annotated_image)\nax[1].set_title('Annotated Image')\nax[2].imshow(blended_image)\nax[2].set_title('Blended Result')\n\nfor a in ax:\n    a.axis('off')"
  },
  {
    "objectID": "notebooks/decision-tree-real-output.html",
    "href": "notebooks/decision-tree-real-output.html",
    "title": "Decision Trees Real Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-real-output.csv\", index_col=[0])\n\n\ndf\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWind\nMinutes Played\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\n20\n\n\nD2\nSunny\nHot\nHigh\nStrong\n24\n\n\nD3\nOvercast\nHot\nHigh\nWeak\n40\n\n\nD4\nRain\nMild\nHigh\nWeak\n50\n\n\nD5\nRain\nCool\nNormal\nWeak\n60\n\n\nD6\nRain\nCool\nNormal\nStrong\n10\n\n\nD7\nOvercast\nCool\nNormal\nStrong\n4\n\n\nD8\nSunny\nMild\nHigh\nWeak\n10\n\n\nD9\nSunny\nCool\nNormal\nWeak\n60\n\n\nD10\nRain\nMild\nNormal\nWeak\n40\n\n\nD11\nSunny\nMild\nHigh\nStrong\n45\n\n\nD12\nOvercast\nMild\nHigh\nStrong\n40\n\n\nD13\nOvercast\nHot\nNormal\nWeak\n35\n\n\nD14\nRain\nMild\nHigh\nStrong\n20\n\n\n\n\n\n\n\n\nmean_mins = df[\"Minutes Played\"].mean()\nprint(mean_mins)\n\n32.714285714285715\n\n\n\ninitial_mse = ((df[\"Minutes Played\"] - mean_mins) ** 2).mean()\nprint(initial_mse)\n\n311.3469387755102\n\n\n\n# Explore MSE for different splits based on the \"Outlook\" attribute\nweighted_total_mse = 0.0\nfor category in df[\"Wind\"].unique():\n    subset = df[df[\"Wind\"] == category]\n    \n    # Calculate MSE for the subset\n    mse_subset = ((subset[\"Minutes Played\"] - subset[\"Minutes Played\"].mean()) ** 2).mean()\n    \n    # Calculate the weighted MSE\n    weighted_mse = (len(subset) / len(df)) * mse_subset\n    weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    print(subset[\"Minutes Played\"].values)\n    print(f\"Wind: {category}\")\n    print(\"Subset MSE:\", mse_subset)\n    print(f\"Weighted MSE = {len(subset)}/{len(df)} * {mse_subset:0.4} = {weighted_mse:0.4}\")\n    print(\"\\n\")\n\nprint(\"Weighted total MSE:\", weighted_total_mse)\n\n[20 40 50 60 10 60 40 35]\nWind: Weak\nSubset MSE: 277.734375\nWeighted MSE = 8/14 * 277.7 = 158.7\n\n\n[24 10  4 45 40 20]\nWind: Strong\nSubset MSE: 218.13888888888889\nWeighted MSE = 6/14 * 218.1 = 93.49\n\n\nWeighted total MSE: 252.19345238095235\n\n\n\nreduction_mse_wind = initial_mse - weighted_total_mse\nprint(reduction_mse_wind)\n\n59.15348639455783\n\n\n\ndef reduction_mse(df_dataset, input_attribute, target_attribute):\n    # Calculate the initial MSE\n    mean_target = df_dataset[target_attribute].mean()\n    initial_mse = ((df_dataset[target_attribute] - mean_target) ** 2).mean()\n    weighted_total_mse = 0.0\n\n    for category in df_dataset[input_attribute].unique():\n        subset = df_dataset[df_dataset[input_attribute] == category]\n        mse_subset = ((subset[target_attribute] - subset[target_attribute].mean()) ** 2).mean()\n        \n        weighted_mse = (len(subset) / len(df_dataset)) * mse_subset\n        weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    return initial_mse - weighted_total_mse\n\n    \n\n\nreduction = {}\nfor attribute in [\"Outlook\", \"Temp\", \"Humidity\", \"Wind\"]:\n    reduction[attribute] = reduction_mse(df, attribute, \"Minutes Played\")\n    \nreduction_ser = pd.Series(reduction)\n\n\nlatexify()\n\n\nbars = reduction_ser.plot(kind='bar', rot=0, color='k')\nformat_axes(plt.gca())\n\n# Add values on top of the bars\nfor bar in bars.patches:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\nplt.xlabel(\"Attribute\")\nplt.ylabel(\"Reduction in MSE\")\nplt.savefig(\"../figures/decision-trees/discrete-input-real-output-level-1.pdf\")"
  },
  {
    "objectID": "notebooks/autograd.html",
    "href": "notebooks/autograd.html",
    "title": "Automatic Gradient Computation",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\n%pip install pydot\n\nCollecting pydot\n  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\nRequirement already satisfied: pyparsing&gt;=2.1.4 in /Users/nipun/miniconda3/lib/python3.9/site-packages (from pydot) (3.0.9)\nInstalling collected packages: pydot\nSuccessfully installed pydot-1.4.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes with labels and operation types\nnode_labels = {\n    'x1': r'$x_1$', 'theta1': r'$\\theta_1$', 'a': r'$a$', \n    'x2': r'$x_2$', 'theta2': r'$\\theta_2$', 'b': r'$b$', \n    'theta0': r'$\\theta_0$', \n    'mul_x1_theta1': r'$\\ast$', 'mul_x2_theta2': r'$\\ast$', \n    'add_a_b_theta0': r'$+$', 'exp': r'$e^{-}$', 'add_one_exp': r'$+$', 'logistic': r'$\\frac{1}{1+e^{-}}$'\n}\n\n# Add nodes with colors for different operations\nG.add_nodes_from([\n    ('x1', {'color': '#61c0bf'}), ('theta1', {'color': '#61c0bf'}), ('a', {'color': '#61c0bf'}),\n    ('mul_x1_theta1', {'color': '#6fc37e'}),\n    ('x2', {'color': '#f48c06'}), ('theta2', {'color': '#f48c06'}), ('b', {'color': '#f48c06'}),\n    ('mul_x2_theta2', {'color': '#6fc37e'}),\n    ('theta0', {'color': '#5e5e5e'}),\n    ('add_a_b_theta0', {'color': '#ff4d4d'}),\n    ('linear_combination', {'color': '#a35ebf'}),\n    ('exp', {'color': '#ff4d4d'}),\n    ('add_one_exp', {'color': '#ff4d4d'}),\n    ('denominator', {'color': '#a35ebf'}),\n    ('logistic', {'color': '#5e5e5e'})\n])\n\n# Add edges\nG.add_edges_from([\n    ('x1', 'mul_x1_theta1'), ('theta1', 'mul_x1_theta1'), ('mul_x1_theta1', 'a'),\n    ('x2', 'mul_x2_theta2'), ('theta2', 'mul_x2_theta2'), ('mul_x2_theta2', 'b'),\n    ('a', 'add_a_b_theta0'), ('b', 'add_a_b_theta0'), ('theta0', 'add_a_b_theta0'),\n    ('add_a_b_theta0', 'linear_combination'),\n    ('linear_combination', 'exp'),\n    ('exp', 'add_one_exp'),\n    ('add_one_exp', 'denominator'),\n    ('denominator', 'logistic')\n])\n\n# Draw the graph with a horizontal layout\npos = {'x1': (0, 3), 'theta1': (1, 3), 'a': (2, 3),\n       'mul_x1_theta1': (1.5, 2.5),\n       'x2': (0, 1), 'theta2': (1, 1), 'b': (2, 1),\n       'mul_x2_theta2': (1.5, 1.5),\n       'theta0': (3, 2),\n       'add_a_b_theta0': (2.5, 2),\n       'linear_combination': (4, 2),\n       'exp': (5, 2),\n       'add_one_exp': (6, 2),\n       'denominator': (7, 2),\n       'logistic': (8, 2)}\n\nnx.draw(G, pos, with_labels=True, labels=node_labels, node_size=200, \n        font_size=8, font_color=\"black\", font_weight=\"bold\", arrowsize=15, node_color=[G.nodes[n]['color'] for n in G.nodes])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndef draw_and_save(graph, filename):\n    pos = nx.drawing.nx_pydot.graphviz_layout(graph, prog='dot')\n    nx.draw(graph, pos, with_labels=True, node_size=200, node_color=\"skyblue\",\n            font_size=10, font_color=\"black\", font_weight=\"bold\", arrowsize=15)\n    plt.savefig(filename)\n    plt.close()\n\n# Stage 1: Input Variables\nG_stage1 = nx.DiGraph()\nG_stage1.add_nodes_from(['x1', 'theta1', 'x2', 'theta2'])\ndraw_and_save(G_stage1, 's1.pdf')\n\n# Stage 2: Intermediate Variables a and b\nG_stage2 = nx.DiGraph()\nG_stage2.add_nodes_from(['x1', 'theta1', 'a', 'x2', 'theta2', 'b'])\nG_stage2.add_edges_from([('x1', 'a'), ('theta1', 'a'), ('x2', 'b'), ('theta2', 'b')])\ndraw_and_save(G_stage2, 's2.pdf')\n\n# Stage 3: Linear Combination\nG_stage3 = nx.DiGraph()\nG_stage3.add_nodes_from(['x1', 'theta1', 'a', 'x2', 'theta2', 'b', 'theta0', 'linear_combination'])\nG_stage3.add_edges_from([('a', 'linear_combination'), ('b', 'linear_combination'), ('theta0', 'linear_combination')])\ndraw_and_save(G_stage3, 's3.pdf')\n\n# Stage 4: Exponential Term\nG_stage4 = nx.DiGraph()\nG_stage4.add_nodes_from(['x1', 'theta1', 'a', 'x2', 'theta2', 'b', 'theta0', 'linear_combination', 'exp'])\nG_stage4.add_edges_from([('linear_combination', 'exp')])\ndraw_and_save(G_stage4, 's4.pdf')\n\n# Stage 5: Denominator\nG_stage5 = nx.DiGraph()\nG_stage5.add_nodes_from(['x1', 'theta1', 'a', 'x2', 'theta2', 'b', 'theta0', 'linear_combination', 'exp', 'denominator'])\nG_stage5.add_edges_from([('exp', 'denominator'), ('1', 'denominator')])\ndraw_and_save(G_stage5, 's5.pdf')\n\n# Stage 6: Logistic Function\nG_stage6 = nx.DiGraph()\nG_stage6.add_nodes_from(['x1', 'theta1', 'a', 'x2', 'theta2', 'b', 'theta0', 'linear_combination', 'exp', 'denominator', 'logistic'])\nG_stage6.add_edge('1', 'logistic')\ndraw_and_save(G_stage6, 's6.pdf')"
  },
  {
    "objectID": "notebooks/logistic-regression-cost.html",
    "href": "notebooks/logistic-regression-cost.html",
    "title": "Logistic Regression - Cost Function",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\n# Matplotlib retina\n%config InlineBackend.figure_format = 'retina'\n\n\nX = np.array([\n    [1],\n    [2],\n    [3],\n    [4],\n    [5],\n    [6]\n])\n\ny = np.array([1, 1, 1, 0, 0, 0])\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nlr = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nlr.fit(X, y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nlr.coef_\n\narray([[-18.33148189]])\n\n\n\nlr.intercept_\n\narray([64.11147504])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(z))\n\n\ntheta_0_li, theta_1_li = np.meshgrid(np.linspace(-10, 10, 200), np.linspace(-10, 10, 200))\n\n\ndef cost_rmse(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    err = np.sum((y-y_hat)**2)\n    return err\n\n\nz = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z[i, j] = cost_rmse(theta_0_li[i, j], theta_1_li[i, j])\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z)\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nplt.colorbar()\nplt.title('RMSE contour plot')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-sse-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z)\nax.set_title('RMSE surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"../figures/logistic-regression/logistic-sse-loss-3d.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n\npd.DataFrame(z).min().min()\n\n9.01794626038055\n\n\n\ndef cost_2(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    \n    err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n    return err\n\n\nz2 = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z2[i, j] = cost_2(theta_0_li[i, j], theta_1_li[i, j])\n\n/tmp/ipykernel_851067/1266618369.py:4: RuntimeWarning: divide by zero encountered in log\n  err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n/tmp/ipykernel_851067/1266618369.py:4: RuntimeWarning: invalid value encountered in multiply\n  err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z2)\nax.set_title('Cross-entropy surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"../figures/logistic-regression/logistic-cross-loss-surface.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z2)\nplt.title('Cross-entropy contour plot')\nplt.colorbar()\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-cross-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\ny.shape, y_bar.shape\n\n((6,), (10000,))\n\n\n\ny = 0\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 0\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"../figures/logistic-regression/logistic-cross-cost-0.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: divide by zero encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: invalid value encountered in multiply\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: invalid value encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n\n\n\n\n\n\n\n\n\n\ny = 1\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 1\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"../figures/logistic-regression/logistic-cross-cost-1.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: divide by zero encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: invalid value encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: invalid value encountered in multiply\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n\n\n\n\n\n\n\n\n\n\nLikelihood\n\nX_with_one = np.hstack((np.ones_like(X), X))\n\n\n\\[\\begin{align*}\nP(y | X, \\theta) &= \\prod_{i=1}^{n} P(y_{i} | x_{i}, \\theta) \\\\ &= \\prod_{i=1}^{n} \\Big\\{\\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{y_{i}}\\Big\\{1 - \\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{1 - y_{i}} \\\\\n\\end{align*}\\]\n\nX_with_one[1]\n\narray([1, 2])\n\n\n\ndef likelihood(theta_0, theta_1):\n    s = 1\n\n    for i in range(len(X)):\n        y_i_hat = sigmoid(-X_with_one[i]@np.array([theta_0, theta_1]))\n        s = s* ((y_i_hat**y[i])*(1-y_i_hat)**(1-y[i]))\n    \n    \n    return s\n\nx_grid_2, y_grid_2 = np.mgrid[-5:100:0.5, -30:10:.1]\n\nli = np.zeros_like(x_grid_2)\nfor i in range(x_grid_2.shape[0]):\n    for j in range(x_grid_2.shape[1]):\n        li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j])\n        \n\n\nplt.contourf(x_grid_2, y_grid_2, li)\n#plt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.colorbar()\nplt.scatter(lr.intercept_[0], lr.coef_[0], s=200, marker='*', color='r', label='MLE')\nplt.title(r\"Likelihood as a function of ($\\theta_0, \\theta_1$)\")\n#plt.gca().set_aspect('equal')\nplt.legend()\nplt.savefig(\"../figures/logistic-regression/logistic-likelihood.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Dummy Variables and Multi-collinearity\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nTaylor’s Series\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric interpretation of Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansion in Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar, Kalash Kankaria, Inderjeet Singh Bhullar\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansion in Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Gradient Computation\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nContour and Surface Plots\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Discrete Input and Discrete Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nMeshgrid\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Feature Importance\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation of Ensemble Models\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparams Tuning Strategies Experimentation\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nBias Variance Tradeoff\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nNotion of Confusion in ML\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nR Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Sophisticated vs Dummy Baseline ML Algorithms for Imbalanced Datasets\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nTraditional Programming vs Machine Learning\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nR Yeeshu Dhurandhar, Rahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Real Input and Discrete Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Real Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Entropy\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Diffusion to Generate Images from Text\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - I\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Basis\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Iris dataset\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Cost Function\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nObject detection using YOLO and segmentation using Segment Anything\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron learning algorithm\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees [Real I/P Real O/P, Bias vs Variance]\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nNipun Batra, Rahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Tuning\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy Pandas Basics\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nR Yeeshu Dhurandhar, Nipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Classes and Plotting Trees\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nCross Validation Diagrams\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube video to transcript using openAI whisper and summary using OLLama\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ensemble/binomial.html",
    "href": "ensemble/binomial.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nSPINE_COLOR = 'gray'\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\ndef bino(n, p, k)\n\n\n choose(n, k) * p**k * (1-p)**(n-k)\n\n\nstats.binom.pmf(n=21, p=0.3, k=4)\n\n0.11277578372328753\n\n\n\nfrom scipy import stats\na=range(21)\nfor error in [0.3, 0.6]:\n    fig, ax = plt.subplots(figsize=(4,3))\n    ax.bar(a,stats.binom.pmf(n=21, p=error, k=a), color='grey', alpha=0.3)\n    ax.bar(a[11:],stats.binom.pmf(n=21, p=error, k=a[11:]), color='grey', alpha=0.9)\n\n\n    ax.set_ylabel(r'$P(X=k)$')\n    ax.set_xlabel(r'$k$')\n    #ax.set_ylim((0,0.4))\n    #ax.legend()\n    format_axes(ax)\n    #plt.fill_betweenx(3, 11, 20)\n    plt.axvline(10.5, color='k',lw=2, label=r'$k=11, \\epsilon={}$'.format(error))\n    plt.title(\"Probability that majority vote \\n (11 out of 21) is wrong = {}\".format(sum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(3)))\n    plt.legend()\n    import tikzplotlib\n\n    tikzplotlib.save(\"test-{}.tex\".format(error), )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!cat test.tex\n\n% This file was created by tikzplotlib v0.9.0.\n\\begin{tikzpicture}\n\n\\definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}\n\n\\begin{axis}[\ntick align=outside,\ntick pos=left,\ntitle={Simple plot \\(\\displaystyle \\frac{\\alpha}{2}\\)},\nx grid style={white!69.0196078431373!black},\nxlabel={time (s)},\nxmin=-0.75, xmax=15.75,\nxtick style={color=black},\ny grid style={white!69.0196078431373!black},\nylabel={Voltage (mV)},\nymin=0, ymax=0.24892,\nytick style={color=black}\n]\n\\draw[draw=none,fill=color0] (axis cs:0,0) rectangle (axis cs:1.5,0.00293333333333333);\n\\draw[draw=none,fill=color0] (axis cs:1.5,0) rectangle (axis cs:3,0.0157333333333333);\n\\draw[draw=none,fill=color0] (axis cs:3,0) rectangle (axis cs:4.5,0.110866666666667);\n\\draw[draw=none,fill=color0] (axis cs:4.5,0) rectangle (axis cs:6,0.116866666666667);\n\\draw[draw=none,fill=color0] (axis cs:6,0) rectangle (axis cs:7.5,0.237066666666667);\n\\draw[draw=none,fill=color0] (axis cs:7.5,0) rectangle (axis cs:9,0.0889333333333333);\n\\draw[draw=none,fill=color0] (axis cs:9,0) rectangle (axis cs:10.5,0.0760666666666667);\n\\draw[draw=none,fill=color0] (axis cs:10.5,0) rectangle (axis cs:12,0.012);\n\\draw[draw=none,fill=color0] (axis cs:12,0) rectangle (axis cs:13.5,0.00593333333333333);\n\\draw[draw=none,fill=color0] (axis cs:13.5,0) rectangle (axis cs:15,0.000266666666666667);\n\\end{axis}\n\n\\end{tikzpicture}\n\n\n\nsum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(2)\n\n0.83"
  },
  {
    "objectID": "knn/knn/knn.html",
    "href": "knn/knn/knn.html",
    "title": "IRIS Dataset",
    "section": "",
    "text": "# https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html\n\nimport numpy as np\nimport pylab as plt\nfrom sklearn import neighbors, datasets\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "knn/knn/knn.html#iris-dataset",
    "href": "knn/knn/knn.html#iris-dataset",
    "title": "IRIS Dataset",
    "section": "IRIS Dataset",
    "text": "IRIS Dataset\n\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ninteresting side note: why setosa was getting high in random forest\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .02 # step size in the mesh\nK = 100\nknn=neighbors.KNeighborsClassifier(n_neighbors=K)\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndef probDist(pts, K, c):\n    pairDist = cdist([pts], X, 'euclidean')\n    idx = np.argsort(pairDist[0])[1:K+1]\n    return (Y[idx] == c).sum()/K\n\n\nfor K in [1, 2, 3]:\n    for B in [0, 1, 2]:\n        print(K, B)\n        h = .1 # step size in the mesh\n        x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n        y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n        Z = np.array([probDist(temp, K, B) for temp in np.c_[xx.ravel(), yy.ravel()]])\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        plt.set_cmap(plt.cm.Paired)\n        plt.contourf(xx, yy, Z, 20, cmap='viridis')\n        plt.colorbar();\n\n        # Plot also the training points\n        plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n        plt.xlabel('Sepal length')\n        plt.ylabel('Sepal width')\n\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.xticks(())\n        plt.yticks(())\n\n        plt.savefig('iris_prob_{}_{}.pdf'.format(K, B), transparent=True, bbox_inches=\"tight\")\n        plt.clf()\n    #plt.show()\n\n1 0\n1 1\n1 2\n2 0\n2 1\n2 2\n3 0\n3 1\n3 2\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "knn/knn/knn.html#random-blobs",
    "href": "knn/knn/knn.html#random-blobs",
    "title": "IRIS Dataset",
    "section": "Random Blobs",
    "text": "Random Blobs\n\nX, Y = datasets.make_blobs(n_samples=500, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('big.pdf'.format(K, B), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .02 # step size in the mesh\nknn=neighbors.KNeighborsClassifier()\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.show()\n\n\n\n\n\n\n\n\n\nK = 1000\nB = 2\n\nh = .1 # step size in the mesh\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = np.array([probDist(temp, K, B) for temp in np.c_[xx.ravel(), yy.ravel()]])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.contourf(xx, yy, Z, 20, cmap='viridis')\nplt.colorbar();\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n\n\n\n\n\n\n\n\nExample 1\n\nX = np.array([[1,5],[2,5],[3,5],[4,5],[1,3],[2,3],[3,3],[4,3],[2.5,1]])\nY = np.array([0,0,0,0,1,1,1,1,0])\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.savefig('exp.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nh = .1 # step size in the mesh\nfor K in [ 1, 3, 9]:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=K)\n    knn.fit(X, Y)\n    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.set_cmap(plt.cm.Paired)\n    plt.pcolormesh(xx, yy, Z)\n\n    # Plot also the training points\n    plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    plt.savefig('exp_knn_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n    #plt.show()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\nExample 2\n\n# X, Y = datasets.make_blobs(n_samples=50, centers=3, n_features=2, random_state=12, cluster_std=2)\n\n# plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n# plt.xlabel('X')\n# plt.ylabel('Y')\n\n\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrain_err = []\ntest_err = []\nk_list = [t for t in range(1,100)]\nfor k in k_list:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n    Z_train = knn.predict(X_train)\n    Z_test = knn.predict(X_test)\n    \n    train_err.append((Z_train != Y_train).sum()/Y_train.size)\n    test_err.append((Z_test != Y_test).sum()/Y_test.size)\n    #print(k,(Z_train != Y_train).sum()/Y_train.size, (Z_test != Y_test).sum()/Y_test.size)\n\n\nplt.plot(k_list, train_err)\nplt.plot(k_list, test_err)"
  },
  {
    "objectID": "knn/knn/knn.html#other-distance-metrics",
    "href": "knn/knn/knn.html#other-distance-metrics",
    "title": "IRIS Dataset",
    "section": "Other Distance Metrics",
    "text": "Other Distance Metrics\n\nX, Y = datasets.make_blobs(n_samples=100, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('iris_knn_data.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K)\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_def_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"manhattan\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_man_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"chebyshev\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_che_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCurse of Dimensionality"
  }
]